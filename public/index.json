[{"content":"Abstract BERT（Devlin等人，2018）和RoBERTa（Liu等人，2019）在语义文本相似性（STS）等句对回归任务上创造了新的最先进性能。然而，它需要将两个句子都送入网络，这造成了大量的计算开销：在10,000个句子的集合中找到最相似的一对，需要用BERT进行大约5千万次推理计算（约65小时）。BERT的构造使得它不适合于语义相似性搜索以及像聚类这样的无监督任务。\n在这份出版物中，我们提出了Sentence-BERT（SBERT），这是对预训练的BERT网络的修改，使用连体和三连体网络结构来得出有语义的句子嵌入，可以使用余弦相似度进行比较。这将寻找最相似对的努力从BERT/RoBERTa的65小时减少到SBERT的约5秒，同时保持BERT的准确性。\n我们在常见的STS任务和转移学习任务上对SBERT和SRoBERTa进行了评估，它的表现超过了其他最先进的句子嵌入方法。\n1 Introduction 在本出版物中，我们提出了Sentence-BERT（SBERT），这是一种使用连体和三连体网络对BERT网络的修改，能够得出有语义的句子嵌入2。这使得BERT能够用于某些新的任务，而这些任务到现在为止还不适用于BERT。这些任务包括大规模的语义相似性比较、聚类和通过语义搜索进行信息检索。\nBERT在各种句子分类和句对回归任务上创造了新的最先进的性能。BERT使用一个交叉编码器：两个句子被传递给变换器网络，并预测目标值。然而，由于可能的组合太多，这种设置不适合于各种对回归任务。在n = 10 000个句子的集合中找到相似度最高的一对，需要用BERT进行n-(n-1)/2 = 49 995 000次推理计算。 在现代V100 GPU上，这需要大约65小时。类似地，在Quora的4000多万个现有问题中找到与一个新问题最相似的问题，可以用BERT建模为一对一的比较，然而，回答一个查询需要50多个小时。\n解决聚类和语义搜索的常见方法是将每个句子映射到一个矢量空间，使语义相似的句子接近。研究人员已经开始将单个句子输入BERT，并得出固定大小的句子嵌入。最常用的方法是对BERT输出层进行平均化（称为BERT嵌入）或通过使用第一个标记（[CLS]标记）的输出。正如我们将表明的那样，这种常见的做法产生的句子嵌入相当糟糕，往往比平均GloVe嵌入更糟糕（Pennington等人，2014）。\n为了缓解这一问题，我们开发了SBERT。连体网络结构使输入句子的固定大小向量可以被导出。使用像余弦相似性或曼哈顿/欧几里得距离这样的相似性措施，可以找到语义上相似的句子。这些相似度测量可以在现代硬件上极其有效地进行，使SBERT可以用于语义相似度搜索以及聚类。在10,000个句子的集合中寻找最相似的句子对的复杂性从使用BERT的65小时减少到计算10,000个句子的嵌入（使用SBERT约5秒）和计算余弦相似度（约0.01秒）。通过使用优化的索引结构，寻找最相似的Quora问题可以从50小时减少到几毫秒（Johnson等人，2017）。\n我们在NLI数据上对SBERT进行了微调，它创建的句子嵌入明显优于其他最先进的句子嵌入方法，如InferSent（Conneau等人，2017）和Universal Sentence Encoder（Cer等人，2018）。在七个语义文本相似性（STS）任务上，SBERT与InferSent相比实现了11.7分的改进，与Universal Sentence Encoder相比实现了5.5分。在句子嵌入的评估工具包SentEval（Conneau和Kiela，2018）上，我们分别实现了2.1和2.6分的改进。\nSBERT可以适应特定的任务。它在一个具有挑战性的论据相似性数据集（Misra等人，2016年）和一个用于区分维基百科文章不同部分的句子的三联体数据集（Dor等人，2018年）上创造了新的最先进的性能。\n本文结构如下：第3节介绍了SBERT，第4节在常见的STS任务和具有挑战性的Argument Facet Similarity（AFS）语料库（Misra等人，2016）上评估了SBERT。第5节在SentEval上评估了SBERT。在第6节，我们进行了一项消融研究，以测试SBERT的一些设计方面。在第7节，我们比较了SBERT句子嵌入的计算效率与其他最先进的句子嵌入方法的对比。\n2 Related Work 我们首先介绍了BERT，然后，我们讨论了目前最先进的句子嵌入方法。\nBERT（Devlin等人，2018）是一个预训练的变换器网络（Vaswani等人，2017），它为各种NLP任务设置了新的最先进的结果，包括问题回答、句子分类和句对回归。用于句对回归的BERT的输入由两个句子组成，由一个特殊的[SEP]标记分开。在12层（基础模型）或24层（大型模型）上应用多头关注，输出被传递给一个简单的回归函数，以得出最终的标签。使用这种设置，BERT在语义文本相似度（STS）基准上创造了新的最先进的性能（Cer等人，2017）。RoBERTa（Liu等人，2019）表明，BERT的性能可以通过对预训练过程的小调整来进一步提高。我们还测试了XLNet（Yang等人，2019），但它总体上导致的结果比BERT更差。\nBERT网络结构的一个很大的缺点是没有计算独立的句子嵌入，这使得从BERT得出句子嵌入变得很困难。为了绕过这一限制，研究人员通过BERT传递单个句子，然后通过平均输出（类似于平均词嵌入）或使用特殊CLS标记的输出，得出一个固定大小的向量（例如：May等人（2019）；Zhang等人（2019）；Qiao等人（2019））。 这两个选项也是由流行的bert-as-a-service-repository3提供。据我们所知，到目前为止，还没有评估这些方法是否会导致有用的句子嵌入。\n句子嵌入是一个研究得很好的领域，有几十种提议的方法。Skip-Thought（Kiros等人，2015）训练一个编码器-解码器架构来预测周围的句子。 InferSent（Conneau等人，2017）使用斯坦福自然语言推理数据集（Bowman等人，2015）和MultiGenre NLI数据集（Williams等人，2018）的标记数据来训练一个连体BiLSTM网络，对输出进行最大池化。Conneau等人的研究表明，InferSent一直优于SkipThought等无监督方法。Universal Sentence Encoder（Cer等人，2018）训练了一个转化器网络，并通过SNLI的训练增强了无监督学习。Hill等人（2016）表明，训练句子嵌入的任务对其质量有很大影响。之前的工作（Conneau等人，2017；Cer等人，2018）发现，SNLI数据集适合训练句子嵌入。Yang等人（2018）提出了一种使用连体DAN和连体变换器网络对Reddit的对话进行训练的方法，在STS基准数据集上取得了良好的效果。\nHumeau等人（2019）解决了来自BERT的交叉编码器的运行时间开销，并提出了一种方法（poly-encoders）来计算m个语境向量和使用注意力的预计算的候选嵌入之间的得分。这个想法对于在更大的集合中寻找最高得分的句子是有效的。然而，多编码器有一个缺点，即分数函数不是对称的，而且对于像聚类这样的用例来说，计算开销太大，这需要$O(n^2)$的分数计算。\n以前的神经句子嵌入方法是从随机初始化开始训练的。 在本出版物中，我们使用预先训练好的BERT和RoBERTa网络，只对其进行微调以产生有用的句子嵌入。这大大减少了所需的训练时间：SBERT可以在不到20分钟内完成调整，同时产生比同类句子嵌入方法更好的结果。\n3 Model SBERT在BERT/RoBERTa的输出上增加了一个池化操作，以得出一个固定大小的句子嵌入。我们试验了三种池化策略：使用CLS-token的输出，计算所有输出向量的平均值（MEANstrategy），以及计算输出向量的最大超时（MAX-strategy）。默认配置是MEAN。\n为了微调BERT / RoBERTa，我们创建了连体和三连体网络（Schroff等人，2015），以更新权重，使产生的句子嵌入具有语义，可以用余弦相似度进行比较。\n网络结构取决于可用的训练数据。我们试验了以下结构和目标函数。\n图1：具有分类目标函数的SBERT结构，例如，用于SNLI数据集的微调。 两个BERT网络具有并列的权重（连体网络结构）。\n分类目标函数。 我们将句子嵌入$u$和$v$的元素之差$|u-v|$连接起来，并与可训练权重$W_t∈R^{3n×k}$相乘：\n$$o = \\text{softmax}(W_t(u, v, |u − v|))$$\n其中n是句子嵌入的维度，k是标签的数量。我们优化交叉熵损失。这个结构在图1中被描述出来。\n图2：推理时的SBERT架构，例如，计算相似性分数。这个架构也用于回归的目标函数。\n回归目标函数。 两个句子嵌入$u$和$v$之间的余弦相似度被计算出来（图2）。我们使用均方误差损失作为目标函数。\n三重目标函数。 给定一个锚点句子$a$，一个正面句子$p$和一个负面句子$n$，triplet loss调整网络，使$a$和$p$之间的距离小于$a$和$n$之间的距离：\n$$max(||s_a − s_p|| − ||s_a − s_n|| + \\epsilon, 0)$$\n$s_x$是$a/n/p$的句子嵌入，$|| \\cdot ||$是距离度量和余量$\\epsilon$。余量$\\epsilon$确保$s_p$至少比$s_n$更接近$s_a$。作为度量，我们使用欧氏距离，在我们的实验中，我们设定$\\epsilon =1$。\n3.1 Training Details 我们在SNLI（Bowman等人，2015）和Multi-Genre NLI（Williams等人，2018）数据集的组合上训练SBERT。SNLI是一个由57万个句子对组成的集合，注释了矛盾、缩略语和中性标签。MultiNLI包含430,000个句子对，涵盖了一系列的口语和书面文本的体裁。我们用3个软分类器的目标函数对SBERT进行了微调。我们使用了16个批次，学习率为2e-5的Adam优化器，以及10%的训练数据的线性学习率预热。我们默认的池化策略是MEAN。\n4 Evaluation - Semantic Textual Similarity 我们评估了SBERT在常见的语义文本相似性（STS）任务中的表现。 最先进的方法通常学习一个（复杂的）回归函数，将句子嵌入映射到相似性分数。相反，我们总是使用余弦相似度来比较两个句子嵌入之间的相似度。我们还用负曼哈顿和负欧几里得距离作为相似度测量方法进行了实验，但所有方法的结果都大致相同。\n表1：在各种文本相似性（STS）任务中，句子表征的余弦相似性和黄金标签之间的Spearman等级相关$ρ$。性能按惯例报告为$ρ×100$。STS12-STS16: SemEval 2012-2016, STSb：STSbenchmark, SICK-R: SICK relatedness dataset.\n4.1 Unsupervised STS 我们在不使用任何STS特定训练数据的情况下评估了SBERT在STS中的表现。我们使用2012-2016年的STS任务（Agirre等人，2012，2013，2014，2015，2016），STS基准（Cer等人，2017），以及SICK-Relatedness数据集（Marelli等人，2014）。这些数据集在句子对的语义相关度上提供了0到5的标签。我们在（Reimers等人，2016）中表明，皮尔逊相关度很不适合STS。相反，我们计算了句子嵌入的余弦相似度和黄金标签之间的Spearman等级相关。其他句子嵌入方法的设置是等同的，相似度是通过余弦相似度计算的。结果见表1\n结果显示，直接使用BERT的输出会导致相当差的性能。对BERT嵌入的平均数只达到了54.81的平均相关度，而使用CLStoken输出只达到了29.19的平均相关度。两者都比计算GloVe的平均嵌入要差。\n使用所描述的连体网络结构和微调机制大大改善了相关性，大大超过了InferSent和Universal Sentence Encoder的表现。SBERT表现比Universal Sentence Encoder差的唯一数据集是SICK-R。Universal Sentence Encoder是在各种数据集上训练的，包括新闻、问答页和讨论区，这似乎更适合SICK-R的数据。相比之下，SBERT只在维基百科（通过BERT）和NLI数据上进行了预训练。\n虽然RoBERTa能够提高几个监督任务的性能，但我们只观察到SBERT和SRoBERTa在生成句子嵌入方面的微小差异。\n表2：对STS基准测试集的评估。 BERT系统用10个随机种子和4个epochs训练。SBERT在STSb数据集上进行了微调，SBERT-NLI在NLI数据集上进行了预训练，然后在STSb数据集上进行了微调。\n4.2 Supervised STS STS基准（STSb）（Cer等人，2017）提供是一个流行的数据集，用于评估有监督的STS系统。该数据包括来自标题、新闻和论坛三个类别的8,628个句子对。它被分为训练（5,749）、设计（1,500）和测试（1,379）。BERT在这个数据集上创造了新的最先进的性能，它将两个句子都传给了网络，并对输出使用了简单的回归方法。\n我们使用训练集，利用回归目标函数对SBERT进行微调。在预测时，我们计算句子嵌入之间的余弦相似度。所有系统都是用10个随机种子进行训练，以对抗变异（Reimers和Gurevych，2018）。\n结果在表2中描述。我们对两种设置进行了实验：只在STSb上训练，以及先在NLI上训练，然后在STSb上训练。我们观察到，后来的策略导致了1-2分的轻微改善。这种两步法对BERT交叉编码器的影响特别大，它的性能提高了3-4个点。我们没有观察到BERT和RoBERTa之间的显著差异\n表3：Argument Facet Similarity（AFS）语料库的平均皮尔逊相关性r和平均斯皮尔曼等级相关性ρ（Misra等人，2016）。Misra等人提议进行10倍交叉验证。我们还在一个跨主题的情况下进行评估：方法在两个主题上训练，并在第三个主题上进行评估。\n4.3 Argument Facet Similarity 我们在Misra等人（2016）的Argument Facet Similarity（AFS）语料库上评估SBERT。AFS语料库注释了来自社交媒体对话的6,000个句子论据对，涉及三个有争议的话题：枪支管制、同性恋婚姻和死刑。这些数据被注释为从0（\u0026ldquo;不同的话题\u0026rdquo;）到5（\u0026ldquo;完全等同\u0026rdquo;）的等级。AFS语料库中的相似性概念与SemEval的STS数据集中的相似性概念相当不同。STS数据通常是描述性的，而AFS数据是对话中的争论性摘录。要被认为是相似的，论据不仅要提出相似的主张，而且要提供相似的推理。此外，AFS中的句子之间的词汇差距要大得多。因此，简单的无监督方法以及最先进的STS系统在这个数据集上表现很差（Reimers等人，2019）。\n我们在两种情况下对该数据集的SBERT进行评估：1）正如Misra等人所建议的，我们使用10倍交叉验证来评估SBERT。这种评估设置的一个缺点是，不清楚方法对不同主题的概括性如何。因此，2）我们在一个跨主题的设置中评估SBERT。两个主题用于训练，方法在被遗漏的主题上被评估。我们对所有三个主题重复这一过程，并对结果进行平均。\nSBERT使用回归目标函数进行微调。相似性得分是使用基于句子嵌入的余弦相似性来计算的。我们还提供了皮尔逊相关r，以使结果与Misra等人的结果相媲美。然而，我们表明（Reimers等人，2016），皮尔逊相关有一些严重的缺点，应该避免用于比较STS系统。结果在表3中描述。\n像tf-idf、平均GloVe嵌入或InferSent这样的无监督方法在这个数据集上表现得相当糟糕，得分很低。在10倍交叉验证设置中训练SBERT，其性能几乎与BERT相当。\n然而，在跨主题的评估中，我们观察到SBERT的性能下降了大约7点Spearman相关性。要被认为是相似的，论据应该针对相同的主张，并提供相同的推理。BERT能够使用注意力来直接比较两个句子（例如逐字比较），而SBERT必须将单个句子从一个未见过的主题映射到一个向量空间，从而使具有类似主张和理由的论点接近。这是一个更具挑战性的任务，它似乎需要超过两个主题的训练，才能与BERT的工作相提并论。\n表4：对维基百科部分三联体数据集的评估（Dor等人，2018）。用三联体损失训练的SBERT为一个 epoch\n4.4 Wikipedia Sections Distinction Dor等人（2018年）使用维基百科为句子嵌入方法创建了一个主题上细化的训练、设计和测试集。维基百科的文章被分成不同的部分，专注于某些方面。Dor等人认为，同一章节的句子在主题上比不同章节的句子更接近。他们利用这一点创建了一个大型的弱标记句子三联体数据集：锚和正面例子来自同一章节，而负面例子来自同一文章的不同章节。例如，来自Alice Arnold的文章：锚点：阿诺德在1988年加入了BBC广播剧团，正面：阿诺德在2012年5月获得了媒体的关注。，负面的：鲍丁和阿诺德都是热衷于业余高尔夫的人。\n我们使用Dor等人的数据集。我们使用三联体目标，在大约180万个训练三联体上训练SBERT一个历时，并在222,957个测试三联体上评估它。测试三联体来自维基百科的一个独特的文章集。作为评估指标，我们使用准确性：正面例子是否比负面例子更接近锚点？\n结果见表4。Dor等人微调了一个具有三倍损失的BiLSTM架构，以得出该数据集的句子嵌入。如表所示，SBERT明显优于Dor等人的BiLSTM方法。\n5 Evaluation - SentEval SentEval（Conneau和Kiela，2018）是一个流行的工具包，用于评估句子嵌入的质量。句子嵌入被用作逻辑回归分类器的特征。逻辑回归分类器在10倍交叉验证设置中对各种任务进行训练，并对测试倍数的预测准确性进行计算。\nSBERT句子嵌入的目的不是为了用于其他任务的转移学习。在这里，我们认为Devlin等人（2018）为新任务描述的微调BERT是更合适的方法，因为它更新了BERT网络的所有层。然而，SentEval仍然可以对我们的句子嵌入在各种任务中的质量给出印象。\n我们在以下七个SentEval转移任务上将SBERT的句子嵌入与其他句子嵌入方法进行比较：\nMR：对电影评论片段的情绪预测，以五级为起点（Pang and Lee, 2005）。 CR：顾客产品评论的情绪预测（Hu and Liu, 2004）。 SUBJ：对电影评论和情节摘要中的句子进行主观性预测（Pang and Lee, 2004）。 MPQA：来自新闻网的短语级意见极性分类（Wiebe等人，2005）。 SST：具有二进制标签的斯坦福情感树库（Socher等人，2013）。 TREC：来自TREC的细粒度问题类型分类（Li and Roth, 2002）。 MRPC：来自平行新闻源的微软研究院转述语料库（Dolan等人，2004）。 表5：使用SentEval工具包对SBERT句子嵌入的评估。SentEval通过训练一个以句子嵌入为特征的逻辑回归分类器，在不同的句子分类任务中评估句子嵌入。分数是基于10倍的交叉验证。\n结果可以在表5中找到。SBERT能够在7个任务中的5个任务中取得最佳性能。与InferSent以及Universal Sentence Encoder相比，平均性能提高了约2个百分点。 尽管转移学习不是SBERT的目的，但它在这项任务上超过了其他最先进的句子嵌入方法。\n看来，SBERT的句子嵌入很好地捕捉了情感信息：与InferSent和Universal Sentence Encoder相比，我们观察到SentEval对所有情感任务（MR、CR和SST）都有很大的改进。\nSBERT明显比Universal Sentence Encoder差的唯一数据集是TREC数据集。通用句子编码器在问题回答数据上进行了预训练，这对于TREC数据集的问题类型分类任务似乎是有益的。\n平均BERT嵌入或使用BERT网络的CLStoken输出在各种STS任务中取得了不好的结果（表1），比平均GloVe嵌入更差。然而，对于SentEval，平均BERT嵌入和BERT CLS-token输出取得了不错的结果（表5），超过了平均GloVe嵌入。 其原因是不同的设置。对于STS任务，我们使用余弦相似度来估计句子嵌入之间的相似性。余弦相似度对所有维度都是平等的。相比之下，SentEval将逻辑回归分类器用于句子嵌入。这使得某些维度对分类结果的影响有高有低。\n我们的结论是，BERT的平均嵌入/CLS-token输出返回的句子嵌入不可能与余弦相似性或曼哈顿/欧氏距离一起使用。 对于转移学习，它们产生的结果比InferSent或通用句子编码器略差。然而，在NLI数据集上使用所描述的具有连体网络结构的微调设置，产生的句子嵌入达到了SentEval工具箱的新的先进水平。\n6 Ablation Study 我们已经为SBERT句子嵌入的质量证明了强有力的经验结果。在本节中，我们对SBERT的不同方面进行了消减研究，以便更好地了解其相对重要性。\n我们评估了不同的集合策略（MEAN、MAX和CLS）。对于分类目标函数，我们评估了不同的连接方法。对于每个可能的配置，我们用10个不同的随机种子训练SBERT，并对其性能进行平均。\n目标函数（分类与回归）取决于注释的数据集。对于分类目标函数，我们在SNLI和Multi-NLI数据集上训练SBERTbase。对于回归目标函数，我们在STS基准数据集的训练集上进行训练。性能是在STS基准数据集的开发部分测量的。结果显示在表6中。\n表6：用分类目标函数在NLI数据上训练的SBERT，用回归目标函数在STS基准（STSb）上训练的SBERT。配置是在STSb的开发集上使用余弦相似度和Spearman等级相关度进行评估的。对于串联方法，我们只报告使用MEAN集合策略的得分。\n当用NLI数据的分类目标函数进行训练时，集合策略的影响相当小。串联模式的影响则大得多。InferSent（Conneau 3989等人，2017）和Universal Sentence Encoder（Cer等人，2018）都使用$(u, v, |u - v|, u ∗ v)$作为softmax分类器的输入。然而，在我们的架构中，添加e element-wise的$u ∗ v$降低了性能。\n最重要的部分是element-wise difference$|u - v|$。请注意，连接模式只与训练softmax分类器有关。在推理中，当预测STS基准数据集的相似性时，只使用句子嵌入$u$和$v$与余弦相似性相结合。element-wise difference衡量两个句子嵌入的维度之间的距离，确保相似的对更接近，不相似的对相距更远。\n当用回归目标函数进行训练时，我们发现集合策略有很大的影响。在这里，MAX策略的表现明显比MEAN或CLS-token策略差。这与（Conneau等人，2017）相反，他们发现InferSent的BiLSTM层使用MAX而不是MEAN池有好处。\n7 Computational Efficiency 句子嵌入有可能需要对数百万个句子进行计算，因此，需要高计算速度。在本节中，我们将SBERT与GloVe平均嵌入、InferSent（Conneau等人，2017）和Universal Sentence Encoder（Cer等人，2018）进行比较。\n对于我们的比较，我们使用STS基准的句子（Cer等人，2017）。我们使用一个简单的for-loop与python字典查询和NumPy计算平均GloVe嵌入。InferSent4是基于PyTorch的。对于Universal Sentence Encoder，我们使用TensorFlow Hub版本5，它是基于TensorFlow的。SBERT是基于PyTorch的。为了改进句子嵌入的计算，我们实施了一个智能批处理策略：这极大地减少了填充标记的计算开销。\n性能是在一台配备英特尔i7-5820K CPU @ 3.30GHz、Nvidia Tesla V100 GPU、CUDA 9.2和cuDNN的服务器上测量的。结果见表7。\n表7：句子嵌入方法的计算速度（每秒钟的句子）。越高越好。\n在CPU上，InferSent比SBERT快约65%。这是由于网络结构要简单得多。InferSent使用一个BiLSTM层，而BERT使用12个堆叠的变压器层。然而，变压器网络的一个优势是在GPU上的计算效率。在那里，带有智能批处理的SBERT比InferSent快约9%，比Universal Sentence Encoder快约55%。智能批处理在CPU上实现了89%的提速，在GPU上实现了48%的提速。平均GloVe嵌入显然在很大程度上是计算句子嵌入的最快方法。\n8 Conclusion 我们表明，BERT开箱即用，将句子映射到一个向量空间，而这个向量空间相当不适合与余弦相似性等常见的相似性措施一起使用。七个STS任务的性能低于GloVe平均嵌入的性能。\n为了克服这个缺点，我们提出了句子ERT（SBERT）。SBERT在一个连体/三连体网络结构中对BERT进行了微调。我们在各种常见的基准上评估了其质量，在那里它可以实现比最先进的句子嵌入方法的重大改进。在我们的实验中，用RoBERTa代替BERT并没有产生明显的改善。\nSBERT在计算上是高效的。在GPU上，它比InferSent快约9%，比Universal Sentence Encoder快约55%。SBERT可用于在计算上不可行的任务，用BERT建模。例如，用分层聚类法对10,000个句子进行聚类，用BERT需要大约65个小时，因为必须计算大约5000万个句子组合。使用SBERT，我们能够将这一努力减少到5秒左右。\n","permalink":"https://lin-roger.github.io/posts/sentencebertsentenceembeddingsusingsiamesebertnetworks/","summary":"🚧目前進度:Deepl全文翻譯🚧","title":"🚧Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks🚧"},{"content":"Abstract 使用語境化詞嵌入替換靜態詞嵌入已在許多NLP任務取得顯著的進步。然而，ELMo和BERT等模型(contextualizing model)所產生的語境化詞嵌入究竟有多語境化?每個詞是否有無限多種特定語境下的表徵或者其本質上是被分配到數量有限的詞表徵中的其中一個?作者發現，在 contextualizing model 的任何一層中，所有詞的語境表徵都不是各向同性的。雖然同一個詞在不同語境下的表徵相較不同詞的表徵具有更大的餘弦相似度，在這種自相似性再上層(輸出)並不明顯。代表 contextualizing model 的上層產生了更多語境表徵，如同LSTM的上層產生更多任務相關的表徵。在ELMo、BERT和GPT-2的所有層中，平均一個詞的語境表徵只有不到5%的方差可以用該詞的靜態嵌入來解釋，為語境表徵的成功提供了一些理由。\n1 Introduction 深度学习方法在NLP中的应用是通过在低维连续空间中将单词表示为向量而实现的。传统上，这些词的嵌入是静态的：每个词都有一个单一的向量，与上下文无关（Mikolov等人，2013a；Pennington等人，2014）。这带来了几个问题，最明显的是，一个多义词的所有意义都必须共享相同的表示。最近的工作，即深度神经语言模型，如ELMo（Peters等人，2018）和BERT（Devlin等人，2018）、我们成功地创建了上下文关联的单词表示，即对其出现的上下文敏感的单词向量。用上下文表征取代静态嵌入，在一系列不同的NLP任务中产生了显著的改进，从回答问题到核心推理。\n语境化词汇表征的成功表明，尽管只用语言建模任务进行训练，但它们可以学习到高度可转移的、与任务无关的语言属性。事实上，在冻结的语境化表征上训练的线性探测模型可以预测单词的语言属性（例如，语篇标签），几乎与最先进的模型一样好（Liu等人，2019a；Hewitt和Manning，2019）。但是，这些表征仍然没有得到很好的理解。 首先，这些语境化的词汇表征到底有多大的语境性？BERT和ELMo是否有无限多的特定语境表征可以分配给每个单词，或者单词基本上是在有限数量的词义表征中分配一个？\n我们通过研究ELMo、BERT和GPT-2的每一层的表示空间的几何形状来回答这个问题。我们的分析产生了一些令人惊讶的发现：\n在所有三个模型的所有层中，所有单词的上下文表征都不是各向同性的：它们在方向上不是均匀分布的。相反，它们是各向异性的，在矢量空间中占据一个狭窄的锥体。GPT-2的最后一层的各向异性是如此的极端，以至于两个随机的词平均来说会有几乎完美的余弦相似性鉴于各向异性对静态嵌入有理论和经验上的好处（Mu等人，2018），上下文代表中各向异性的程度令人惊讶。 同一个词在不同语境中的出现具有非相同的向量表示。在矢量相似性被定义为余弦相似性的情况下，这些表征在上层中彼此更加不相似。这表明，就像LSTM的上层产生更多特定任务的表征一样（Liu等人，2019a），上下文模型的上层产生更多的特定语境表征。 语境特定性在ELMo、BERT和GPT-2中表现得非常不同。在ELMo中，同一句子中的词的表征随着上层的语境特异性的增加而变得更加相似；在BERT中，它们在上层变得更加不相似，但仍然比随机抽样的词平均更加相似；然而在GPT-2中，同一句子中的词并不比两个随机选择的词更加相似。 在对各向异性的影响进行调整后，平均而言，一个词的上下文表征中只有不到5%的方差可以由其第一主成分来解释。 这表明，上下文表征并不对应于有限数量的词义表征，即使在最好的情况下，静态嵌入也是上下文表征的糟糕替代。不过，通过提取一个词的上下文表征的第一个主成分而创建的静态嵌入在许多词向量基准上的表现优于GloVe和FastText嵌入。 这些见解有助于证明为什么使用上下文表征能使许多NLP任务得到如此显著的改善。\n2 Related Work 2.1 Static Word Embeddings Skip-gram with negative sampling（SGNS）（Mikolov等人，2013a）和GloVe（Pennington等人，2014）是生成静态词嵌入的最著名的模型之一。尽管它们在实践中迭代地学习嵌入，但已经证明在理论上它们都隐含了一个包含共现统计的词-语境矩阵的因子（Levy和Goldberg，2014a，b）。由于它们为每个词创建了一个单一的表示，静态词嵌入的一个显著问题是，一个多义词的所有意义必须共享一个矢量。\n2.2 Contextualized Word Representations 鉴于静态单词嵌入的局限性，最近的工作试图创建对语境敏感的单词表示。ELMo（Peters等人，2018）、BERT（Devlin等人，2018）和GPT-2（Radford等人，2019）是深度神经语言模型，它们经过微调，为广泛的下游NLP任务创建模型。他们对单词的内部表征被称为语境化的单词表征，因为它们是整个输入句子的一个函数。这种方法的成功表明，这些表征捕捉到了语言的高度可转移性和任务无关的属性（Liu等人，2019a）\nELMo通过串联在双向语言建模任务上训练的2层biLSTM的内部状态来创建每个标记的语境化表示（Peters等人，2018）。相比之下，BERT和GPT-2分别是双向和单向的基于转化器的语言模型。12层的BERT（基础，套管）和12层的GPT-2的每个转化器层通过关注输入句子的不同部分来创建每个标记的语境化表示（Devlin等人，2018；Radford等人，2019）。BERT\u0026ndash;以及BERT的后续迭代（Liu等人，2019b；Yang等人，2019）\u0026ndash;在各种下游NLP任务上取得了最先进的性能，包括从问题回答到情感分析。\n2.3 Probing Tasks 之前对语境化词汇表征的分析主要限于探测任务（Tenney等人，2019；Hewitt和Manning，2019）。这涉及到训练线性模型来预测单词的句法（如语篇标签）和语义（如单词关系）属性。探测模型的前提是，如果一个简单的线性模型可以被训练来准确预测语言属性，那么表征就会隐含地开始编码这一信息。虽然这些分析发现，语境化的表征编码了语义和句法信息，但它们无法回答这些表征的语境性如何，以及它们在多大程度上可以被静态词缀取代，如果有的话。因此，我们在本文中的工作与大多数对语境化表征的剖析明显不同。它更类似于Mimno和Thompson（2017），后者研究了静态词嵌入空间的几何学。\n3 Approach 3.1 Contextualizing Models 我们在本文中研究的语境化模型是ELMo、BERT和GPT-21。我们选择BERT的基础案例版本，因为它在层数和维度方面与GPT-2最具可比性。我们所使用的模型都是在各自的语言建模任务中预先训练过的。尽管ELMo、BERT和GPT-2分别有2、12和12个隐藏层，但我们也将每个语境化模型的输入层作为其第0层。这是因为第0层没有进行上下文处理，使其成为比较后续层所做的上下文处理的有用基线。\n3.2 Data 为了分析语境化的单词表征，我们需要输入的句子来输入我们的预训练模型。我们的输入数据来自2012-2016年的SemEval语义文本相似性任务（Agirre等人，2012，2013，2014，2015）。我们使用这些数据集是因为它们包含了相同的词出现在不同语境中的句子。例如，\u0026ldquo;狗 \u0026ldquo;这个词出现在 \u0026ldquo;一只熊猫狗在路上跑 \u0026ldquo;和 \u0026ldquo;一只狗想把培根从背上拿下来 \u0026ldquo;中。 如果一个模型在这两个句子中为 \u0026ldquo;狗 \u0026ldquo;生成了相同的表示，我们可以推断出没有上下文；相反，如果这两个表示不同，我们可以推断出它们在某种程度上被上下文化。利用这些数据集，我们将单词映射到它们出现的句子列表以及它们在这些句子中的索引。在我们的分析中，我们不考虑那些出现在少于5个独特语境中的词。\n3.3 Measures of Contextuality 我们用三个不同的指标来衡量一个词的上下文表述的程度：自相似性、句内相似性和最大可解释方差。\nDefinition 1 让 $w$ 是一个分别出现在句子 ${s_1,\u0026hellip;,s_n}$ 的索引 ${i_1,\u0026hellip;,i_n}$ 的词，这样 $w=s_1[i_1]=\u0026hellip;=s_n[i_n]$ 。让 $f_\\ell (s,i)$ 成为一个函数，将 $s[i]$ 映射到模型 $f$ 的第 $\\ell$ 层中的表示。 $w$ 在第 $\\ell$ 层中的自相似性为\n$$SelfSim_\\ell (w) = \\frac{1}{n^2-n} \\sum_{j} \\sum_{k \\not = j} \\cos (f\\ell(s_j, i_j), f\\ell(s_k, i_k))$$\n其中 cos 表示余弦相似度。换句话说，一个单词 $w$ 在第 $\\ell$ 层中的自相似性是其在 $n$ 个独特语境中的上下文表征之间的平均余弦相似度。 如果第 $\\ell$ 层完全不对表征进行上下文化，那么 $SelfSim_\\ell(w)=1$ （即表征在所有语境中都是相同的）。对于 $w$ 来说，表征的情境化程度越高，我们期望它的自我相似性就越低。\n$SelfSim$計算同一個詞在不同句子(語境)中嵌入向量(表徵)的相似度，設有$A、B、C$三個不同的句子，且三個句子中皆有詞$W$，取得$W$在所有句子的嵌入$W_A、W_B、W_C$，並倆兩計算他們之間的餘弦相似度$Sim_{AB}、Sim_{AC}、Sim_{BA}、Sim_{BC}、Sim_{CA}、Sim_{CB}$並計算平均。表徵的語境化程度越高，$SelfSim$就越低(代表不同語境的表徵十分不同)\nDefinition 2 让 $s$ 是一个句子，是 $n$ 个词的序列 $\\langle w_{1},\u0026hellip;,w_{ni}\\rangle$ 。让 $f_\\ell (s,i)$ 是一个函数，将 $s[i]$ 映射到模型 $f$ 的第 $\\ell$ 层中的表示。 $s$ 在第 $\\ell$ 层中的句子内部相似度为\n$$IntraSim_\\ell (s) = \\frac{1}{n} \\sum_{i} \\cos(\\vec{s_\\ell}, f_\\ell(s, i))$$ $$\\text{where} \\ \\vec{s_\\ell} = \\frac{1}{n} \\sum_i f_{\\ell}(s, i)$$\n更简单地说，一个句子的句内相似性是其单词表征和句子向量之间的平均余弦相似度，而句子向量只是这些单词向量的平均值。这种测量方法捕捉到了语境的特殊性在向量空间中的表现。这个衡量标准抓住了上下文特定性在向量空间中的表现。例如，如果 $IntraSim_\\ell(s)$ 和 $SelfSim_\\ell(w)$ 都是低的 $∀ w∈s$(趨異)，那么模型通过给每个词一个上下文特定的表示，使该层中的词与句子中所有其他词的表示不同，从而实现上下文的关联。如果 $IntraSim_\\ell(s)$ 是高的(趨同)，但 $SelfSim_\\ell(w)$ 是低的，这表明上下文的细微差别较小，句子中的词只是通过使它们的表征在向量空间中趋同而被上下文化。\n$IntraSim$計算句中每個詞的嵌入向量和句嵌入向量(使用句中所有詞嵌入之平均)的平均相似性。\n\\ $IntraSim \\ Low$ $IntraSim \\ Hight$ $SelfSim \\ Low$ 模型在不同語境下給予每個詞一個獨特的表徵，從而實現語境化嵌入 模型透過同化句子中詞的嵌入表徵來實現語境化嵌入 $SelfSim \\ Hight$ 模型給予每個詞一個讀特的表徵，在所有語境中皆使用同一表徵，與傳統靜態嵌入無異 極為糟糕，模型給予每個詞幾乎相同的表徵 Definition 3 让 $w$ 是一个分别出现在句子 ${s_1,\u0026hellip;,s_n}$ 的索引 ${i_1,\u0026hellip;,i_n}$ 的词，这样 $w = s_1[i_1] = \u0026hellip; = s_n[i_n]$ 。让 $f\\ell(s,i)$ 成为一个函数，将 $s[i]$ 映射到模型f的第 $\\ell$ 层中的表示。其中 $[ f\\ell(s1,i1)\u0026hellip;f\\ell(sn,in)]$ 是 $w$ 的发生矩阵， $σ_1\u0026hellip;σ_m$ 是该矩阵的前 $m$ 个奇异值，最大可解释方差为\n$$MEV_{\\ell}(w)=\\frac{\\sigma^2_1}{\\sum_i \\sigma_1^2}$$\n$MEV_\\ell(w)$ 是指在某一层的 $w$ 的上下文表征中，可由其第一主成分解释的方差比例。它为我们提供了一个关于静态嵌入可以取代一个词的上下文表征的上限。 $MEV_\\ell(w)$ 越接近于 $0$，静态嵌入的替代性就越差；如果 $MEV_\\ell(w)=1$ ，那么静态嵌入将是上下文表征的完美替代。\nMEV代表同一個詞在不同語境中的表徵所形成的分布，每一個主成分可以當成一個軸，如果第一主成分(最大的那個)，可以表達越多資訊(MEV越大)，代表同一個詞在不同語境的表徵會聚集到空間中的一個軸附近(容易使用一個固定的表徵替代)，反之MEV越小，代表同一個詞在不同語境的表徵會均於分散於空間中。\n3.4 Adjusting for Anisotropy 在讨论语境性时，考虑各向同性（或缺乏各向同性）是很重要的。例如，如果单词向量是完全各向同性的（即方向一致，任意兩個詞的相似度低），那么 $SelfSim_\\ell (w)=0.95$ 将表明 $w$ 的表征的语境性很差。然而，考虑到这样的情况，即词向量是如此的各向异性，任何两个词的平均余弦相似度为$0.99$。那么 $SelfSim_\\ell (w)=0.95$ 实际上就表明了相反的情况\u0026ndash;$w$的表征被很好地上下文化了。这是因为$w$在不同语境中的表征平均来说比两个随机选择的词更不相似。\n为了调整各向异性的影响，我们使用了三种各向异性的基线，每一种用于我们的语境性测量。对于自相似性和句子内相似性，基线是来自不同语境的均匀随机抽样的词的表征之间的平均余弦相似度。在一个给定的层中，单词表征的各向异性越大，这个基线就越接近于1。对于最大可解释方差（MEV），基线是均匀随机抽样的单词表征中被其第一主成分解释的方差比例。某一层的表征越是各向异性，这个基线就越接近于1：即使是随机抽样的词，主成分也能解释很大一部分的方差\n由于情境性测量是针对情境化模型的每一层计算的，我们也为每一层计算单独的基线。然后，我们从每个测量值中减去其各自的基线，得到各向异性调整后的背景性测量。例如，各向异性调整后的自相似性为\n$$Baseline(f_\\ell) = E_{x,y∼U(O)}[\\cos(f_\\ell(x), f_\\ell(y))]$$ $$SelfSim^∗_\\ell(w) = SelfSim_\\ell(w)−Baseline(f_\\ell)$$\n其中$O$是所有单词出现的集合，$f_\\ell()$将一个单词出现映射到模型$f$的$\\ell$层中的表示。除非另有说明，本文其余部分中提到的语境性测量是指各向异性调整后的测量，其中原始测量和基线都是用1K个均匀的随机抽样的词表征来估计的。\nbaseLine of $SelfSim、IntraSim$: 隨機抽兩個詞的表徵間的平均餘弦相似度(1K組)，與各向異性程度成正比\nbaseLine of $MEV$: 隨機抽取1K個詞的表徵，並計算其構成的矩陣之$MEV$，與各向異性程度成正比\n4 Findings 图1：在BERT、ELMo和GPT-2的几乎所有层中，单词表征都是各向异性的（每個方向的分布並非都是均勻分布）：统一随机采样的单词之间的平均余弦相似度不为零。 唯一的例外是ELMo的输入层；鉴于它生成字符级嵌入而不使用上下文，这并不令人惊讶。高层的表征通常比低层的表征更加各向异性。\n4.1 (An)Isotropy 上下文的表征在所有非输入层都是各向异性的。 如果来自某一层的单词表征是各向同性的（每個方向的分布都是均勻分布），那么均匀随机采样的单词之间的平均余弦相似度将是0（Arora等人，2017）。这个平均值越接近于1，表示各向异性越大。各向异性的几何解释是，词的表征都在向量空间中占据一个狭窄的锥体，而不是在所有方向上都是均匀的；各向异性越大，这个圆锥就越窄（Mimno和Thompson，2017）。如图1所示，这意味着在BERT、ELMo和GPT-2的几乎所有层中，所有单词的表征都在向量空间中占据一个狭窄的圆锥。唯一的例外是ELMo的输入层，它产生静态的字符级嵌入，而不使用上下文甚至位置信息（Peters等人，2018）。然而，应该注意的是，并非所有的静态嵌入都一定是各向同性的；Mimno和Thompson（2017）发现，同样是静态的skipgram嵌入并不是各向同性的。\n那靜態的skipgram嵌入各向異性的程度為何?各向異性程度與餘弦相似度的準確性是否明確成反比?\n语境化的表征一般在较高的层次上更具有各向异性。 如图1所示，对于GPT-2，均匀随机的单词之间的平均余弦相似度在第2层到第8层大致为0.6，但从第8层到第12层呈指数级增长。事实上，GPT-2的最后一层的单词表示是如此的各向异性，以至于任何两个单词的平均余弦相似度几乎是完美的!这种模式也适用于BERT和ELMo，但也有例外：例如，BERT的倒数第二层的各向异性比最后一层高得多。\n各向同性对静态词嵌入有理论和经验上的好处。在理论上，它允许在训练期间进行更强的 \u0026ldquo;自我规范化\u0026rdquo;（Arora等人，2017），并且在实践中，从静态嵌入中减去平均矢量会导致在几个下游NLP任务上的改进（Mu等人，2018）。因此，在语境化词汇表征中看到的极端程度的各向异性\u0026ndash;特别是在较高的层中\u0026ndash;是令人惊讶的。如图1所示，对于所有三个模型，上下文隐蔽层的表征几乎都比输入层的表征更加各向异性，后者没有纳入上下文。这表明高各向异性是语境化过程所固有的，或者至少是语境化过程的副产品。\n图2：同一个词在不同语境中的平均余弦相似度被称为该词的自相似度（见定义1）。上面，我们绘制了各向异性调整后的均匀随机抽样词的平均自相似度（见3.4节）。在所有三个模型中，层数越高，自相似度越低，这表明在较高的层数中，语境化的词汇表征更具有语境针对性。\n图3：句内相似度是指一个句子中每个词的表现形式与它们的平均值之间的平均余弦相似度（见定义2）。上面，我们绘制了均匀随机抽样的句子的平均句内相似度，并对各向异性进行了调整。这个统计数字反映了上下文特定性在表示空间中的表现，如上所述，它在ELMo、BERT和GPT-2中的表现非常不同。\n4.2 Context-Specificity 语境化的词汇表征在更高的层次上更具有语境针对性。 从定义1中可以看出，一个词的自相似性，在一个给定模型的某一层中，是它在不同语境中的表征的平均余弦相似度，并根据各向异性进行调整。如果自相似性为1，那么这些表征就完全没有语境特异性；如果自相似性为0，那么这些表征就具有最大的语境特异性。在图2中，我们绘制了BERT、ELMo和GPT-2各层中均匀随机抽样的词的平均自相似度。例如，ELMo的输入层的自相似度是1.0，因为该层的表征是静态字符级嵌入。\n在所有三个模型中，层数越高，自我相似度平均越低。换句话说，层数越高，上下文表征的具体内容越多。这一发现具有直观的意义。在图像分类模型中，低层识别更多的通用特征，如边缘，而高层识别更多的特定类别特征（Yosinski等人，2014）。同样，在NLP任务上训练的LSTM的上层会学习更多的特定任务表征（Liu等人，2019a）。因此，由此可见，神经语言模型的上层会学习更多的特定语境表征，从而更准确地预测特定语境下的下一个单词。在所有三个模型中，GPT-2的表征是最有语境针对性的，GPT-2最后一层的表征几乎是最大的语境针对性。\n停顿词（例如，\u0026ldquo;the\u0026rdquo;、\u0026ldquo;of\u0026rdquo;、\u0026ldquo;to\u0026rdquo;）是最有语境针对性的表述之一。 在所有的层中，停顿词的自我相似度是所有词中最低的，这意味着它们的上下文表征是最有背景的。例如，在ELMo的各层中，平均自我相似度最低的词是 \u0026ldquo;and\u0026rdquo;、\u0026ldquo;of\u0026rdquo;、\u0026ldquo;s\u0026rdquo;、\u0026ldquo;the \u0026ldquo;和 \u0026ldquo;to\u0026rdquo;。鉴于这些词不是多义词，这相对来说是令人惊讶的。这一发现表明，一个词所出现的各种语境，而不是其固有的多义性，是推动其语境化表征变化的原因。这回答了我们在介绍中提出的一个问题：ELMo、BERT和GPT-2并不是简单地将有限数量的词义表征中的一个分配给每个词；否则，在具有如此少的词义的词的表征中就不会有如此多的变化。\n上下文的特定性在ELMo、BERT和GPT-2中表现得非常不同。 如前所述，在ELMo、BERT和GPT2的上层，情境化的表征更具情境特异性。然而，这种增加的语境特异性在向量空间中是如何体现的？同一句子中的单词表征是否会收敛到一个点上，或者它们在与其他语境中的表征不同的同时，仍然保持着彼此的不同？为了回答这个问题，我们可以测量一个句子的句内相似度。从定义2中可以看出，在给定模型的某一层中，一个句子的句内相似性是其每个词表征与它们的平均值之间的平均余弦相似度，并根据各向异性进行调整。在图3中，我们绘制了500个均匀随机抽样的句子的平均句内相似度。\n在ELMo中，同一句子中的单词在上层的相似度更高。 随着一个句子中的单词表征在上层变得更加具有语境针对性，句子内的相似度也会上升。这表明，在实践中，ELMo最终将Firth（1957）的分布假说背后的直觉延伸到了句子层面：因为同一句子中的词有相同的语境，它们的语境化表示也应该是相似的。\n在BERT中，同一句子中的单词在上层的相异性更大。 随着一个句子中的单词表征在上层变得更有语境针对性，它们会彼此渐行渐远，尽管也有例外（见图3中第12层）。然而，在所有层中，同一句子中的单词之间的平均相似度仍然大于随机选择的单词之间的平均相似度（即各向异性基线）。这表明，与ELMo相比，BERT有一个更细微的上下文，它认识到，虽然周围的句子告知了一个词的含义，但同一句子中的两个词不一定有相似的含义，因为它们有相同的背景。\n在GPT-2中，同一句子中的单词表征与随机抽样的单词相比，彼此之间的相似度不高。 平均而言，未经调整的句内相似度与各向异性基线大致相同，因此从图3中可以看出，各向异性调整后的句内相似度在GPT-2的大多数层中接近于0。事实上，句内相似度在输入层中是最高的，该层完全不对词进行上下文处理。这与ELMo和BERT形成鲜明对比，在这两个机构中，除了一个层之外，其他层的平均句内相似度都高于0.20。\n正如前面讨论BERT时指出的，这种行为仍然具有直观的意义：同一个句子中的两个词不一定有相似的意义，仅仅是因为它们有相同的上下文。GPT-2的成功表明，与各向异性不同的是，在所有三个模型中，各向异性都伴随着语境特异性，句子内的高相似性并不是语境化所固有的。同一句子中的词可以有高度的语境化表征，而这些表征并不比两个随机的词表征更相似。然而，目前还不清楚这些句子内相似性的差异是否可以追溯到模型结构的差异；我们把这个问题留给未来的工作。\n图4：一个词的最大可解释方差（MEV）是其上下文表征中可由其第一主成分解释的方差比例（见定义3）。上面，我们绘制了各向异性调整后的均匀随机抽样词的平均MEV。在任何模型的任何一层中，静态嵌入都不能解释一个词的上下文表征中超过5%的方差。\n表1：各种静态嵌入在单词嵌入基准任务中的表现。每项任务的最佳结果用粗体表示。对于上下文模型（ELMo，BERT，GPT-2），我们使用一个词在某一层的上下文表征的第一主成分作为其静态嵌入。使用ELMo和BERT的语境化表征创建的静态嵌入往往比GloVe和FastText向量的表现更好。\n4.3 Static vs. Contextualized 平均来说，一个词的上下文表征中只有不到5%的方差可以由静态嵌入来解释。 从定义3中可以看出，对于给定模型的某一层，一个词的最大可解释方差（MEV）是其上下文表征中可由其第一主成分解释的方差比例。这为我们提供了一个关于静态嵌入能多好地取代一个词的上下文表征的上限。因为上下文表征是各向异性的（见第4.1节），所有词的大部分变化都可以由一个单一的向量来解释。我们通过计算统一随机抽样的单词表征的第一主成分所解释的方差比例，并从原始MEV中减去这一比例来调整各向异性。在图4中，我们绘制了各向异性调整后的平均MEV，横跨均匀随机抽样的词。\n在ELMo、BERT或GPT-2的任何一层中，平均来说，一个词的上下文表征中的方差都不能被静态嵌入所解释，而超过5%。虽然在图4中看不到，但许多词的原始MEV实际上低于各向异性基线：也就是说，所有词的方差中更大的比例可以由一个单一的向量来解释，而不是一个词的所有表征的方差。请注意，5%的阈值代表了最好的情况，理论上不能保证使用GloVe获得的词向量，例如，与最大限度提高MEV的静态嵌入相似。这表明，语境化模型并不是简单地将有限数量的词义表征中的一个分配给每个词\u0026ndash;否则，解释的方差比例会高得多。即使是ELMo和BERT的所有层的平均原始MEV也低于5%；只有GPT-2的原始MEV是不可忽略的，由于极高的各向异性，第2至11层的原始MEV平均为30%左右。\n在许多基准上，低层的上下文表征的主成分超过了GloVe和FastText。 如前所述，我们可以通过在给定层中提取其上下文表征的第一个主成分（PC）来为每个词创建静态嵌入。在表1中，我们绘制了这些PC静态嵌入在几个基准任务中的表现2。这些任务包括语义相似性、类比解决和概念分类：SimLex999（Hill等人，2015）、MEN（Bruni等人，2014）、WS353（Finkelstein等人，2002）、RW（Luong等人，2013）、SemEval-2012（Jurgens等人，2012）、Google类比解决（Mikolov等人，2013a）MSR类比解决（Mikolov等人，2013b）、BLESS（Baroni和Lenci，2011）以及AP（Almuhareb和Poesio，2004）。我们在表1中不考虑第3-10层，因为它们的性能介于第2和11层之间。\n表现最好的PC静态嵌入属于BERT的第一层，尽管BERT和ELMo的其他层的PC静态嵌入在大多数基准上也优于GloVe和FastText。对于所有三个上下文模型，从低层创建的PC静态嵌入比从高层创建的更有效。使用GPT-2创建的静态嵌入也明显比来自ELMo和BERT的静态嵌入表现差。鉴于上层比下层更具有语境特异性，而且GPT2的表征比ELMo和BERT的更具有语境特异性（见图2），这表明高度语境特异性表征的PC在传统基准上的效果较差。那些从较少的上下文特定表征中得出的表征，如来自BERT第1层的表征，要有效得多。\n5 Future Work 我们的发现为未来的工作提供了一些新的方向。首先，正如本文前面所指出的，Mu等人（2018）发现，使静态嵌入更加各向同性\u0026ndash;通过从每个嵌入中减去其平均值\u0026ndash;导致下游任务的性能有惊人的改善。 鉴于各向同性对静态嵌入有好处，它可能对上下文词表征也有好处，尽管后者已经产生了重大改善，尽管是高度各向异性。因此，在语言建模的目标中加入各向异性的惩罚\u0026ndash;鼓励上下文表征更加各向同性\u0026ndash;可能会产生更好的结果。\n未来工作的另一个方向是从语境化的词生成静态的词表示。虽然后者提供了卓越的性能，但在生产中部署大型模型，如BERT，往往存在内存和运行时间方面的挑战。我们在第4.3节中的工作表明，不仅有可能从上下文模型中提取静态表示，而且这些提取的向量与传统的静态嵌入（如GloVe和FastText）相比，在一系列不同的任务中通常表现得更好。这可能是一种从语境化模型中提取一些用途的手段，而不需要承担在生产中使用它们的全部成本。\n6 Conclusion 在本文中，我们调查了上下文语境下的单词表征到底是怎样的。其一，我们发现ELMo、BERT和GPT-2的上层比下层产生更多的上下文特定表征。这种语境特异性的增加总是伴随着各向异性的增加。然而，语境特异性在三个模型中的表现也不同；各向异性调整后的同一句子中单词之间的相似性在ELMo中最高，但在GPT-2中几乎不存在。我们最终发现，在对各向异性进行调整后，平均来说，一个词的上下文表征中只有不到5%的方差可以用静态嵌入来解释。这意味着，即使在最好的情况下，在所有模型的所有层中，静态的单词嵌入也不能很好地替代上下文的嵌入。这些见解有助于解释上下文表征在一系列不同的NLP任务中所取得的显著成功。\n","permalink":"https://lin-roger.github.io/posts/howcontextualarecontextualizedwordrepresentationscomparingthegeometryofbertelmoandgpt2embeddings/","summary":"🚧目前進度:deepl全文翻譯+重點標記+提問🚧","title":"🚧How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings🚧"},{"content":"Abstract BERT 等預訓練模型在許多自然語言處理任務中取得了巨大成功。然而，如何通過這些預訓練模型獲得更好的句表徵向量仍然值得探索。先前的工作表明，各向異性問題是基於 BERT 的句表徵的關鍵瓶頸，阻礙模型充分利用底層語義特徵。因此，一些增強句子分佈各向同性的嘗試，例如基於流的模型(BERT-flow)，已被應用於句表徵並取得了一些改進。在本文中，我們發現傳統機器學習中的白化(球化)變換同樣可以增強句表徵的各向同性並取得有競爭力的結果。此外，白化還能夠降低句子表示的維度。我們的實驗結果表明，它不僅可以實現良好的性能，而且可以顯著降低存儲成本並加快模型檢索速度。\nIntroduction 深度神經語言模型的應用(BERT、GPT)近年來取得了巨大成功，因為它們創造了對隨前後文語境改變的詞彙表徵向量。這一趨勢也刺激了為長文本產生語義崁入(句崁入向量、段落崁入向量)的研究進展。然而句崁入已被證明不能完整捕捉句子的基本語義。據先前研究(Gao et al., 2019; Ethayarajh, 2019; Li et al., 2020)，所有的詞語的表徵向量都不是各項同性的：它們在方向上並非均勻分布的，而是在崁入空間中佔據一個狹小的錐體，呈現各向異性。Ethayarajh, 2019; 證明，取自預訓練模型的崁入向量的極度的各向異性，以致任兩個詞的崁入向量COS相似度平均為0.99(極度相似)。Li et al., 2020進一步調查發現，BERT的句崁入空間存在兩問題:\n詞頻令崁入空間產生偏移：高頻詞距離原點較近，低頻詞距離原點較遠。在相同權重下，低頻詞對整體句崁入向量的遠比高頻詞影響更強 低頻詞分散且稀疏：低頻詞在嵌入空間中與其他詞的最近距離(L2-dist)相較高頻詞更遠，使低頻的同義詞相似性更低 ","permalink":"https://lin-roger.github.io/posts/whiteningsentencerepresentationsforbettersemanticsandfasterretrieval/","summary":"🚧還在寫喔!!語意相似度計算🚧","title":"🚧Whitening Sentence Representations for Better Semantics and Faster Retrieval🚧"},{"content":"Zero-Shot Prompting \u0026amp; Few(N)-Shot Prompting How it work Shot意指範例，顧名思義，Zero-Shot就是提示不包含任何範例，反之N-Shot就是提示包含N個範例，在簡單任務中您使用Zero-Shot Prompting通常也可獲得不錯的效果，但當問題開始複雜起來時，Few-Shot Prompting 可以更好的解決問題\nThe model has somehow learned how to perform the task by providing it with just one example (i.e., 1-shot). For more difficult tasks, we can experiment with increasing the demonstrations (e.g., 3-shot, 5-shot, 10-shot, etc.).\n模型通過僅提供一個示例（即 1-shot）以某種方式學會瞭如何執行任務。對於更困難的任務，我們可以嘗試增加演示次數（例如 3 次、5 次、10 次等）。\nFollowing the findings from Min et al. (2022), here are a few more tips about demonstrations/exemplars when doing few-shot:\n根據 Min 等人的研究結果，這裡有一些關於進行小樣本時演示/範例的更多提示：\n\u0026ldquo;the label space and the distribution of the input text specified by the demonstrations are both important (regardless of whether the labels are correct for individual inputs)\u0026rdquo;\n“演示指定的標籤空間和輸入文本的分佈都很重要（無論標籤對於各個輸入是否正確）”\nthe format you use also plays a key role in performance, even if you just use random labels, this is much better than no labels at all.\n您使用的格式對性能也起著關鍵作用，即使您只是使用隨機標籤，這也比根本沒有標籤要好得多。\nadditional results show that selecting random labels from a true distribution of labels (instead of a uniform distribution) also helps.\n其他結果表明，從真實的標籤分佈（而不是均勻分佈）中選擇隨機標籤也有幫助。\nLimitations 當問題牽扯到具有多步驟的複雜推裡時，Few(N)-Shot 的效果會減弱，這時將指令拆解會是更好的選擇\nChain-of-Thought Prompting Self-Consistency Generated Knowledge Prompting Tree of Thoughts (ToT) Retrieval Augmented Generation (RAG) Automatic Reasoning and Tool-use (ART) Automatic Prompt Engineer (APE) Active-Prompt Directional Stimulus Prompting ReAct Prompting Multimodal CoT Prompting Graph Prompts ","permalink":"https://lin-roger.github.io/posts/advancedpromptingengineering/","summary":"🚧目前進度:deepl部分翻譯🚧","title":"🚧Advanced Prompting Engineering 進階提示工程🚧"},{"content":"Base LLM \u0026amp; Instruction Tuned LLM Base LLM 使用大量從網路爬取的資料訓練，輸入一文本後預測出最有可能接續在該文本後的下一個字(token)，所以給定一文章的前半部分其可將其後半部分生成。問題 -\u0026gt; 最有可能接續的字詞和人類預想的相同嗎?假設以下為您的訓練資料\n常見問題:\nQ1:為什麼同樣行程不同旅行社價格會不一樣？\nQ2:行程上有的景點，是不是一定都會走到？\nQ3:為什麼我和朋友的座位沒有安排在一起？\nA1:其實旅遊產品牽涉範圍極廣\u0026hellip;\nA2:旅客在報名旅遊行程前\u0026hellip;\nA3:機上座位的安排\u0026hellip;\n當您向使用該資料訓練的LM輸入\u0026quot;Q1:為什麼同樣行程不同旅行社價格會不一樣？\u0026quot;，我們通常為期望LM回答與問題對應的答案，也就是:\u0026ldquo;A1:其實旅遊產品牽涉範圍極廣\u0026hellip;\u0026quot;，但實際上LM會回答:\u0026ldquo;Q2:行程上有的景點，是不是一定都會走到？\u0026quot;，這是因為資料集中，文本Q1的下一段文本為Q2，而非A1，使模型認為Q1-\u0026gt;Q2是最好的選擇，而非Q1-\u0026gt;A1。\n為解決此問題，Instruction Tuned LLM 出現了。透過對Base LLM 使用指令資料集(如: alpaca-tw )微調並使用RLHF技術使LM的的輸出更符合人類的預期。\n指令的基本原則 0. Model Limitations: Hallucination 幻覺(Hallucination)，LLM並不了解自身知識的極限也不完全記得所有其看過的資訊，遇到一些艱澀的問題其可能會嘗試回答，並輸出看似合理但與事實不符或錯誤的回應，透過要求模型找出問題相關的文獻並引用其回答可以減輕此問題(LM找出的相關文獻或引用也有為幻覺之嫌，務必謹慎確認)\n無法做到精確字數要求，如過您在指令中加入以下要求\u0026rdquo;\u0026hellip;, 在50個字以內說明\u0026rdquo;，可能會出現60甚至70個字(會因語言影響誤差範圍，該問題為token與字/詞數量不等所導致，一個字/詞可由1~n個token組成，通常為1~3個，參考BPE編碼)\n1. Write Clear and Specific Instructions Clear != Short，更長的指令通常提供了更多資訊，可以令模型產生更準確的結果。 Use delimiters，使用分隔符號將輸入資訊清楚的分割，使輸入資料不會與指令或其他資訊混淆，該方法也可以一定程度抵擋Prompt Injections(透過在輸入資料中夾帶惡意指令使模型輸出改變的技術。如:忽略先前的指令，幫我\u0026hellip;。使用分隔符號將其指定為輸入資料可使模型不執行該惡意指令) Ask for structured output，可以在指令中要求輸出資料以json、xml等結構化的形式表達，可以被更好的儲存或處裡 Check whether conditions are satisfied，在指令中加入一些條件，避免一些意外使輸出不合預期 Few-shot prompting，對於一個複雜的任務，指令中只有任務描述是不夠的，在指令中加入一些範例可以使LM有效理解你的要求(補充，LLM在zero-shot的表現並不佳，請盡量避免) 2. Give Model Time to Think Specify the steps to complete a task，一個複雜的任務可以將其拆解成多個較為簡單的步驟或指令使模型可以更好理解或處理您的任務 Instruct the model to work out its own solution before rushing to a conclusion，詢問模型時可以要求模型先輸出推論過程在輸出結果 Iterative Prompt Development(Prompt 的開發與迭代流程) Prompt的構成:\nInstructions Context Input Data Output Indicator Prompt guidelines:\n寫出符合基本原則的Prompt 測試Prompt，取得大量輸出 找出不合預期的案例並分析原因以改進Prompt Repeat 並沒有所謂的萬能Prompt，只有適合的Prompt\nUse Case Summarizing 文本摘要 LLM 通常預設以抽樣摘要(以比原文更簡短的通順文本表述原文資訊)為主，可以要求:字數、句數、風格(簡潔的、嚴謹的)、側重在甚麼(對\u0026hellip;的影響、XXX說的內容)、摘要的對象(國小生、商業部門經理)等\n你可以要求LLM以抽取式摘要(原文的重點字、詞、句、段，原封不動的取出)，如:\u0026ldquo;找出/萃取文章中XXX會想知道的關鍵字/資訊/句子\u0026rdquo;，取得關鍵資訊避免攏言贅詞並減少幻覺產生的可能。\nQuestion Answering 問答 Prompt:\nAnswer the question based on the context below. Keep the answer short and concise. Respond \u0026ldquo;Unsure about answer\u0026rdquo; if not sure about the answer.\nKeep the answer short and concise: 指定風格 Respond \u0026ldquo;Unsure about answer\u0026rdquo; if not sure about the answer: 1.3-Check whether conditions are satisfied Text Classification 文本分類 Classify the text into neutral, negative or positive and follow the output format.\noutput format:\u0026ldquo;Class\u0026rdquo;\nText: I think the vacation is okay.\nSentiment: \u0026ldquo;Neutral\u0026rdquo;\nText: I think the food was okay. Sentiment:\n","permalink":"https://lin-roger.github.io/posts/promptee/","summary":"簡單介紹prompt-engineering","title":"prompt-engineering 基礎"},{"content":"摘要 SVM被公認是許多任務中效果最好的分類方法之一，SVM的學習能力和訓練計算複雜度與特徵空間維度無關，但在文本分類任務中，降低複雜度是有效處裡大量詞語的一個要點。該論文採用新的降維方法降低文檔向量的維度。還為基於中心的分類算法和SVM分類器引入決策函數，處理一個文檔可能屬於多個class的問題。分析大量的實驗結果表明使用為聚類資料設計的降維算法，使輸入維度降低，可在不犧牲預測精度的情況下取得更好的訓練效率。\n簡介 文本分類是一項監督是任務，將文本分類到預定義的class，用來從大樣文本中尋找有價值的資訊，base on Vctor Space的方法有以下特性，input高維且稀疏(one hot, bag of word)，線性可分性(存在超平面將資料分割)，少數特徵不相關(多數相關)。有人猜測，積極降維會導致資訊嚴重損失，導致分類效果不佳。\n給定訓練資料:\n$$(x_i, y_i)$$\n$$1\\le y_i \\le 1$$\n$$1\\le i\\le n$$\n具K, C的soft margin SVM之對偶式(Loss(target) funtion, Constraint(條件、約束))為:\n$\\max_{\\alpha_i} \\sum_{i=1}^{n} \\alpha i -\\frac{1}{2}\\sum{i,j=1}^{n} \\alpha_i \\alpha_j y_i y_jK(\\mathrm {x}_i, \\mathrm {x}_j),$\n$s.t. \\sum_{i=1}^{n} \\alpha_iy_i=0, 0\\le \\alpha_i\\le C, i=1,\\dots ,n.$\nK is (\u0026lt;, \u0026gt; 矩陣乘法，處裡非線性可分, $\\phi$是一個mapping)\n$K(\\mathrm {x}_i, \\mathrm {x}_j) = \u0026lt; \\phi(\\mathrm {x}_i), \\phi(\\mathrm {x}_j)\u0026gt;$\n如上式所述，SVM的複雜度取決於訓練樣本數，為$O(n)$。並且因K函數的使用不受dim of feature space影響，然而K函數的計算複雜性被忽略了其取決於dim of input space，就算在最佳分割超平面的情況K函數的計算複雜性也無法被省略。因此降維必定可以使訓練SVM和預測帶來更高效率。\n該論文假設文件集合表示為Document-Term Matrix(Bag of Word)，加權兩倍，假設資料的聚類已經進行。\n下一章回顧LSI，使用svd分解做a的低秩近似，但忽略了資料的聚類結構。第三節中，回顧幾種對聚類資料特別有效的降維演算法:兩種群中心方法和使用GSVD(廣義奇異值分解)的泛化LDA。通過降維SVM()和K-mens(計算向量對距離)等分類器的計算複雜度皆可大降低。\n多數文本資料集中，文本可被分入多個類，為更有效處理此問題，在第四節中介紹基於閥值的分類算法擴展，實驗表明，該論文提出的cluster preserving降維演算法沒有造成訊息損失，反而提升了分類器的預測精度(推斷具有去噪效果)。\n低秩近似使用隱含語意索引 LSI假設:Document-Term Matrix中存在隱含的語意結構，其被文件中出現各種詞所破壞(polysemy and synonymy)。基本概念:若兩個文檔向量代表同一主題，其會共享許多與關鍵詞相關的關聯詞，通過SVD其語意結構會十分接近(term vectors表示為左奇異向量document vectors表示為右奇異向量)。然而，LSI再降維時忽略了聚類結構，且並沒有理論最佳的參數選擇，需多次實驗來確定最佳維度(如第五章所述)。實驗結果證實，當資料已被聚類時，下節介紹的降維法對新資料的分類效果更好。\n聚類資料的降維演算法 為提升高維度資料的處裡效率，須將資料降維，此節回顧三種保留聚類結構的降維算法\nCentroid-based Algorithms for Dimension Reduction of Clustered Data 給定一Document-Term Matrix，找一變換映射每個document vector從m維空間降到l維空間(m\u0026gt;l)，兩種方法:\n線性轉換($G^T_{l\\times m}$) 低秩近似(分解為兩個矩陣) $A\\approx BY$\n只要計算給定資料的降維表示，就無須從B計算降維變換G，若確定矩陣B，Y即可用最小平方法求解。\n$\\min_{B,Y}\\left|BY-A\\right|_{F.}$\n給定任意文本$q\\in\\mathbb{R}^{m\\times1}$透過解最小化問題轉換到低維空間。\n$\\min_{\\hat{q}\\in\\mathbb{R}^{l\\times1}}\\left| B\\hat{q}-q\\right|_{2.}$\n在Centroid降維法中(A1)，B的第$i, (1\\le i\\le p)$列是第$i$個群的中心(均值中心)點向量，任一向量$q$，可在$p$維空間表示成$\\hat{q}$即為最小平方法的解。\n在Orthogonal Centroid演算法中(A2)，使用$p$維表示資料向量$q\\in \\mathbb{R}^{m\\times 1}$被給定為$\\hat{q}=Q_{p}^{T}q$，$Q_p$為$B$的正交基底(QR分解)。\n以上兩種等Centroid-based降維法在計算成本比LSI更低，且在聚類資料下的效果更好。雖然此方案只能在線性可分的資料使用，但文本資料扔然可用，因文本資料通常是線性可分的(非線性可分in 18)\nGeneralized Discriminant Analysis based on the Generalized Singular Value Decomposition 近期(2003)，出現了GSVD base 的 cluster-preserving降維法，使SVM泛化到高維空間資料。\n經典判別分析透過最大化聚類之間的散度和最小化即群內的散度，維持聚類結構。為此，其定義聚類內散度矩陣$S_w$和聚類間散度矩陣$S_b$，$N_i$表示集群$i$的列索引集合$n_i$表示集群$i$的列數，$C$表全域中心點。目標使群內散度最小化，和降維後群間散度最大化。再次請出$G^T\\in \\mathbb{R}^{l\\times m}$將A的每列m維向量映射到l維的變換，目標表示為最小化$trace(G^T S_wG)$和最大化$trace(G^T S_bG)$。\n當$S_w$可逆(nonsingular、invertible)，可視為解最大化問題。 全局最大成立於:$G$的列是$S_{w}^{-1}S_b$的特徵向量並對應於$l$個最大特徵值。 when $l\\le p-1$ is equals $λ_1 + \\dots +λ_{p−1},$ each $λ_i ≥ 0$。設Document-Term Matrix $A$被分為$A=[A_1,\\ \\dots , \\ A_p]$，$A_i \\in \\mathbb{R}^{m\\times n_i}\\ \\text{in cluster}\\ i$\n$H_w = [a_1-c_1, a_2-c_2,\\dots ,a_n-c_p]\\in \\mathbb{R}^{m\\times n}$\n$H_b = [\\sqrt{n_1}(c_1-c),\\dots ,\\sqrt{n_p}(c_p-c)]\\in \\mathbb{R}^{m\\times p}$\n$S_w=H_wH_w^T\\ \u0026amp; \\ S_b=H_bH_b^T$\n當詞數(terms) $m$ \u0026gt; 文本數(doc) $n$，$S_w$不可逆(singular)，經典SVM失效。將問題(特徵值) $S_w^{-1}S_b\\mathrm{x}_i = λ_i\\mathrm{x}^i$ 改寫為 $\\mathrm\\beta_i^2H_bH_b^T\\mathrm{x}_i = \\mathrm\\alpha_i^2H_wH_w^T\\mathrm{x}_i$即可透過GSVD處理(LDA/GSVD in A3)。其中最複雜的計算部分複合矩陣$H$的完全正交分解，當$\\max (p, n)\\ll m$，$H=[H_b^T,H_w^T]\\in \\mathbb{R}^{(p+n)\\times m}$ 的SVD分解可被計算為:\n計算$H_t$的QR分解$Q_HR_H$ 計算$R_H\\in \\mathbb{R}^{(p+n)\\times (p+n)}$ 的SVD分解 $=Z\\begin{pmatrix} \\sum_H \u0026amp; 0 \\ 0 \u0026amp; 0 \\end{pmatrix}P^T$，使$H=R_H^TQ_H^T=P\\begin{pmatrix} \\sum_H \u0026amp; 0 \\ 0 \u0026amp; 0 \\end{pmatrix}Z^TQ_H^T$。其中$Q_HZ\\in\\mathbb{R}^{m\\times (p+n)}$的列之間為標準正交的(內積0、距離1)，存在正交$Q\\in \\mathbb{R} ^{m\\times m}$其前$p+n$列等同$Q_HZ$。將式整理為：\n$H = P\\begin{pmatrix} \\sum_H \u0026amp; 0 \\ 0 \u0026amp; 0 \\end{pmatrix}Q^T$\n式中$\\sum_H$的右邊有$m-t$個0列，因$R_H\\in \\mathbb{R}^{(p+n)\\times (p+n)}$遠小於$H\\in \\mathbb{R}^{(p+n)\\times m}$，記憶體需求大減，複雜度也降至$O(mn^2)+O(n^3)$ 分類方法 為測試降維效果，使用三種分類器測試:中心點分類、kNN和SVMs。皆引入閥值進行修改，以確保文本被判有多重類別資格時可以正確分類。\nCentroid-based 設新的文本資料為$q$，訓練資料共有$p$個群，$c_i$為第$i$個群的中心點向量:\n$arg \\max_{1\\le i\\le p} \\frac{q^Tc_i}{|q|_2 |c_i|_2}$\n多類別擴展($\\theta$為閥值):\n$y(\\mathrm{x}, j) = \\text{sign}{ sim(\\mathrm{x}, \\mathrm{c}_i)-\\theta_j^c }$\n$y(\\mathrm{x}, j)\\in { +1,-1 }$\n$$ \\begin{cases} \\text{Class is j} \u0026amp; \\text{:} \u0026amp; y(\\mathrm{x}, j)\u0026gt;0 \\ \\text{Class is not j} \u0026amp; \\text{:} \u0026amp; y(\\mathrm{x}, j)\\le 0 \\end{cases} $$\nk-Nearest Neighbor 設新的文本資料為$q$，訓練文本資料共有$p$個群:\n在訓練資料中，使用餘弦相似度計算與$q$最近的k個文本向量 在這k個向量中，計算屬於各個群的數量，$q$將被分配到最多的那個。 多類別擴展($\\theta$為閥值，$kNN$為文本$x$的$k$個鄰近向量集合):\n$y(\\mathrm{x}, j) = \\text{sign}{ \\sum_{\\mathrm{d}_i\\in kNN} sim(\\mathrm{x}, \\mathrm{d}_i) y(\\mathrm{d}_i, j) -\\theta_j^{kNN} }$\nSVM OvR策略(為每個Class建一個分類器)的二元分類器的最佳分割超平面可透conventional SVM取得。引入多類別擴展:\n$$ y(\\mathrm{x}, j) = \\text{sign}{ \\sum_{\\mathrm{x}_i\\in SV} \\alpha_i y_i K(\\mathrm{x}, \\mathrm{x}_i)+ b -\\theta_j^{SVM} }\\ K=\u0026lt;\\mathrm{x}, \\mathrm{x}_i\u0026gt; \\ K=[\u0026lt;\\mathrm{x}, \\mathrm{x}_i\u0026gt;+1]^d \\ K=\\exp(-\\gamma|\\mathrm{x}, \\mathrm{x}_i|^2) $$\n$SV$為支援向量的集合，$\\theta$為閥值，$\\gamma$與高斯函數寬度成反比。\n實驗結果 預測結果包含:\n無降維 LSI/SVD Centroid Orthogonal Centroid LDA/GSVD SVM優化:\n正則參數$C$ polynomial 角度$d$ Gaussian RBF $\\gamma$ 資料集 subset of MEDLINE database: 5個class、每個class各有500份文本、每份文本只有一個class、train:test = 50:50、做詞型還原和處理剔除字(此為國家教育研究院翻譯版，俗稱為停用詞)後訓練集有22095個不重複的詞。 Reuter-21578 文本集的\u0026quot;ModApte\u0026quot;分割: 90個class、每份文本可能有多個class、每個class至少有一個train和一個test、共7769個train和3019個tess、做詞型還原和處理剔除字(此為國家教育研究院翻譯版，俗稱為停用詞)後訓練集有11941個不重複的詞、引入閥值模型。 DTM不用BoW，改用TF-IDF並歸一化。\n表一 對MEDLINE資料集使用LSI/SVD，用使用centroid-based, kNN和SVMs分類器分類的結果。觀察到:\nkNN使用L2 norm在$l$為100-500時效果不佳，與餘弦相似性在未正則化資料表現更好的印象相符，且5NN明顯落後其他更高的K，表明k=5太小。 表二 SVM使用不同的K(核)函數與降維演算法在MEDLINE資料集的結果。觀察到:\n降維後的預測結果與原始空間的預測結果相似且複雜度降低。 正交中心降維演算法對K函數的選擇不敏感，可以選複雜度更低的線性K函數。 表三 選擇不同分類演算法與降維演算法在MEDLINE資料集的結果。觀察到:\n使用LDA/GSVD降維演算法時，使用餘弦相似度的centroid-based與kNN分類演算法效果較差，而使用L2-norm的效果較好，因跡最佳化使用L2-norm表示。 因LDA/GSVD最小化群內散點的跡(距離)，自同一(相似)類別的文檔向量會被變換為一個緻密的群甚至一個點，使得SVM難以找到泛化性高的超平面。 表四 MEDLINE資料集中使用SVM配合不同降維演算法在5個不同類別中的準確率。觀察到:\ncolon cancer與oral cancer難以被區分。 表五 選擇不同分類演算法與降維演算法(Centroid、Orthogonal Centroid)在REUTERS資料集的結果。觀察到:\nOrthogonal Centroid 效果無明顯下降，Centroid則明顯降低，推測因將各個聚類中心映射至單位矩陣導致每個class間的資訊消失所造成。 結論與討論 本文使用三種降維方法Centroid、Orthogonal Centroid和LDA/GSVD，皆是為集群資料所設計，做為比較也使用了LSI/SVD這種不保留集群結構的降維演算法。其也測試了三種不同的分類演算法SVMs、kNN與centroid-based classification測試在不同降維法中的分類效果。測試結果取得了高的準度，即使在強力的降維下，也可與未降維的狀態近似。\n該論文還引入了基於閥值的分類器，用於centroid-based和SVM做到一對多的分類。centroid在Class互不關聯的情況下表現更好。\n結論 不犧牲精度的情況下，可以做到對文本大幅降為。Orthogonal Centroid在保留聚類結構的能力做的極佳，降為前後的預測準確率幾乎不變，在KNN或SVM使用可以極大減少計算複雜度。\n","permalink":"https://lin-roger.github.io/posts/dimensionreductionintextclassificationwithsupportvectormachines/","summary":"非經典古文，可以不用看，主要是我的筆記。SVM被公認是許多任務中效果最好的分類方法之一，SVM的學習能力和訓練計算複雜度與特徵空間維度無關，但在文本分類任務中，降低複雜度是有效處裡大量詞語的一個要點。該論文採用新的降維方法降低文檔向量的維度。還為基於中心的分類算法和SVM分類器引入決策函數，處理一個文檔可能屬於多個class的問題。分析大量的實驗結果表明使用為聚類資料設計的降維算法，使輸入維度降低，可在不犧牲預測精度的情況下取得更好的訓練效率。","title":"Dimension Reduction in Text Classification with Support Vector Machines"}]