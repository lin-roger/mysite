[{"content":"Zero-Shot Prompting \u0026amp; Few(N)-Shot Prompting How it work Shot意指範例，顧名思義，Zero-Shot就是提示不包含任何範例，反之N-Shot就是提示包含N個範例，在簡單任務中您使用Zero-Shot Prompting通常也可獲得不錯的效果，但當問題開始複雜起來時，Few-Shot Prompting 可以更好的解決問題\nThe model has somehow learned how to perform the task by providing it with just one example (i.e., 1-shot). For more difficult tasks, we can experiment with increasing the demonstrations (e.g., 3-shot, 5-shot, 10-shot, etc.).\n模型通過僅提供一個示例（即 1-shot）以某種方式學會瞭如何執行任務。對於更困難的任務，我們可以嘗試增加演示次數（例如 3 次、5 次、10 次等）。\nFollowing the findings from Min et al. (2022), here are a few more tips about demonstrations/exemplars when doing few-shot:\n根據 Min 等人的研究結果，這裡有一些關於進行小樣本時演示/範例的更多提示：\n\u0026ldquo;the label space and the distribution of the input text specified by the demonstrations are both important (regardless of whether the labels are correct for individual inputs)\u0026rdquo;\n“演示指定的標籤空間和輸入文本的分佈都很重要（無論標籤對於各個輸入是否正確）”\nthe format you use also plays a key role in performance, even if you just use random labels, this is much better than no labels at all.\n您使用的格式對性能也起著關鍵作用，即使您只是使用隨機標籤，這也比根本沒有標籤要好得多。\nadditional results show that selecting random labels from a true distribution of labels (instead of a uniform distribution) also helps.\n其他結果表明，從真實的標籤分佈（而不是均勻分佈）中選擇隨機標籤也有幫助。\nLimitations 當問題牽扯到具有多步驟的複雜推裡時，Few(N)-Shot 的效果會減弱，這時將指令拆解會是更好的選擇\nChain-of-Thought Prompting Self-Consistency Generated Knowledge Prompting Tree of Thoughts (ToT) Retrieval Augmented Generation (RAG) Automatic Reasoning and Tool-use (ART) Automatic Prompt Engineer (APE) Active-Prompt Directional Stimulus Prompting ReAct Prompting Multimodal CoT Prompting Graph Prompts ","permalink":"https://lin-roger.github.io/posts/advancedpromptingengineering/","summary":"🚧還在寫喔!!🚧","title":"🚧Advanced Prompting Engineering 進階提示工程🚧"},{"content":"Base LLM \u0026amp; Instruction Tuned LLM Base LLM 使用大量從網路爬取的資料訓練，輸入一文本後預測出最有可能接續在該文本後的下一個字(token)，所以給定一文章的前半部分其可將其後半部分生成。問題 -\u0026gt; 最有可能接續的字詞和人類預想的相同嗎?假設以下為您的訓練資料\n常見問題:\nQ1:為什麼同樣行程不同旅行社價格會不一樣？\nQ2:行程上有的景點，是不是一定都會走到？\nQ3:為什麼我和朋友的座位沒有安排在一起？\nA1:其實旅遊產品牽涉範圍極廣\u0026hellip;\nA2:旅客在報名旅遊行程前\u0026hellip;\nA3:機上座位的安排\u0026hellip;\n當您向使用該資料訓練的LM輸入\u0026quot;Q1:為什麼同樣行程不同旅行社價格會不一樣？\u0026quot;，我們通常為期望LM回答與問題對應的答案，也就是:\u0026ldquo;A1:其實旅遊產品牽涉範圍極廣\u0026hellip;\u0026quot;，但實際上LM會回答:\u0026ldquo;Q2:行程上有的景點，是不是一定都會走到？\u0026quot;，這是因為資料集中，文本Q1的下一段文本為Q2，而非A1，使模型認為Q1-\u0026gt;Q2是最好的選擇，而非Q1-\u0026gt;A1。\n為解決此問題，Instruction Tuned LLM 出現了。透過對Base LLM 使用指令資料集(如: alpaca-tw )微調並使用RLHF技術使LM的的輸出更符合人類的預期。\n指令的基本原則 0. Model Limitations: Hallucination 幻覺(Hallucination)，LLM並不了解自身知識的極限也不完全記得所有其看過的資訊，遇到一些艱澀的問題其可能會嘗試回答，並輸出看似合理但與事實不符或錯誤的回應，透過要求模型找出問題相關的文獻並引用其回答可以減輕此問題(LM找出的相關文獻或引用也有為幻覺之嫌，務必謹慎確認)\n無法做到精確字數要求，如過您在指令中加入以下要求\u0026rdquo;\u0026hellip;, 在50個字以內說明\u0026rdquo;，可能會出現60甚至70個字(會因語言影響誤差範圍，該問題為token與字/詞數量不等所導致，一個字/詞可由1~n個token組成，通常為1~3個，參考BPE編碼)\n1. Write Clear and Specific Instructions Clear != Short，更長的指令通常提供了更多資訊，可以令模型產生更準確的結果。 Use delimiters，使用分隔符號將輸入資訊清楚的分割，使輸入資料不會與指令或其他資訊混淆，該方法也可以一定程度抵擋Prompt Injections(透過在輸入資料中夾帶惡意指令使模型輸出改變的技術。如:忽略先前的指令，幫我\u0026hellip;。使用分隔符號將其指定為輸入資料可使模型不執行該惡意指令) Ask for structured output，可以在指令中要求輸出資料以json、xml等結構化的形式表達，可以被更好的儲存或處裡 Check whether conditions are satisfied，在指令中加入一些條件，避免一些意外使輸出不合預期 Few-shot prompting，對於一個複雜的任務，指令中只有任務描述是不夠的，在指令中加入一些範例可以使LM有效理解你的要求(補充，LLM在zero-shot的表現並不佳，請盡量避免) 2. Give Model Time to Think Specify the steps to complete a task，一個複雜的任務可以將其拆解成多個較為簡單的步驟或指令使模型可以更好理解或處理您的任務 Instruct the model to work out its own solution before rushing to a conclusion，詢問模型時可以要求模型先輸出推論過程在輸出結果 Iterative Prompt Development(Prompt 的開發與迭代流程) Prompt的構成:\nInstructions Context Input Data Output Indicator Prompt guidelines:\n寫出符合基本原則的Prompt 測試Prompt，取得大量輸出 找出不合預期的案例並分析原因以改進Prompt Repeat 並沒有所謂的萬能Prompt，只有適合的Prompt\nUse Case Summarizing 文本摘要 LLM 通常預設以抽樣摘要(以比原文更簡短的通順文本表述原文資訊)為主，可以要求:字數、句數、風格(簡潔的、嚴謹的)、側重在甚麼(對\u0026hellip;的影響、XXX說的內容)、摘要的對象(國小生、商業部門經理)等\n你可以要求LLM以抽取式摘要(原文的重點字、詞、句、段，原封不動的取出)，如:\u0026ldquo;找出/萃取文章中XXX會想知道的關鍵字/資訊/句子\u0026rdquo;，取得關鍵資訊避免攏言贅詞並減少幻覺產生的可能。\nQuestion Answering 問答 Prompt:\nAnswer the question based on the context below. Keep the answer short and concise. Respond \u0026ldquo;Unsure about answer\u0026rdquo; if not sure about the answer.\nKeep the answer short and concise: 指定風格 Respond \u0026ldquo;Unsure about answer\u0026rdquo; if not sure about the answer: 1.3-Check whether conditions are satisfied Text Classification 文本分類 Classify the text into neutral, negative or positive and follow the output format.\noutput format:\u0026ldquo;Class\u0026rdquo;\nText: I think the vacation is okay.\nSentiment: \u0026ldquo;Neutral\u0026rdquo;\nText: I think the food was okay. Sentiment:\n","permalink":"https://lin-roger.github.io/posts/promptee/","summary":"簡單介紹prompt-engineering","title":"prompt-engineering 基礎"},{"content":"摘要 SVM被公認是許多任務中效果最好的分類方法之一，SVM的學習能力和訓練計算複雜度與特徵空間維度無關，但在文本分類任務中，降低複雜度是有效處裡大量詞語的一個要點。該論文採用新的降維方法降低文檔向量的維度。還為基於中心的分類算法和SVM分類器引入決策函數，處理一個文檔可能屬於多個class的問題。分析大量的實驗結果表明使用為聚類資料設計的降維算法，使輸入維度降低，可在不犧牲預測精度的情況下取得更好的訓練效率。\n簡介 文本分類是一項監督是任務，將文本分類到預定義的class，用來從大樣文本中尋找有價值的資訊，base on Vctor Space的方法有以下特性，input高維且稀疏(one hot, bag of word)，線性可分性(存在超平面將資料分割)，少數特徵不相關(多數相關)。有人猜測，積極降維會導致資訊嚴重損失，導致分類效果不佳。\n給定訓練資料:\n$$(x_i, y_i)$$\n$$1\\le y_i \\le 1$$\n$$1\\le i\\le n$$\n具K, C的soft margin SVM之對偶式(Loss(target) funtion, Constraint(條件、約束))為:\n$\\max_{\\alpha_i} \\sum_{i=1}^{n} \\alpha i -\\frac{1}{2}\\sum{i,j=1}^{n} \\alpha_i \\alpha_j y_i y_jK(\\mathrm {x}_i, \\mathrm {x}_j),$\n$s.t. \\sum_{i=1}^{n} \\alpha_iy_i=0, 0\\le \\alpha_i\\le C, i=1,\\dots ,n.$\nK is (\u0026lt;, \u0026gt; 矩陣乘法，處裡非線性可分, $\\phi$是一個mapping)\n$K(\\mathrm {x}_i, \\mathrm {x}_j) = \u0026lt; \\phi(\\mathrm {x}_i), \\phi(\\mathrm {x}_j)\u0026gt;$\n如上式所述，SVM的複雜度取決於訓練樣本數，為$O(n)$。並且因K函數的使用不受dim of feature space影響，然而K函數的計算複雜性被忽略了其取決於dim of input space，就算在最佳分割超平面的情況K函數的計算複雜性也無法被省略。因此降維必定可以使訓練SVM和預測帶來更高效率。\n該論文假設文件集合表示為Document-Term Matrix(Bag of Word)，加權兩倍，假設資料的聚類已經進行。\n下一章回顧LSI，使用svd分解做a的低秩近似，但忽略了資料的聚類結構。第三節中，回顧幾種對聚類資料特別有效的降維演算法:兩種群中心方法和使用GSVD(廣義奇異值分解)的泛化LDA。通過降維SVM()和K-mens(計算向量對距離)等分類器的計算複雜度皆可大降低。\n多數文本資料集中，文本可被分入多個類，為更有效處理此問題，在第四節中介紹基於閥值的分類算法擴展，實驗表明，該論文提出的cluster preserving降維演算法沒有造成訊息損失，反而提升了分類器的預測精度(推斷具有去噪效果)。\n低秩近似使用隱含語意索引 LSI假設:Document-Term Matrix中存在隱含的語意結構，其被文件中出現各種詞所破壞(polysemy and synonymy)。基本概念:若兩個文檔向量代表同一主題，其會共享許多與關鍵詞相關的關聯詞，通過SVD其語意結構會十分接近(term vectors表示為左奇異向量document vectors表示為右奇異向量)。然而，LSI再降維時忽略了聚類結構，且並沒有理論最佳的參數選擇，需多次實驗來確定最佳維度(如第五章所述)。實驗結果證實，當資料已被聚類時，下節介紹的降維法對新資料的分類效果更好。\n聚類資料的降維演算法 為提升高維度資料的處裡效率，須將資料降維，此節回顧三種保留聚類結構的降維算法\nCentroid-based Algorithms for Dimension Reduction of Clustered Data 給定一Document-Term Matrix，找一變換映射每個document vector從m維空間降到l維空間(m\u0026gt;l)，兩種方法:\n線性轉換($G^T_{l\\times m}$) 低秩近似(分解為兩個矩陣) $A\\approx BY$\n只要計算給定資料的降維表示，就無須從B計算降維變換G，若確定矩陣B，Y即可用最小平方法求解。\n$\\min_{B,Y}\\left|BY-A\\right|_{F.}$\n給定任意文本$q\\in\\mathbb{R}^{m\\times1}$透過解最小化問題轉換到低維空間。\n$\\min_{\\hat{q}\\in\\mathbb{R}^{l\\times1}}\\left| B\\hat{q}-q\\right|_{2.}$\n在Centroid降維法中(A1)，B的第$i, (1\\le i\\le p)$列是第$i$個群的中心(均值中心)點向量，任一向量$q$，可在$p$維空間表示成$\\hat{q}$即為最小平方法的解。\n在Orthogonal Centroid演算法中(A2)，使用$p$維表示資料向量$q\\in \\mathbb{R}^{m\\times 1}$被給定為$\\hat{q}=Q_{p}^{T}q$，$Q_p$為$B$的正交基底(QR分解)。\n以上兩種等Centroid-based降維法在計算成本比LSI更低，且在聚類資料下的效果更好。雖然此方案只能在線性可分的資料使用，但文本資料扔然可用，因文本資料通常是線性可分的(非線性可分in 18)\nGeneralized Discriminant Analysis based on the Generalized Singular Value Decomposition 近期(2003)，出現了GSVD base 的 cluster-preserving降維法，使SVM泛化到高維空間資料。\n經典判別分析透過最大化聚類之間的散度和最小化即群內的散度，維持聚類結構。為此，其定義聚類內散度矩陣$S_w$和聚類間散度矩陣$S_b$，$N_i$表示集群$i$的列索引集合$n_i$表示集群$i$的列數，$C$表全域中心點。目標使群內散度最小化，和降維後群間散度最大化。再次請出$G^T\\in \\mathbb{R}^{l\\times m}$將A的每列m維向量映射到l維的變換，目標表示為最小化$trace(G^T S_wG)$和最大化$trace(G^T S_bG)$。\n當$S_w$可逆(nonsingular、invertible)，可視為解最大化問題。 全局最大成立於:$G$的列是$S_{w}^{-1}S_b$的特徵向量並對應於$l$個最大特徵值。 when $l\\le p-1$ is equals $λ_1 + \\dots +λ_{p−1},$ each $λ_i ≥ 0$。設Document-Term Matrix $A$被分為$A=[A_1,\\ \\dots , \\ A_p]$，$A_i \\in \\mathbb{R}^{m\\times n_i}\\ \\text{in cluster}\\ i$\n$H_w = [a_1-c_1, a_2-c_2,\\dots ,a_n-c_p]\\in \\mathbb{R}^{m\\times n}$\n$H_b = [\\sqrt{n_1}(c_1-c),\\dots ,\\sqrt{n_p}(c_p-c)]\\in \\mathbb{R}^{m\\times p}$\n$S_w=H_wH_w^T\\ \u0026amp; \\ S_b=H_bH_b^T$\n當詞數(terms) $m$ \u0026gt; 文本數(doc) $n$，$S_w$不可逆(singular)，經典SVM失效。將問題(特徵值) $S_w^{-1}S_b\\mathrm{x}_i = λ_i\\mathrm{x}^i$ 改寫為 $\\mathrm\\beta_i^2H_bH_b^T\\mathrm{x}_i = \\mathrm\\alpha_i^2H_wH_w^T\\mathrm{x}_i$即可透過GSVD處理(LDA/GSVD in A3)。其中最複雜的計算部分複合矩陣$H$的完全正交分解，當$\\max (p, n)\\ll m$，$H=[H_b^T,H_w^T]\\in \\mathbb{R}^{(p+n)\\times m}$ 的SVD分解可被計算為:\n計算$H_t$的QR分解$Q_HR_H$ 計算$R_H\\in \\mathbb{R}^{(p+n)\\times (p+n)}$ 的SVD分解 $=Z\\begin{pmatrix} \\sum_H \u0026amp; 0 \\ 0 \u0026amp; 0 \\end{pmatrix}P^T$，使$H=R_H^TQ_H^T=P\\begin{pmatrix} \\sum_H \u0026amp; 0 \\ 0 \u0026amp; 0 \\end{pmatrix}Z^TQ_H^T$。其中$Q_HZ\\in\\mathbb{R}^{m\\times (p+n)}$的列之間為標準正交的(內積0、距離1)，存在正交$Q\\in \\mathbb{R} ^{m\\times m}$其前$p+n$列等同$Q_HZ$。將式整理為：\n$H = P\\begin{pmatrix} \\sum_H \u0026amp; 0 \\ 0 \u0026amp; 0 \\end{pmatrix}Q^T$\n式中$\\sum_H$的右邊有$m-t$個0列，因$R_H\\in \\mathbb{R}^{(p+n)\\times (p+n)}$遠小於$H\\in \\mathbb{R}^{(p+n)\\times m}$，記憶體需求大減，複雜度也降至$O(mn^2)+O(n^3)$ 分類方法 為測試降維效果，使用三種分類器測試:中心點分類、kNN和SVMs。皆引入閥值進行修改，以確保文本被判有多重類別資格時可以正確分類。\nCentroid-based 設新的文本資料為$q$，訓練資料共有$p$個群，$c_i$為第$i$個群的中心點向量:\n$arg \\max_{1\\le i\\le p} \\frac{q^Tc_i}{|q|_2 |c_i|_2}$\n多類別擴展($\\theta$為閥值):\n$y(\\mathrm{x}, j) = \\text{sign}{ sim(\\mathrm{x}, \\mathrm{c}_i)-\\theta_j^c }$\n$y(\\mathrm{x}, j)\\in { +1,-1 }$\n$$ \\begin{cases} \\text{Class is j} \u0026amp; \\text{:} \u0026amp; y(\\mathrm{x}, j)\u0026gt;0 \\ \\text{Class is not j} \u0026amp; \\text{:} \u0026amp; y(\\mathrm{x}, j)\\le 0 \\end{cases} $$\nk-Nearest Neighbor 設新的文本資料為$q$，訓練文本資料共有$p$個群:\n在訓練資料中，使用餘弦相似度計算與$q$最近的k個文本向量 在這k個向量中，計算屬於各個群的數量，$q$將被分配到最多的那個。 多類別擴展($\\theta$為閥值，$kNN$為文本$x$的$k$個鄰近向量集合):\n$y(\\mathrm{x}, j) = \\text{sign}{ \\sum_{\\mathrm{d}_i\\in kNN} sim(\\mathrm{x}, \\mathrm{d}_i) y(\\mathrm{d}_i, j) -\\theta_j^{kNN} }$\nSVM OvR策略(為每個Class建一個分類器)的二元分類器的最佳分割超平面可透conventional SVM取得。引入多類別擴展:\n$$ y(\\mathrm{x}, j) = \\text{sign}{ \\sum_{\\mathrm{x}_i\\in SV} \\alpha_i y_i K(\\mathrm{x}, \\mathrm{x}_i)+ b -\\theta_j^{SVM} }\\ K=\u0026lt;\\mathrm{x}, \\mathrm{x}_i\u0026gt; \\ K=[\u0026lt;\\mathrm{x}, \\mathrm{x}_i\u0026gt;+1]^d \\ K=\\exp(-\\gamma|\\mathrm{x}, \\mathrm{x}_i|^2) $$\n$SV$為支援向量的集合，$\\theta$為閥值，$\\gamma$與高斯函數寬度成反比。\n實驗結果 預測結果包含:\n無降維 LSI/SVD Centroid Orthogonal Centroid LDA/GSVD SVM優化:\n正則參數$C$ polynomial 角度$d$ Gaussian RBF $\\gamma$ 資料集 subset of MEDLINE database: 5個class、每個class各有500份文本、每份文本只有一個class、train:test = 50:50、做詞型還原和處理剔除字(此為國家教育研究院翻譯版，俗稱為停用詞)後訓練集有22095個不重複的詞。 Reuter-21578 文本集的\u0026quot;ModApte\u0026quot;分割: 90個class、每份文本可能有多個class、每個class至少有一個train和一個test、共7769個train和3019個tess、做詞型還原和處理剔除字(此為國家教育研究院翻譯版，俗稱為停用詞)後訓練集有11941個不重複的詞、引入閥值模型。 DTM不用BoW，改用TF-IDF並歸一化。\n表一 對MEDLINE資料集使用LSI/SVD，用使用centroid-based, kNN和SVMs分類器分類的結果。觀察到:\nkNN使用L2 norm在$l$為100-500時效果不佳，與餘弦相似性在未正則化資料表現更好的印象相符，且5NN明顯落後其他更高的K，表明k=5太小。 表二 SVM使用不同的K(核)函數與降維演算法在MEDLINE資料集的結果。觀察到:\n降維後的預測結果與原始空間的預測結果相似且複雜度降低。 正交中心降維演算法對K函數的選擇不敏感，可以選複雜度更低的線性K函數。 表三 選擇不同分類演算法與降維演算法在MEDLINE資料集的結果。觀察到:\n使用LDA/GSVD降維演算法時，使用餘弦相似度的centroid-based與kNN分類演算法效果較差，而使用L2-norm的效果較好，因跡最佳化使用L2-norm表示。 因LDA/GSVD最小化群內散點的跡(距離)，自同一(相似)類別的文檔向量會被變換為一個緻密的群甚至一個點，使得SVM難以找到泛化性高的超平面。 表四 MEDLINE資料集中使用SVM配合不同降維演算法在5個不同類別中的準確率。觀察到:\ncolon cancer與oral cancer難以被區分。 表五 選擇不同分類演算法與降維演算法(Centroid、Orthogonal Centroid)在REUTERS資料集的結果。觀察到:\nOrthogonal Centroid 效果無明顯下降，Centroid則明顯降低，推測因將各個聚類中心映射至單位矩陣導致每個class間的資訊消失所造成。 結論與討論 本文使用三種降維方法Centroid、Orthogonal Centroid和LDA/GSVD，皆是為集群資料所設計，做為比較也使用了LSI/SVD這種不保留集群結構的降維演算法。其也測試了三種不同的分類演算法SVMs、kNN與centroid-based classification測試在不同降維法中的分類效果。測試結果取得了高的準度，即使在強力的降維下，也可與未降維的狀態近似。\n該論文還引入了基於閥值的分類器，用於centroid-based和SVM做到一對多的分類。centroid在Class互不關聯的情況下表現更好。\n結論 不犧牲精度的情況下，可以做到對文本大幅降為。Orthogonal Centroid在保留聚類結構的能力做的極佳，降為前後的預測準確率幾乎不變，在KNN或SVM使用可以極大減少計算複雜度。\n","permalink":"https://lin-roger.github.io/posts/dimensionreductionintextclassificationwithsupportvectormachines/","summary":"非經典古文，可以不用看，主要是我的筆記。SVM被公認是許多任務中效果最好的分類方法之一，SVM的學習能力和訓練計算複雜度與特徵空間維度無關，但在文本分類任務中，降低複雜度是有效處裡大量詞語的一個要點。該論文採用新的降維方法降低文檔向量的維度。還為基於中心的分類算法和SVM分類器引入決策函數，處理一個文檔可能屬於多個class的問題。分析大量的實驗結果表明使用為聚類資料設計的降維算法，使輸入維度降低，可在不犧牲預測精度的情況下取得更好的訓練效率。","title":"Dimension Reduction in Text Classification with Support Vector Machines"}]