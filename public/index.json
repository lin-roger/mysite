[{"content":"Abstract 使用語境化詞嵌入替換靜態詞嵌入已在許多NLP任務取得顯著的進步。然而，ELMo和BERT等模型(contextualizing model)所產生的語境化詞嵌入究竟有多語境化?每個詞是否有無限多種特定語境下的表徵或者其本質上是被分配到數量有限的詞表徵中的其中一個?作者發現，在 contextualizing model 的任何一層中，所有詞的語境表徵都不是各向同性的。雖然同一個詞在不同語境下的表徵相較不同詞的表徵具有更大的餘弦相似度，在這種自相似性再上層(輸出)並不明顯。代表 contextualizing model 的上層產生了更多語境表徵，如同LSTM的上層產生更多任務相關的表徵。在ELMo、BERT和GPT-2的所有層中，平均一個詞的語境表徵只有不到5%的方差可以用該詞的靜態嵌入來解釋，為語境表徵的成功提供了一些理由。\n1 Introduction 深度学习方法在NLP中的应用是通过在低维连续空间中将单词表示为向量而实现的。传统上，这些词的嵌入是静态的：每个词都有一个单一的向量，与上下文无关（Mikolov等人，2013a；Pennington等人，2014）。这带来了几个问题，最明显的是，一个多义词的所有意义都必须共享相同的表示。最近的工作，即深度神经语言模型，如ELMo（Peters等人，2018）和BERT（Devlin等人，2018）、我们成功地创建了上下文关联的单词表示，即对其出现的上下文敏感的单词向量。用上下文表征取代静态嵌入，在一系列不同的NLP任务中产生了显著的改进，从回答问题到核心推理。\n语境化词汇表征的成功表明，尽管只用语言建模任务进行训练，但它们可以学习到高度可转移的、与任务无关的语言属性。事实上，在冻结的语境化表征上训练的线性探测模型可以预测单词的语言属性（例如，语篇标签），几乎与最先进的模型一样好（Liu等人，2019a；Hewitt和Manning，2019）。但是，这些表征仍然没有得到很好的理解。 首先，这些语境化的词汇表征到底有多大的语境性？BERT和ELMo是否有无限多的特定语境表征可以分配给每个单词，或者单词基本上是在有限数量的词义表征中分配一个？\n我们通过研究ELMo、BERT和GPT-2的每一层的表示空间的几何形状来回答这个问题。我们的分析产生了一些令人惊讶的发现：\n在所有三个模型的所有层中，所有单词的上下文表征都不是各向同性的：它们在方向上不是均匀分布的。相反，它们是各向异性的，在矢量空间中占据一个狭窄的锥体。GPT-2的最后一层的各向异性是如此的极端，以至于两个随机的词平均来说会有几乎完美的余弦相似性鉴于各向异性对静态嵌入有理论和经验上的好处（Mu等人，2018），上下文代表中各向异性的程度令人惊讶。 同一个词在不同语境中的出现具有非相同的向量表示。在矢量相似性被定义为余弦相似性的情况下，这些表征在上层中彼此更加不相似。这表明，就像LSTM的上层产生更多特定任务的表征一样（Liu等人，2019a），上下文模型的上层产生更多的特定语境表征。 语境特定性在ELMo、BERT和GPT-2中表现得非常不同。在ELMo中，同一句子中的词的表征随着上层的语境特异性的增加而变得更加相似；在BERT中，它们在上层变得更加不相似，但仍然比随机抽样的词平均更加相似；然而在GPT-2中，同一句子中的词并不比两个随机选择的词更加相似。 在对各向异性的影响进行调整后，平均而言，一个词的上下文表征中只有不到5%的方差可以由其第一主成分来解释。 这表明，上下文表征并不对应于有限数量的词义表征，即使在最好的情况下，静态嵌入也是上下文表征的糟糕替代。不过，通过提取一个词的上下文表征的第一个主成分而创建的静态嵌入在许多词向量基准上的表现优于GloVe和FastText嵌入。 这些见解有助于证明为什么使用上下文表征能使许多NLP任务得到如此显著的改善。\n2 Related Work 2.1 Static Word Embeddings Skip-gram with negative sampling（SGNS）（Mikolov等人，2013a）和GloVe（Pennington等人，2014）是生成静态词嵌入的最著名的模型之一。尽管它们在实践中迭代地学习嵌入，但已经证明在理论上它们都隐含了一个包含共现统计的词-语境矩阵的因子（Levy和Goldberg，2014a，b）。由于它们为每个词创建了一个单一的表示，静态词嵌入的一个显著问题是，一个多义词的所有意义必须共享一个矢量。\n2.2 Contextualized Word Representations 鉴于静态单词嵌入的局限性，最近的工作试图创建对语境敏感的单词表示。ELMo（Peters等人，2018）、BERT（Devlin等人，2018）和GPT-2（Radford等人，2019）是深度神经语言模型，它们经过微调，为广泛的下游NLP任务创建模型。他们对单词的内部表征被称为语境化的单词表征，因为它们是整个输入句子的一个函数。这种方法的成功表明，这些表征捕捉到了语言的高度可转移性和任务无关的属性（Liu等人，2019a）\nELMo通过串联在双向语言建模任务上训练的2层biLSTM的内部状态来创建每个标记的语境化表示（Peters等人，2018）。相比之下，BERT和GPT-2分别是双向和单向的基于转化器的语言模型。12层的BERT（基础，套管）和12层的GPT-2的每个转化器层通过关注输入句子的不同部分来创建每个标记的语境化表示（Devlin等人，2018；Radford等人，2019）。BERT\u0026ndash;以及BERT的后续迭代（Liu等人，2019b；Yang等人，2019）\u0026ndash;在各种下游NLP任务上取得了最先进的性能，包括从问题回答到情感分析。\n2.3 Probing Tasks 之前对语境化词汇表征的分析主要限于探测任务（Tenney等人，2019；Hewitt和Manning，2019）。这涉及到训练线性模型来预测单词的句法（如语篇标签）和语义（如单词关系）属性。探测模型的前提是，如果一个简单的线性模型可以被训练来准确预测语言属性，那么表征就会隐含地开始编码这一信息。虽然这些分析发现，语境化的表征编码了语义和句法信息，但它们无法回答这些表征的语境性如何，以及它们在多大程度上可以被静态词缀取代，如果有的话。因此，我们在本文中的工作与大多数对语境化表征的剖析明显不同。它更类似于Mimno和Thompson（2017），后者研究了静态词嵌入空间的几何学。\n3 Approach 3.1 Contextualizing Models 我们在本文中研究的语境化模型是ELMo、BERT和GPT-21。我们选择BERT的基础案例版本，因为它在层数和维度方面与GPT-2最具可比性。我们所使用的模型都是在各自的语言建模任务中预先训练过的。尽管ELMo、BERT和GPT-2分别有2、12和12个隐藏层，但我们也将每个语境化模型的输入层作为其第0层。这是因为第0层没有进行上下文处理，使其成为比较后续层所做的上下文处理的有用基线。\n3.2 Data 为了分析语境化的单词表征，我们需要输入的句子来输入我们的预训练模型。我们的输入数据来自2012-2016年的SemEval语义文本相似性任务（Agirre等人，2012，2013，2014，2015）。我们使用这些数据集是因为它们包含了相同的词出现在不同语境中的句子。例如，\u0026ldquo;狗 \u0026ldquo;这个词出现在 \u0026ldquo;一只熊猫狗在路上跑 \u0026ldquo;和 \u0026ldquo;一只狗想把培根从背上拿下来 \u0026ldquo;中。 如果一个模型在这两个句子中为 \u0026ldquo;狗 \u0026ldquo;生成了相同的表示，我们可以推断出没有上下文；相反，如果这两个表示不同，我们可以推断出它们在某种程度上被上下文化。利用这些数据集，我们将单词映射到它们出现的句子列表以及它们在这些句子中的索引。在我们的分析中，我们不考虑那些出现在少于5个独特语境中的词。\n3.3 Measures of Contextuality 我们用三个不同的指标来衡量一个词的上下文表述的程度：自相似性、句内相似性和最大可解释方差。\nDefinition 1 让 $w$ 是一个分别出现在句子 ${s_1,\u0026hellip;,s_n}$ 的索引 ${i_1,\u0026hellip;,i_n}$ 的词，这样 $w=s_1[i_1]=\u0026hellip;=s_n[i_n]$ 。让 $f_\\ell (s,i)$ 成为一个函数，将 $s[i]$ 映射到模型 $f$ 的第 $\\ell$ 层中的表示。 $w$ 在第 $\\ell$ 层中的自相似性为\n$$SelfSim_\\ell (w) = \\frac{1}{n^2-n} \\sum_{j} \\sum_{k \\not = j} \\cos (f\\ell(s_j, i_j), f\\ell(s_k, i_k))$$\n其中 cos 表示余弦相似度。换句话说，一个单词 $w$ 在第 $\\ell$ 层中的自相似性是其在 $n$ 个独特语境中的上下文表征之间的平均余弦相似度。 如果第 $\\ell$ 层完全不对表征进行上下文化，那么 $SelfSim_\\ell(w)=1$ （即表征在所有语境中都是相同的）。对于 $w$ 来说，表征的情境化程度越高，我们期望它的自我相似性就越低。\n$SelfSim$計算同一個詞在不同句子(語境)中嵌入向量(表徵)的相似度，設有$A、B、C$三個不同的句子，且三個句子中皆有詞$W$，取得$W$在所有句子的嵌入$W_A、W_B、W_C$，並倆兩計算他們之間的餘弦相似度$Sim_{AB}、Sim_{AC}、Sim_{BA}、Sim_{BC}、Sim_{CA}、Sim_{CB}$並計算平均。表徵的語境化程度越高，$SelfSim$就越低(代表不同語境的表徵十分不同)\nDefinition 2 让 $s$ 是一个句子，是 $n$ 个词的序列 $\\langle w_{1},\u0026hellip;,w_{ni}\\rangle$ 。让 $f_\\ell (s,i)$ 是一个函数，将 $s[i]$ 映射到模型 $f$ 的第 $\\ell$ 层中的表示。 $s$ 在第 $\\ell$ 层中的句子内部相似度为\n$$IntraSim_\\ell (s) = \\frac{1}{n} \\sum_{i} \\cos(\\vec{s_\\ell}, f_\\ell(s, i))$$ $$\\text{where} \\ \\vec{s_\\ell} = \\frac{1}{n} \\sum_i f_{\\ell}(s, i)$$\n更简单地说，一个句子的句内相似性是其单词表征和句子向量之间的平均余弦相似度，而句子向量只是这些单词向量的平均值。这种测量方法捕捉到了语境的特殊性在向量空间中的表现。这个衡量标准抓住了上下文特定性在向量空间中的表现。例如，如果 $IntraSim_\\ell(s)$ 和 $SelfSim_\\ell(w)$ 都是低的 $∀ w∈s$(趨異)，那么模型通过给每个词一个上下文特定的表示，使该层中的词与句子中所有其他词的表示不同，从而实现上下文的关联。如果 $IntraSim_\\ell(s)$ 是高的(趨同)，但 $SelfSim_\\ell(w)$ 是低的，这表明上下文的细微差别较小，句子中的词只是通过使它们的表征在向量空间中趋同而被上下文化。\n$IntraSim$計算句中每個詞的嵌入向量和句嵌入向量(使用句中所有詞嵌入之平均)的平均相似性。\n\\ $IntraSim \\ Low$ $IntraSim \\ Hight$ $SelfSim \\ Low$ 模型在不同語境下給予每個詞一個獨特的表徵，從而實現語境化嵌入 模型透過同化句子中詞的嵌入表徵來實現語境化嵌入 $SelfSim \\ Hight$ 模型給予每個詞一個讀特的表徵，在所有語境中皆使用同一表徵，與傳統靜態嵌入無異 極為糟糕，模型給予每個詞幾乎相同的表徵 Definition 3 让 $w$ 是一个分别出现在句子 ${s_1,\u0026hellip;,s_n}$ 的索引 ${i_1,\u0026hellip;,i_n}$ 的词，这样 $w = s_1[i_1] = \u0026hellip; = s_n[i_n]$ 。让 $f\\ell(s,i)$ 成为一个函数，将 $s[i]$ 映射到模型f的第 $\\ell$ 层中的表示。其中 $[ f\\ell(s1,i1)\u0026hellip;f\\ell(sn,in)]$ 是 $w$ 的发生矩阵， $σ_1\u0026hellip;σ_m$ 是该矩阵的前 $m$ 个奇异值，最大可解释方差为\n$$MEV_{\\ell}(w)=\\frac{\\sigma^2_1}{\\sum_i \\sigma_1^2}$$\n$MEV_\\ell(w)$ 是指在某一层的 $w$ 的上下文表征中，可由其第一主成分解释的方差比例。它为我们提供了一个关于静态嵌入可以取代一个词的上下文表征的上限。 $MEV_\\ell(w)$ 越接近于 $0$，静态嵌入的替代性就越差；如果 $MEV_\\ell(w)=1$ ，那么静态嵌入将是上下文表征的完美替代。\nMEV代表同一個詞在不同語境中的表徵所形成的分布，每一個主成分可以當成一個軸，如果第一主成分(最大的那個)，可以表達越多資訊(MEV越大)，代表同一個詞在不同語境的表徵會聚集到空間中的一個軸附近(容易使用一個固定的表徵替代)，反之MEV越小，代表同一個詞在不同語境的表徵會均於分散於空間中。\n3.4 Adjusting for Anisotropy 在讨论语境性时，考虑各向同性（或缺乏各向同性）是很重要的。例如，如果单词向量是完全各向同性的（即方向一致，任意兩個詞的相似度低），那么 $SelfSim_\\ell (w)=0.95$ 将表明 $w$ 的表征的语境性很差。然而，考虑到这样的情况，即词向量是如此的各向异性，任何两个词的平均余弦相似度为$0.99$。那么 $SelfSim_\\ell (w)=0.95$ 实际上就表明了相反的情况\u0026ndash;$w$的表征被很好地上下文化了。这是因为$w$在不同语境中的表征平均来说比两个随机选择的词更不相似。\n为了调整各向异性的影响，我们使用了三种各向异性的基线，每一种用于我们的语境性测量。对于自相似性和句子内相似性，基线是来自不同语境的均匀随机抽样的词的表征之间的平均余弦相似度。在一个给定的层中，单词表征的各向异性越大，这个基线就越接近于1。对于最大可解释方差（MEV），基线是均匀随机抽样的单词表征中被其第一主成分解释的方差比例。某一层的表征越是各向异性，这个基线就越接近于1：即使是随机抽样的词，主成分也能解释很大一部分的方差\n由于情境性测量是针对情境化模型的每一层计算的，我们也为每一层计算单独的基线。然后，我们从每个测量值中减去其各自的基线，得到各向异性调整后的背景性测量。例如，各向异性调整后的自相似性为\n$$Baseline(f_\\ell) = E_{x,y∼U(O)}[\\cos(f_\\ell(x), f_\\ell(y))]$$ $$SelfSim^∗_\\ell(w) = SelfSim_\\ell(w)−Baseline(f_\\ell)$$\n其中$O$是所有单词出现的集合，$f_\\ell()$将一个单词出现映射到模型$f$的$\\ell$层中的表示。除非另有说明，本文其余部分中提到的语境性测量是指各向异性调整后的测量，其中原始测量和基线都是用1K个均匀的随机抽样的词表征来估计的。\nbaseLine of $SelfSim、IntraSim$: 隨機抽兩個詞的表徵間的平均餘弦相似度(1K組)，與各向異性程度成正比\nbaseLine of $MEV$: 隨機抽取1K個詞的表徵，並計算其構成的矩陣之$MEV$，與各向異性程度成正比\n4 Findings 图1：在BERT、ELMo和GPT-2的几乎所有层中，单词表征都是各向异性的（每個方向的分布並非都是均勻分布）：统一随机采样的单词之间的平均余弦相似度不为零。 唯一的例外是ELMo的输入层；鉴于它生成字符级嵌入而不使用上下文，这并不令人惊讶。高层的表征通常比低层的表征更加各向异性。\n4.1 (An)Isotropy 上下文的表征在所有非输入层都是各向异性的。 如果来自某一层的单词表征是各向同性的（每個方向的分布都是均勻分布），那么均匀随机采样的单词之间的平均余弦相似度将是0（Arora等人，2017）。这个平均值越接近于1，表示各向异性越大。各向异性的几何解释是，词的表征都在向量空间中占据一个狭窄的锥体，而不是在所有方向上都是均匀的；各向异性越大，这个圆锥就越窄（Mimno和Thompson，2017）。如图1所示，这意味着在BERT、ELMo和GPT-2的几乎所有层中，所有单词的表征都在向量空间中占据一个狭窄的圆锥。唯一的例外是ELMo的输入层，它产生静态的字符级嵌入，而不使用上下文甚至位置信息（Peters等人，2018）。然而，应该注意的是，并非所有的静态嵌入都一定是各向同性的；Mimno和Thompson（2017）发现，同样是静态的skipgram嵌入并不是各向同性的。\n那靜態的skipgram嵌入各向異性的程度為何?各向異性程度與餘弦相似度的準確性是否明確成反比?\n语境化的表征一般在较高的层次上更具有各向异性。 如图1所示，对于GPT-2，均匀随机的单词之间的平均余弦相似度在第2层到第8层大致为0.6，但从第8层到第12层呈指数级增长。事实上，GPT-2的最后一层的单词表示是如此的各向异性，以至于任何两个单词的平均余弦相似度几乎是完美的!这种模式也适用于BERT和ELMo，但也有例外：例如，BERT的倒数第二层的各向异性比最后一层高得多。\n各向同性对静态词嵌入有理论和经验上的好处。在理论上，它允许在训练期间进行更强的 \u0026ldquo;自我规范化\u0026rdquo;（Arora等人，2017），并且在实践中，从静态嵌入中减去平均矢量会导致在几个下游NLP任务上的改进（Mu等人，2018）。因此，在语境化词汇表征中看到的极端程度的各向异性\u0026ndash;特别是在较高的层中\u0026ndash;是令人惊讶的。如图1所示，对于所有三个模型，上下文隐蔽层的表征几乎都比输入层的表征更加各向异性，后者没有纳入上下文。这表明高各向异性是语境化过程所固有的，或者至少是语境化过程的副产品。\n图2：同一个词在不同语境中的平均余弦相似度被称为该词的自相似度（见定义1）。上面，我们绘制了各向异性调整后的均匀随机抽样词的平均自相似度（见3.4节）。在所有三个模型中，层数越高，自相似度越低，这表明在较高的层数中，语境化的词汇表征更具有语境针对性。\n图3：句内相似度是指一个句子中每个词的表现形式与它们的平均值之间的平均余弦相似度（见定义2）。上面，我们绘制了均匀随机抽样的句子的平均句内相似度，并对各向异性进行了调整。这个统计数字反映了上下文特定性在表示空间中的表现，如上所述，它在ELMo、BERT和GPT-2中的表现非常不同。\n4.2 Context-Specificity 语境化的词汇表征在更高的层次上更具有语境针对性。 从定义1中可以看出，一个词的自相似性，在一个给定模型的某一层中，是它在不同语境中的表征的平均余弦相似度，并根据各向异性进行调整。如果自相似性为1，那么这些表征就完全没有语境特异性；如果自相似性为0，那么这些表征就具有最大的语境特异性。在图2中，我们绘制了BERT、ELMo和GPT-2各层中均匀随机抽样的词的平均自相似度。例如，ELMo的输入层的自相似度是1.0，因为该层的表征是静态字符级嵌入。\n在所有三个模型中，层数越高，自我相似度平均越低。换句话说，层数越高，上下文表征的具体内容越多。这一发现具有直观的意义。在图像分类模型中，低层识别更多的通用特征，如边缘，而高层识别更多的特定类别特征（Yosinski等人，2014）。同样，在NLP任务上训练的LSTM的上层会学习更多的特定任务表征（Liu等人，2019a）。因此，由此可见，神经语言模型的上层会学习更多的特定语境表征，从而更准确地预测特定语境下的下一个单词。在所有三个模型中，GPT-2的表征是最有语境针对性的，GPT-2最后一层的表征几乎是最大的语境针对性。\n停顿词（例如，\u0026ldquo;the\u0026rdquo;、\u0026ldquo;of\u0026rdquo;、\u0026ldquo;to\u0026rdquo;）是最有语境针对性的表述之一。 在所有的层中，停顿词的自我相似度是所有词中最低的，这意味着它们的上下文表征是最有背景的。例如，在ELMo的各层中，平均自我相似度最低的词是 \u0026ldquo;and\u0026rdquo;、\u0026ldquo;of\u0026rdquo;、\u0026ldquo;s\u0026rdquo;、\u0026ldquo;the \u0026ldquo;和 \u0026ldquo;to\u0026rdquo;。鉴于这些词不是多义词，这相对来说是令人惊讶的。这一发现表明，一个词所出现的各种语境，而不是其固有的多义性，是推动其语境化表征变化的原因。这回答了我们在介绍中提出的一个问题：ELMo、BERT和GPT-2并不是简单地将有限数量的词义表征中的一个分配给每个词；否则，在具有如此少的词义的词的表征中就不会有如此多的变化。\n上下文的特定性在ELMo、BERT和GPT-2中表现得非常不同。 如前所述，在ELMo、BERT和GPT2的上层，情境化的表征更具情境特异性。然而，这种增加的语境特异性在向量空间中是如何体现的？同一句子中的单词表征是否会收敛到一个点上，或者它们在与其他语境中的表征不同的同时，仍然保持着彼此的不同？为了回答这个问题，我们可以测量一个句子的句内相似度。从定义2中可以看出，在给定模型的某一层中，一个句子的句内相似性是其每个词表征与它们的平均值之间的平均余弦相似度，并根据各向异性进行调整。在图3中，我们绘制了500个均匀随机抽样的句子的平均句内相似度。\n在ELMo中，同一句子中的单词在上层的相似度更高。 随着一个句子中的单词表征在上层变得更加具有语境针对性，句子内的相似度也会上升。这表明，在实践中，ELMo最终将Firth（1957）的分布假说背后的直觉延伸到了句子层面：因为同一句子中的词有相同的语境，它们的语境化表示也应该是相似的。\n在BERT中，同一句子中的单词在上层的相异性更大。 随着一个句子中的单词表征在上层变得更有语境针对性，它们会彼此渐行渐远，尽管也有例外（见图3中第12层）。然而，在所有层中，同一句子中的单词之间的平均相似度仍然大于随机选择的单词之间的平均相似度（即各向异性基线）。这表明，与ELMo相比，BERT有一个更细微的上下文，它认识到，虽然周围的句子告知了一个词的含义，但同一句子中的两个词不一定有相似的含义，因为它们有相同的背景。\n在GPT-2中，同一句子中的单词表征与随机抽样的单词相比，彼此之间的相似度不高。 平均而言，未经调整的句内相似度与各向异性基线大致相同，因此从图3中可以看出，各向异性调整后的句内相似度在GPT-2的大多数层中接近于0。事实上，句内相似度在输入层中是最高的，该层完全不对词进行上下文处理。这与ELMo和BERT形成鲜明对比，在这两个机构中，除了一个层之外，其他层的平均句内相似度都高于0.20。\n正如前面讨论BERT时指出的，这种行为仍然具有直观的意义：同一个句子中的两个词不一定有相似的意义，仅仅是因为它们有相同的上下文。GPT-2的成功表明，与各向异性不同的是，在所有三个模型中，各向异性都伴随着语境特异性，句子内的高相似性并不是语境化所固有的。同一句子中的词可以有高度的语境化表征，而这些表征并不比两个随机的词表征更相似。然而，目前还不清楚这些句子内相似性的差异是否可以追溯到模型结构的差异；我们把这个问题留给未来的工作。\n图4：一个词的最大可解释方差（MEV）是其上下文表征中可由其第一主成分解释的方差比例（见定义3）。上面，我们绘制了各向异性调整后的均匀随机抽样词的平均MEV。在任何模型的任何一层中，静态嵌入都不能解释一个词的上下文表征中超过5%的方差。\n表1：各种静态嵌入在单词嵌入基准任务中的表现。每项任务的最佳结果用粗体表示。对于上下文模型（ELMo，BERT，GPT-2），我们使用一个词在某一层的上下文表征的第一主成分作为其静态嵌入。使用ELMo和BERT的语境化表征创建的静态嵌入往往比GloVe和FastText向量的表现更好。\n4.3 Static vs. Contextualized 平均来说，一个词的上下文表征中只有不到5%的方差可以由静态嵌入来解释。 从定义3中可以看出，对于给定模型的某一层，一个词的最大可解释方差（MEV）是其上下文表征中可由其第一主成分解释的方差比例。这为我们提供了一个关于静态嵌入能多好地取代一个词的上下文表征的上限。因为上下文表征是各向异性的（见第4.1节），所有词的大部分变化都可以由一个单一的向量来解释。我们通过计算统一随机抽样的单词表征的第一主成分所解释的方差比例，并从原始MEV中减去这一比例来调整各向异性。在图4中，我们绘制了各向异性调整后的平均MEV，横跨均匀随机抽样的词。\n在ELMo、BERT或GPT-2的任何一层中，平均来说，一个词的上下文表征中的方差都不能被静态嵌入所解释，而超过5%。虽然在图4中看不到，但许多词的原始MEV实际上低于各向异性基线：也就是说，所有词的方差中更大的比例可以由一个单一的向量来解释，而不是一个词的所有表征的方差。请注意，5%的阈值代表了最好的情况，理论上不能保证使用GloVe获得的词向量，例如，与最大限度提高MEV的静态嵌入相似。这表明，语境化模型并不是简单地将有限数量的词义表征中的一个分配给每个词\u0026ndash;否则，解释的方差比例会高得多。即使是ELMo和BERT的所有层的平均原始MEV也低于5%；只有GPT-2的原始MEV是不可忽略的，由于极高的各向异性，第2至11层的原始MEV平均为30%左右。\n在许多基准上，低层的上下文表征的主成分超过了GloVe和FastText。 如前所述，我们可以通过在给定层中提取其上下文表征的第一个主成分（PC）来为每个词创建静态嵌入。在表1中，我们绘制了这些PC静态嵌入在几个基准任务中的表现2。这些任务包括语义相似性、类比解决和概念分类：SimLex999（Hill等人，2015）、MEN（Bruni等人，2014）、WS353（Finkelstein等人，2002）、RW（Luong等人，2013）、SemEval-2012（Jurgens等人，2012）、Google类比解决（Mikolov等人，2013a）MSR类比解决（Mikolov等人，2013b）、BLESS（Baroni和Lenci，2011）以及AP（Almuhareb和Poesio，2004）。我们在表1中不考虑第3-10层，因为它们的性能介于第2和11层之间。\n表现最好的PC静态嵌入属于BERT的第一层，尽管BERT和ELMo的其他层的PC静态嵌入在大多数基准上也优于GloVe和FastText。对于所有三个上下文模型，从低层创建的PC静态嵌入比从高层创建的更有效。使用GPT-2创建的静态嵌入也明显比来自ELMo和BERT的静态嵌入表现差。鉴于上层比下层更具有语境特异性，而且GPT2的表征比ELMo和BERT的更具有语境特异性（见图2），这表明高度语境特异性表征的PC在传统基准上的效果较差。那些从较少的上下文特定表征中得出的表征，如来自BERT第1层的表征，要有效得多。\n5 Future Work 我们的发现为未来的工作提供了一些新的方向。首先，正如本文前面所指出的，Mu等人（2018）发现，使静态嵌入更加各向同性\u0026ndash;通过从每个嵌入中减去其平均值\u0026ndash;导致下游任务的性能有惊人的改善。 鉴于各向同性对静态嵌入有好处，它可能对上下文词表征也有好处，尽管后者已经产生了重大改善，尽管是高度各向异性。因此，在语言建模的目标中加入各向异性的惩罚\u0026ndash;鼓励上下文表征更加各向同性\u0026ndash;可能会产生更好的结果。\n未来工作的另一个方向是从语境化的词生成静态的词表示。虽然后者提供了卓越的性能，但在生产中部署大型模型，如BERT，往往存在内存和运行时间方面的挑战。我们在第4.3节中的工作表明，不仅有可能从上下文模型中提取静态表示，而且这些提取的向量与传统的静态嵌入（如GloVe和FastText）相比，在一系列不同的任务中通常表现得更好。这可能是一种从语境化模型中提取一些用途的手段，而不需要承担在生产中使用它们的全部成本。\n6 Conclusion 在本文中，我们调查了上下文语境下的单词表征到底是怎样的。其一，我们发现ELMo、BERT和GPT-2的上层比下层产生更多的上下文特定表征。这种语境特异性的增加总是伴随着各向异性的增加。然而，语境特异性在三个模型中的表现也不同；各向异性调整后的同一句子中单词之间的相似性在ELMo中最高，但在GPT-2中几乎不存在。我们最终发现，在对各向异性进行调整后，平均来说，一个词的上下文表征中只有不到5%的方差可以用静态嵌入来解释。这意味着，即使在最好的情况下，在所有模型的所有层中，静态的单词嵌入也不能很好地替代上下文的嵌入。这些见解有助于解释上下文表征在一系列不同的NLP任务中所取得的显著成功。\n","permalink":"https://lin-roger.github.io/posts/howcontextualarecontextualizedwordrepresentationscomparingthegeometryofbertelmoandgpt2embeddings/","summary":"🚧還在寫喔!!🚧","title":"🚧How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings🚧"},{"content":"Abstract BERT 等預訓練模型在許多自然語言處理任務中取得了巨大成功。然而，如何通過這些預訓練模型獲得更好的句表徵向量仍然值得探索。先前的工作表明，各向異性問題是基於 BERT 的句表徵的關鍵瓶頸，阻礙模型充分利用底層語義特徵。因此，一些增強句子分佈各向同性的嘗試，例如基於流的模型(BERT-flow)，已被應用於句表徵並取得了一些改進。在本文中，我們發現傳統機器學習中的白化(球化)變換同樣可以增強句表徵的各向同性並取得有競爭力的結果。此外，白化還能夠降低句子表示的維度。我們的實驗結果表明，它不僅可以實現良好的性能，而且可以顯著降低存儲成本並加快模型檢索速度。\nIntroduction 深度神經語言模型的應用(BERT、GPT)近年來取得了巨大成功，因為它們創造了對隨前後文語境改變的詞彙表徵向量。這一趨勢也刺激了為長文本產生語義崁入(句崁入向量、段落崁入向量)的研究進展。然而句崁入已被證明不能完整捕捉句子的基本語義。據先前研究(Gao et al., 2019; Ethayarajh, 2019; Li et al., 2020)，所有的詞語的表徵向量都不是各項同性的：它們在方向上並非均勻分布的，而是在崁入空間中佔據一個狹小的錐體，呈現各向異性。Ethayarajh, 2019; 證明，取自預訓練模型的崁入向量的極度的各向異性，以致任兩個詞的崁入向量COS相似度平均為0.99(極度相似)。Li et al., 2020進一步調查發現，BERT的句崁入空間存在兩問題:\n詞頻令崁入空間產生偏移：高頻詞距離原點較近，低頻詞距離原點較遠。在相同權重下，低頻詞對整體句崁入向量的遠比高頻詞影響更強 低頻詞分散且稀疏： ","permalink":"https://lin-roger.github.io/posts/whiteningsentencerepresentationsforbettersemanticsandfasterretrieval/","summary":"🚧還在寫喔!!語意相似度計算🚧","title":"🚧Whitening Sentence Representations for Better Semantics and Faster Retrieval🚧"},{"content":"Zero-Shot Prompting \u0026amp; Few(N)-Shot Prompting How it work Shot意指範例，顧名思義，Zero-Shot就是提示不包含任何範例，反之N-Shot就是提示包含N個範例，在簡單任務中您使用Zero-Shot Prompting通常也可獲得不錯的效果，但當問題開始複雜起來時，Few-Shot Prompting 可以更好的解決問題\nThe model has somehow learned how to perform the task by providing it with just one example (i.e., 1-shot). For more difficult tasks, we can experiment with increasing the demonstrations (e.g., 3-shot, 5-shot, 10-shot, etc.).\n模型通過僅提供一個示例（即 1-shot）以某種方式學會瞭如何執行任務。對於更困難的任務，我們可以嘗試增加演示次數（例如 3 次、5 次、10 次等）。\nFollowing the findings from Min et al. (2022), here are a few more tips about demonstrations/exemplars when doing few-shot:\n根據 Min 等人的研究結果，這裡有一些關於進行小樣本時演示/範例的更多提示：\n\u0026ldquo;the label space and the distribution of the input text specified by the demonstrations are both important (regardless of whether the labels are correct for individual inputs)\u0026rdquo;\n“演示指定的標籤空間和輸入文本的分佈都很重要（無論標籤對於各個輸入是否正確）”\nthe format you use also plays a key role in performance, even if you just use random labels, this is much better than no labels at all.\n您使用的格式對性能也起著關鍵作用，即使您只是使用隨機標籤，這也比根本沒有標籤要好得多。\nadditional results show that selecting random labels from a true distribution of labels (instead of a uniform distribution) also helps.\n其他結果表明，從真實的標籤分佈（而不是均勻分佈）中選擇隨機標籤也有幫助。\nLimitations 當問題牽扯到具有多步驟的複雜推裡時，Few(N)-Shot 的效果會減弱，這時將指令拆解會是更好的選擇\nChain-of-Thought Prompting Self-Consistency Generated Knowledge Prompting Tree of Thoughts (ToT) Retrieval Augmented Generation (RAG) Automatic Reasoning and Tool-use (ART) Automatic Prompt Engineer (APE) Active-Prompt Directional Stimulus Prompting ReAct Prompting Multimodal CoT Prompting Graph Prompts ","permalink":"https://lin-roger.github.io/posts/advancedpromptingengineering/","summary":"🚧還在寫喔!!🚧","title":"🚧Advanced Prompting Engineering 進階提示工程🚧"},{"content":"Base LLM \u0026amp; Instruction Tuned LLM Base LLM 使用大量從網路爬取的資料訓練，輸入一文本後預測出最有可能接續在該文本後的下一個字(token)，所以給定一文章的前半部分其可將其後半部分生成。問題 -\u0026gt; 最有可能接續的字詞和人類預想的相同嗎?假設以下為您的訓練資料\n常見問題:\nQ1:為什麼同樣行程不同旅行社價格會不一樣？\nQ2:行程上有的景點，是不是一定都會走到？\nQ3:為什麼我和朋友的座位沒有安排在一起？\nA1:其實旅遊產品牽涉範圍極廣\u0026hellip;\nA2:旅客在報名旅遊行程前\u0026hellip;\nA3:機上座位的安排\u0026hellip;\n當您向使用該資料訓練的LM輸入\u0026quot;Q1:為什麼同樣行程不同旅行社價格會不一樣？\u0026quot;，我們通常為期望LM回答與問題對應的答案，也就是:\u0026ldquo;A1:其實旅遊產品牽涉範圍極廣\u0026hellip;\u0026quot;，但實際上LM會回答:\u0026ldquo;Q2:行程上有的景點，是不是一定都會走到？\u0026quot;，這是因為資料集中，文本Q1的下一段文本為Q2，而非A1，使模型認為Q1-\u0026gt;Q2是最好的選擇，而非Q1-\u0026gt;A1。\n為解決此問題，Instruction Tuned LLM 出現了。透過對Base LLM 使用指令資料集(如: alpaca-tw )微調並使用RLHF技術使LM的的輸出更符合人類的預期。\n指令的基本原則 0. Model Limitations: Hallucination 幻覺(Hallucination)，LLM並不了解自身知識的極限也不完全記得所有其看過的資訊，遇到一些艱澀的問題其可能會嘗試回答，並輸出看似合理但與事實不符或錯誤的回應，透過要求模型找出問題相關的文獻並引用其回答可以減輕此問題(LM找出的相關文獻或引用也有為幻覺之嫌，務必謹慎確認)\n無法做到精確字數要求，如過您在指令中加入以下要求\u0026rdquo;\u0026hellip;, 在50個字以內說明\u0026rdquo;，可能會出現60甚至70個字(會因語言影響誤差範圍，該問題為token與字/詞數量不等所導致，一個字/詞可由1~n個token組成，通常為1~3個，參考BPE編碼)\n1. Write Clear and Specific Instructions Clear != Short，更長的指令通常提供了更多資訊，可以令模型產生更準確的結果。 Use delimiters，使用分隔符號將輸入資訊清楚的分割，使輸入資料不會與指令或其他資訊混淆，該方法也可以一定程度抵擋Prompt Injections(透過在輸入資料中夾帶惡意指令使模型輸出改變的技術。如:忽略先前的指令，幫我\u0026hellip;。使用分隔符號將其指定為輸入資料可使模型不執行該惡意指令) Ask for structured output，可以在指令中要求輸出資料以json、xml等結構化的形式表達，可以被更好的儲存或處裡 Check whether conditions are satisfied，在指令中加入一些條件，避免一些意外使輸出不合預期 Few-shot prompting，對於一個複雜的任務，指令中只有任務描述是不夠的，在指令中加入一些範例可以使LM有效理解你的要求(補充，LLM在zero-shot的表現並不佳，請盡量避免) 2. Give Model Time to Think Specify the steps to complete a task，一個複雜的任務可以將其拆解成多個較為簡單的步驟或指令使模型可以更好理解或處理您的任務 Instruct the model to work out its own solution before rushing to a conclusion，詢問模型時可以要求模型先輸出推論過程在輸出結果 Iterative Prompt Development(Prompt 的開發與迭代流程) Prompt的構成:\nInstructions Context Input Data Output Indicator Prompt guidelines:\n寫出符合基本原則的Prompt 測試Prompt，取得大量輸出 找出不合預期的案例並分析原因以改進Prompt Repeat 並沒有所謂的萬能Prompt，只有適合的Prompt\nUse Case Summarizing 文本摘要 LLM 通常預設以抽樣摘要(以比原文更簡短的通順文本表述原文資訊)為主，可以要求:字數、句數、風格(簡潔的、嚴謹的)、側重在甚麼(對\u0026hellip;的影響、XXX說的內容)、摘要的對象(國小生、商業部門經理)等\n你可以要求LLM以抽取式摘要(原文的重點字、詞、句、段，原封不動的取出)，如:\u0026ldquo;找出/萃取文章中XXX會想知道的關鍵字/資訊/句子\u0026rdquo;，取得關鍵資訊避免攏言贅詞並減少幻覺產生的可能。\nQuestion Answering 問答 Prompt:\nAnswer the question based on the context below. Keep the answer short and concise. Respond \u0026ldquo;Unsure about answer\u0026rdquo; if not sure about the answer.\nKeep the answer short and concise: 指定風格 Respond \u0026ldquo;Unsure about answer\u0026rdquo; if not sure about the answer: 1.3-Check whether conditions are satisfied Text Classification 文本分類 Classify the text into neutral, negative or positive and follow the output format.\noutput format:\u0026ldquo;Class\u0026rdquo;\nText: I think the vacation is okay.\nSentiment: \u0026ldquo;Neutral\u0026rdquo;\nText: I think the food was okay. Sentiment:\n","permalink":"https://lin-roger.github.io/posts/promptee/","summary":"簡單介紹prompt-engineering","title":"prompt-engineering 基礎"},{"content":"摘要 SVM被公認是許多任務中效果最好的分類方法之一，SVM的學習能力和訓練計算複雜度與特徵空間維度無關，但在文本分類任務中，降低複雜度是有效處裡大量詞語的一個要點。該論文採用新的降維方法降低文檔向量的維度。還為基於中心的分類算法和SVM分類器引入決策函數，處理一個文檔可能屬於多個class的問題。分析大量的實驗結果表明使用為聚類資料設計的降維算法，使輸入維度降低，可在不犧牲預測精度的情況下取得更好的訓練效率。\n簡介 文本分類是一項監督是任務，將文本分類到預定義的class，用來從大樣文本中尋找有價值的資訊，base on Vctor Space的方法有以下特性，input高維且稀疏(one hot, bag of word)，線性可分性(存在超平面將資料分割)，少數特徵不相關(多數相關)。有人猜測，積極降維會導致資訊嚴重損失，導致分類效果不佳。\n給定訓練資料:\n$$(x_i, y_i)$$\n$$1\\le y_i \\le 1$$\n$$1\\le i\\le n$$\n具K, C的soft margin SVM之對偶式(Loss(target) funtion, Constraint(條件、約束))為:\n$\\max_{\\alpha_i} \\sum_{i=1}^{n} \\alpha i -\\frac{1}{2}\\sum{i,j=1}^{n} \\alpha_i \\alpha_j y_i y_jK(\\mathrm {x}_i, \\mathrm {x}_j),$\n$s.t. \\sum_{i=1}^{n} \\alpha_iy_i=0, 0\\le \\alpha_i\\le C, i=1,\\dots ,n.$\nK is (\u0026lt;, \u0026gt; 矩陣乘法，處裡非線性可分, $\\phi$是一個mapping)\n$K(\\mathrm {x}_i, \\mathrm {x}_j) = \u0026lt; \\phi(\\mathrm {x}_i), \\phi(\\mathrm {x}_j)\u0026gt;$\n如上式所述，SVM的複雜度取決於訓練樣本數，為$O(n)$。並且因K函數的使用不受dim of feature space影響，然而K函數的計算複雜性被忽略了其取決於dim of input space，就算在最佳分割超平面的情況K函數的計算複雜性也無法被省略。因此降維必定可以使訓練SVM和預測帶來更高效率。\n該論文假設文件集合表示為Document-Term Matrix(Bag of Word)，加權兩倍，假設資料的聚類已經進行。\n下一章回顧LSI，使用svd分解做a的低秩近似，但忽略了資料的聚類結構。第三節中，回顧幾種對聚類資料特別有效的降維演算法:兩種群中心方法和使用GSVD(廣義奇異值分解)的泛化LDA。通過降維SVM()和K-mens(計算向量對距離)等分類器的計算複雜度皆可大降低。\n多數文本資料集中，文本可被分入多個類，為更有效處理此問題，在第四節中介紹基於閥值的分類算法擴展，實驗表明，該論文提出的cluster preserving降維演算法沒有造成訊息損失，反而提升了分類器的預測精度(推斷具有去噪效果)。\n低秩近似使用隱含語意索引 LSI假設:Document-Term Matrix中存在隱含的語意結構，其被文件中出現各種詞所破壞(polysemy and synonymy)。基本概念:若兩個文檔向量代表同一主題，其會共享許多與關鍵詞相關的關聯詞，通過SVD其語意結構會十分接近(term vectors表示為左奇異向量document vectors表示為右奇異向量)。然而，LSI再降維時忽略了聚類結構，且並沒有理論最佳的參數選擇，需多次實驗來確定最佳維度(如第五章所述)。實驗結果證實，當資料已被聚類時，下節介紹的降維法對新資料的分類效果更好。\n聚類資料的降維演算法 為提升高維度資料的處裡效率，須將資料降維，此節回顧三種保留聚類結構的降維算法\nCentroid-based Algorithms for Dimension Reduction of Clustered Data 給定一Document-Term Matrix，找一變換映射每個document vector從m維空間降到l維空間(m\u0026gt;l)，兩種方法:\n線性轉換($G^T_{l\\times m}$) 低秩近似(分解為兩個矩陣) $A\\approx BY$\n只要計算給定資料的降維表示，就無須從B計算降維變換G，若確定矩陣B，Y即可用最小平方法求解。\n$\\min_{B,Y}\\left|BY-A\\right|_{F.}$\n給定任意文本$q\\in\\mathbb{R}^{m\\times1}$透過解最小化問題轉換到低維空間。\n$\\min_{\\hat{q}\\in\\mathbb{R}^{l\\times1}}\\left| B\\hat{q}-q\\right|_{2.}$\n在Centroid降維法中(A1)，B的第$i, (1\\le i\\le p)$列是第$i$個群的中心(均值中心)點向量，任一向量$q$，可在$p$維空間表示成$\\hat{q}$即為最小平方法的解。\n在Orthogonal Centroid演算法中(A2)，使用$p$維表示資料向量$q\\in \\mathbb{R}^{m\\times 1}$被給定為$\\hat{q}=Q_{p}^{T}q$，$Q_p$為$B$的正交基底(QR分解)。\n以上兩種等Centroid-based降維法在計算成本比LSI更低，且在聚類資料下的效果更好。雖然此方案只能在線性可分的資料使用，但文本資料扔然可用，因文本資料通常是線性可分的(非線性可分in 18)\nGeneralized Discriminant Analysis based on the Generalized Singular Value Decomposition 近期(2003)，出現了GSVD base 的 cluster-preserving降維法，使SVM泛化到高維空間資料。\n經典判別分析透過最大化聚類之間的散度和最小化即群內的散度，維持聚類結構。為此，其定義聚類內散度矩陣$S_w$和聚類間散度矩陣$S_b$，$N_i$表示集群$i$的列索引集合$n_i$表示集群$i$的列數，$C$表全域中心點。目標使群內散度最小化，和降維後群間散度最大化。再次請出$G^T\\in \\mathbb{R}^{l\\times m}$將A的每列m維向量映射到l維的變換，目標表示為最小化$trace(G^T S_wG)$和最大化$trace(G^T S_bG)$。\n當$S_w$可逆(nonsingular、invertible)，可視為解最大化問題。 全局最大成立於:$G$的列是$S_{w}^{-1}S_b$的特徵向量並對應於$l$個最大特徵值。 when $l\\le p-1$ is equals $λ_1 + \\dots +λ_{p−1},$ each $λ_i ≥ 0$。設Document-Term Matrix $A$被分為$A=[A_1,\\ \\dots , \\ A_p]$，$A_i \\in \\mathbb{R}^{m\\times n_i}\\ \\text{in cluster}\\ i$\n$H_w = [a_1-c_1, a_2-c_2,\\dots ,a_n-c_p]\\in \\mathbb{R}^{m\\times n}$\n$H_b = [\\sqrt{n_1}(c_1-c),\\dots ,\\sqrt{n_p}(c_p-c)]\\in \\mathbb{R}^{m\\times p}$\n$S_w=H_wH_w^T\\ \u0026amp; \\ S_b=H_bH_b^T$\n當詞數(terms) $m$ \u0026gt; 文本數(doc) $n$，$S_w$不可逆(singular)，經典SVM失效。將問題(特徵值) $S_w^{-1}S_b\\mathrm{x}_i = λ_i\\mathrm{x}^i$ 改寫為 $\\mathrm\\beta_i^2H_bH_b^T\\mathrm{x}_i = \\mathrm\\alpha_i^2H_wH_w^T\\mathrm{x}_i$即可透過GSVD處理(LDA/GSVD in A3)。其中最複雜的計算部分複合矩陣$H$的完全正交分解，當$\\max (p, n)\\ll m$，$H=[H_b^T,H_w^T]\\in \\mathbb{R}^{(p+n)\\times m}$ 的SVD分解可被計算為:\n計算$H_t$的QR分解$Q_HR_H$ 計算$R_H\\in \\mathbb{R}^{(p+n)\\times (p+n)}$ 的SVD分解 $=Z\\begin{pmatrix} \\sum_H \u0026amp; 0 \\ 0 \u0026amp; 0 \\end{pmatrix}P^T$，使$H=R_H^TQ_H^T=P\\begin{pmatrix} \\sum_H \u0026amp; 0 \\ 0 \u0026amp; 0 \\end{pmatrix}Z^TQ_H^T$。其中$Q_HZ\\in\\mathbb{R}^{m\\times (p+n)}$的列之間為標準正交的(內積0、距離1)，存在正交$Q\\in \\mathbb{R} ^{m\\times m}$其前$p+n$列等同$Q_HZ$。將式整理為：\n$H = P\\begin{pmatrix} \\sum_H \u0026amp; 0 \\ 0 \u0026amp; 0 \\end{pmatrix}Q^T$\n式中$\\sum_H$的右邊有$m-t$個0列，因$R_H\\in \\mathbb{R}^{(p+n)\\times (p+n)}$遠小於$H\\in \\mathbb{R}^{(p+n)\\times m}$，記憶體需求大減，複雜度也降至$O(mn^2)+O(n^3)$ 分類方法 為測試降維效果，使用三種分類器測試:中心點分類、kNN和SVMs。皆引入閥值進行修改，以確保文本被判有多重類別資格時可以正確分類。\nCentroid-based 設新的文本資料為$q$，訓練資料共有$p$個群，$c_i$為第$i$個群的中心點向量:\n$arg \\max_{1\\le i\\le p} \\frac{q^Tc_i}{|q|_2 |c_i|_2}$\n多類別擴展($\\theta$為閥值):\n$y(\\mathrm{x}, j) = \\text{sign}{ sim(\\mathrm{x}, \\mathrm{c}_i)-\\theta_j^c }$\n$y(\\mathrm{x}, j)\\in { +1,-1 }$\n$$ \\begin{cases} \\text{Class is j} \u0026amp; \\text{:} \u0026amp; y(\\mathrm{x}, j)\u0026gt;0 \\ \\text{Class is not j} \u0026amp; \\text{:} \u0026amp; y(\\mathrm{x}, j)\\le 0 \\end{cases} $$\nk-Nearest Neighbor 設新的文本資料為$q$，訓練文本資料共有$p$個群:\n在訓練資料中，使用餘弦相似度計算與$q$最近的k個文本向量 在這k個向量中，計算屬於各個群的數量，$q$將被分配到最多的那個。 多類別擴展($\\theta$為閥值，$kNN$為文本$x$的$k$個鄰近向量集合):\n$y(\\mathrm{x}, j) = \\text{sign}{ \\sum_{\\mathrm{d}_i\\in kNN} sim(\\mathrm{x}, \\mathrm{d}_i) y(\\mathrm{d}_i, j) -\\theta_j^{kNN} }$\nSVM OvR策略(為每個Class建一個分類器)的二元分類器的最佳分割超平面可透conventional SVM取得。引入多類別擴展:\n$$ y(\\mathrm{x}, j) = \\text{sign}{ \\sum_{\\mathrm{x}_i\\in SV} \\alpha_i y_i K(\\mathrm{x}, \\mathrm{x}_i)+ b -\\theta_j^{SVM} }\\ K=\u0026lt;\\mathrm{x}, \\mathrm{x}_i\u0026gt; \\ K=[\u0026lt;\\mathrm{x}, \\mathrm{x}_i\u0026gt;+1]^d \\ K=\\exp(-\\gamma|\\mathrm{x}, \\mathrm{x}_i|^2) $$\n$SV$為支援向量的集合，$\\theta$為閥值，$\\gamma$與高斯函數寬度成反比。\n實驗結果 預測結果包含:\n無降維 LSI/SVD Centroid Orthogonal Centroid LDA/GSVD SVM優化:\n正則參數$C$ polynomial 角度$d$ Gaussian RBF $\\gamma$ 資料集 subset of MEDLINE database: 5個class、每個class各有500份文本、每份文本只有一個class、train:test = 50:50、做詞型還原和處理剔除字(此為國家教育研究院翻譯版，俗稱為停用詞)後訓練集有22095個不重複的詞。 Reuter-21578 文本集的\u0026quot;ModApte\u0026quot;分割: 90個class、每份文本可能有多個class、每個class至少有一個train和一個test、共7769個train和3019個tess、做詞型還原和處理剔除字(此為國家教育研究院翻譯版，俗稱為停用詞)後訓練集有11941個不重複的詞、引入閥值模型。 DTM不用BoW，改用TF-IDF並歸一化。\n表一 對MEDLINE資料集使用LSI/SVD，用使用centroid-based, kNN和SVMs分類器分類的結果。觀察到:\nkNN使用L2 norm在$l$為100-500時效果不佳，與餘弦相似性在未正則化資料表現更好的印象相符，且5NN明顯落後其他更高的K，表明k=5太小。 表二 SVM使用不同的K(核)函數與降維演算法在MEDLINE資料集的結果。觀察到:\n降維後的預測結果與原始空間的預測結果相似且複雜度降低。 正交中心降維演算法對K函數的選擇不敏感，可以選複雜度更低的線性K函數。 表三 選擇不同分類演算法與降維演算法在MEDLINE資料集的結果。觀察到:\n使用LDA/GSVD降維演算法時，使用餘弦相似度的centroid-based與kNN分類演算法效果較差，而使用L2-norm的效果較好，因跡最佳化使用L2-norm表示。 因LDA/GSVD最小化群內散點的跡(距離)，自同一(相似)類別的文檔向量會被變換為一個緻密的群甚至一個點，使得SVM難以找到泛化性高的超平面。 表四 MEDLINE資料集中使用SVM配合不同降維演算法在5個不同類別中的準確率。觀察到:\ncolon cancer與oral cancer難以被區分。 表五 選擇不同分類演算法與降維演算法(Centroid、Orthogonal Centroid)在REUTERS資料集的結果。觀察到:\nOrthogonal Centroid 效果無明顯下降，Centroid則明顯降低，推測因將各個聚類中心映射至單位矩陣導致每個class間的資訊消失所造成。 結論與討論 本文使用三種降維方法Centroid、Orthogonal Centroid和LDA/GSVD，皆是為集群資料所設計，做為比較也使用了LSI/SVD這種不保留集群結構的降維演算法。其也測試了三種不同的分類演算法SVMs、kNN與centroid-based classification測試在不同降維法中的分類效果。測試結果取得了高的準度，即使在強力的降維下，也可與未降維的狀態近似。\n該論文還引入了基於閥值的分類器，用於centroid-based和SVM做到一對多的分類。centroid在Class互不關聯的情況下表現更好。\n結論 不犧牲精度的情況下，可以做到對文本大幅降為。Orthogonal Centroid在保留聚類結構的能力做的極佳，降為前後的預測準確率幾乎不變，在KNN或SVM使用可以極大減少計算複雜度。\n","permalink":"https://lin-roger.github.io/posts/dimensionreductionintextclassificationwithsupportvectormachines/","summary":"非經典古文，可以不用看，主要是我的筆記。SVM被公認是許多任務中效果最好的分類方法之一，SVM的學習能力和訓練計算複雜度與特徵空間維度無關，但在文本分類任務中，降低複雜度是有效處裡大量詞語的一個要點。該論文採用新的降維方法降低文檔向量的維度。還為基於中心的分類算法和SVM分類器引入決策函數，處理一個文檔可能屬於多個class的問題。分析大量的實驗結果表明使用為聚類資料設計的降維算法，使輸入維度降低，可在不犧牲預測精度的情況下取得更好的訓練效率。","title":"Dimension Reduction in Text Classification with Support Vector Machines"}]