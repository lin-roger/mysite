<!DOCTYPE html>
<html lang="zh-hant" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@200;300;400;500;600;700;900&display=swap" rel="stylesheet">
<meta name="robots" content="index, follow">
<title>Reinforcement Learning | Flowey &amp; Code</title>
<meta name="keywords" content="Reinforcement Learning">
<meta name="description" content="從馬可夫決策過程到雙重深度Q網路">
<meta name="author" content="">
<link rel="canonical" href="https://lin-roger.github.io/posts/reinforcementlearning/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.1b6ea1bd2f99c94cb5f9c51aedab930c41a52c8c34bf10bc587b0fc7301eb0df.css" integrity="sha256-G26hvS&#43;ZyUy1&#43;cUa7auTDEGlLIw0vxC8WHsPxzAesN8=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js" integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://lin-roger.github.io/flowey.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://lin-roger.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lin-roger.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lin-roger.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://lin-roger.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    
</noscript>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<meta property="og:title" content="Reinforcement Learning" />
<meta property="og:description" content="從馬可夫決策過程到雙重深度Q網路" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lin-roger.github.io/posts/reinforcementlearning/" />
<meta property="og:image" content="https://lin-roger.github.io/cover.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-27T15:36:25+08:00" />
<meta property="article:modified_time" content="2024-01-27T15:36:25+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://lin-roger.github.io/cover.png" />
<meta name="twitter:title" content="Reinforcement Learning"/>
<meta name="twitter:description" content="從馬可夫決策過程到雙重深度Q網路"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://lin-roger.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Reinforcement Learning",
      "item": "https://lin-roger.github.io/posts/reinforcementlearning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Reinforcement Learning",
  "name": "Reinforcement Learning",
  "description": "從馬可夫決策過程到雙重深度Q網路",
  "keywords": [
    "Reinforcement Learning"
  ],
  "articleBody": "馬可夫決策過程 強化學習(Reinforcement Learning, RL)藉由互動學習如何行動以實現目標，而馬可夫決策過程(Markov decision processes, MDPs)，是RL數學化的一種理想表示。MDPs中，代理(agent)與其環境(environment)在一連串分立的時步互動，其中代理是完全可知且可控的，而環境是不完全可控且很有可能難以理解的。代理與環境之互動遵循以下定義:\n代理選擇動作(action) 動作的選擇基於狀態(states) 選擇的驗證基於回饋(rewards) 策略(policy)定義代理在給定狀態$S_t$下如何選擇動作$A_t$，而代理的目標為最大化期望回饋$G_t$。\n$$G_t = R_{t+1}+R_{t+2}+R_{t+3}+\\dotsb+R_T$$ $T$為最終時步，該形式如果再$T$極大時令$G_t$發散至無窮，所以實務上會使用以下形式:\n$$ G_t = R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\dotsb=\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}, \\text{ in } 0 \\le \\gamma \\le 1 $$\n$\\gamma$為衰減係數。上式以遞迴形式表達:\n$$ \\begin{align*} G_t \u0026= R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\dotsb \\newline \u0026= R_{t+1}+\\gamma (R_{t+2}+\\gamma R_{t+3}+\\dotsb) \\newline \u0026= R_{t+1}+\\gamma G_{t+1} \\end{align*} $$\n範例: $$ \\begin{align*} \\mathcal{S} \u0026= \\set{ \\text{high}, \\text{low} } \\newline \\mathcal{R} \u0026= \\set{ -3, 0, r_{\\text{wait}}, r_{\\text{search}} } \\newline \\mathcal{A(\\text{hight})} \u0026= \\set{ \\text{search}, \\text{wait} } \\newline \\mathcal{A(\\text{low})} \u0026= \\set{ \\text{search}, \\text{wait}, \\text{recharge} } \\newline \\end{align*} $$\nMDPs以表表示:\n以轉移機率圖表示:\n策略與價值函數 大部分的強化學習演算法需要價值函數，其估計代理在給定的狀態(或狀態-操作對)在未來的期望回饋總和，與策略有關。\n策略將狀態$S$映射為每個可能操作的機率，如果代理在時間$t$遵循策略$\\pi$，則$\\pi(a|s)$即為在狀態$S_t=s$下操作為$A_t=a$機率。\n策略$\\pi$的狀態-價值函數 :\n$$ \\begin{align*} v_{\\pi}(s) \u0026= \\mathbb{E}{\\pi}[G_t|S_t=s] \\newline \u0026= \\mathbb{E}{\\pi}[\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}|S_t=s], \\text{for all } s \\in \\mathcal{S} \\end{align*} $$\n策略$\\pi$的動作-價值函數 :\n$$ q_{\\pi}(s, a) = \\mathbb{E}{\\pi}[G_t|S_t=s, A_t=a] = \\mathbb{E}{\\pi}[\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}|S_t=s, A_t=a] $$\n最佳化策略和最佳化價值函數 解RL任務即為找一可以在長久運行下取得大量回饋的策略。\n",
  "wordCount" : "122",
  "inLanguage": "zh-hant",
  "image":"https://lin-roger.github.io/cover.png","datePublished": "2024-01-27T15:36:25+08:00",
  "dateModified": "2024-01-27T15:36:25+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lin-roger.github.io/posts/reinforcementlearning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Flowey \u0026 Code",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lin-roger.github.io/flowey.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lin-roger.github.io/" accesskey="h" title="Flowey &amp; code (Alt + H)">
                <img src="https://lin-roger.github.io/flowey.png" alt="" aria-label="logo"
                    height="30">Flowey &amp; code</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lin-roger.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://lin-roger.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://lin-roger.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://lin-roger.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://lin-roger.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Reinforcement Learning
    </h1>
    <div class="post-meta"><span title='2024-01-27 15:36:25 +0800 CST'>1月 27, 2024</span>&nbsp;·&nbsp;1 min

</div>
  </header> 
<figure class="entry-cover">
        <img loading="lazy" srcset="https://lin-roger.github.io/posts/reinforcementlearning/cover_hu81d0d780715f9e72bfc0523331f5b083_26082_360x0_resize_box_3.png 360w ,https://lin-roger.github.io/posts/reinforcementlearning/cover_hu81d0d780715f9e72bfc0523331f5b083_26082_480x0_resize_box_3.png 480w ,https://lin-roger.github.io/posts/reinforcementlearning/cover.png 547w" 
            sizes="(min-width: 768px) 720px, 100vw" src="https://lin-roger.github.io/posts/reinforcementlearning/cover.png" alt="強化學習中代理與環境的互動" 
            width="547" height="180">
        <p>強化學習中代理與環境的互動</p>
</figure>
  <button class="toc-btn noShowToc" id="toc-btn">
    <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="bi bi-square-half on"
      viewBox="0 0 16 16">
      <path
        d="M8 15V1h6a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1zm6 1a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H2a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2z" />
    </svg>
    <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="bi bi-x-square off"
      viewBox="0 0 16 16">
      <path
        d="M14 1a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1zM2 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2z" />
      <path
        d="M4.646 4.646a.5.5 0 0 1 .708 0L8 7.293l2.646-2.647a.5.5 0 0 1 .708.708L8.707 8l2.647 2.646a.5.5 0 0 1-.708.708L8 8.707l-2.646 2.647a.5.5 0 0 1-.708-.708L7.293 8 4.646 5.354a.5.5 0 0 1 0-.708" />
    </svg>
  </button><aside id="toc-container" class="toc-container">
        <div class="inc">
            <div class="toc" id="toc">
                <details>
                    <summary accesskey="c" title="(Alt + C)">
                        <span class="details">Table of Contents</span>
                    </summary>
                    <div class="inner"><ul>
                            <li>
                                <a href="#%e9%a6%ac%e5%8f%af%e5%a4%ab%e6%b1%ba%e7%ad%96%e9%81%8e%e7%a8%8b" aria-label="馬可夫決策過程">馬可夫決策過程</a><ul>
                                        
                            <li>
                                <a href="#%e7%ad%96%e7%95%a5%e8%88%87%e5%83%b9%e5%80%bc%e5%87%bd%e6%95%b8" aria-label="策略與價值函數">策略與價值函數</a></li></ul>
                            </li>
                            <li>
                                <a href="#%e6%9c%80%e4%bd%b3%e5%8c%96%e7%ad%96%e7%95%a5%e5%92%8c%e6%9c%80%e4%bd%b3%e5%8c%96%e5%83%b9%e5%80%bc%e5%87%bd%e6%95%b8" aria-label="最佳化策略和最佳化價值函數">最佳化策略和最佳化價值函數</a>
                            </li>
                        </ul>
                    </div>
                </details>
            </div>
            <hr>
            <div class="toc" id="loi">
                <details>
                    <summary>
                        <span class="details">List of Illustrations</span>
                    </summary>
                    <div class="inner">
                
                        <img src="table1.png" alt="table1" loading="lazy">
                
                        <img src="fig1.png" alt="fig1" loading="lazy"></div>
                </details>
            </div>
        </div>

    </aside>
    <script>
        let activeElement;
        let elements;

        document.querySelectorAll('.toc details').forEach((details, idx) => {
            details.addEventListener("toggle", (event) => {
                if (details.open) {
                    document.querySelectorAll('.toc details').forEach((d, i) => {
                        console.log(i, idx)
                        if (i != idx) {
                            d.open = null;
                        }
                    });
                } else {
                    console.log("close")
                }
            });
        });


        window.addEventListener('DOMContentLoaded', function (event) {
            

            elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
            
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
        }, false);

        window.addEventListener('scroll', () => {
            
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                    (getOffsetTop(element) - window.pageYOffset) < window.innerHeight / 2) {
                    return element;
                }
            }) || activeElement

            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                if (element === activeElement) {
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
                } else {
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
                }
            })
        }, false);

        function getOffsetTop(element) {
            if (!element.getClientRects().length) {
                return 0;
            }
            let rect = element.getBoundingClientRect();
            let win = element.ownerDocument.defaultView;
            return rect.top + win.pageYOffset;
        }
    </script>
  <div class="post-content"><h2 id="馬可夫決策過程">馬可夫決策過程<a hidden class="anchor" aria-hidden="true" href="#馬可夫決策過程">#</a></h2>
<p>強化學習(Reinforcement Learning, RL)藉由互動學習如何行動以實現目標，而馬可夫決策過程(Markov decision processes, MDPs)，是RL數學化的一種理想表示。MDPs中，代理(agent)與其環境(environment)在一連串分立的時步互動，其中代理是完全可知且可控的，而環境是不完全可控且很有可能難以理解的。代理與環境之互動遵循以下定義:</p>
<ol>
<li>代理選擇動作(action)</li>
<li>動作的選擇基於狀態(states)</li>
<li>選擇的驗證基於回饋(rewards)</li>
</ol>
<p>策略(policy)定義代理在給定狀態$S_t$下如何選擇動作$A_t$，而代理的目標為最大化期望回饋$G_t$。</p>
<p>$$G_t = R_{t+1}+R_{t+2}+R_{t+3}+\dotsb+R_T$$
$T$為最終時步，該形式如果再$T$極大時令$G_t$發散至無窮，所以實務上會使用以下形式:</p>
<p>$$
G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dotsb=\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}, \text{ in }
0 \le \gamma \le 1
$$</p>
<p>$\gamma$為衰減係數。上式以遞迴形式表達:</p>
<p>$$
\begin{align*}
G_t &amp;= R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dotsb \newline
&amp;= R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+3}+\dotsb) \newline
&amp;= R_{t+1}+\gamma G_{t+1}
\end{align*}
$$</p>
<p>範例:
$$
\begin{align*}
\mathcal{S} &amp;= \set{ \text{high}, \text{low} } \newline
\mathcal{R} &amp;= \set{ -3, 0, r_{\text{wait}}, r_{\text{search}} } \newline
\mathcal{A(\text{hight})} &amp;= \set{ \text{search}, \text{wait} } \newline
\mathcal{A(\text{low})} &amp;= \set{ \text{search}, \text{wait}, \text{recharge} } \newline
\end{align*}
$$</p>
<p>MDPs以表表示:</p>
<p><img loading="lazy" src="table1.png" alt="table1"  />
</p>
<p>以轉移機率圖表示:</p>
<p><img loading="lazy" src="fig1.png" alt="fig1"  />
</p>
<h3 id="策略與價值函數">策略與價值函數<a hidden class="anchor" aria-hidden="true" href="#策略與價值函數">#</a></h3>
<p>大部分的強化學習演算法需要<em><strong>價值函數</strong></em>，其估計代理在給定的狀態(或狀態-操作對)在未來的期望回饋總和，與策略有關。</p>
<p><em><strong>策略</strong></em>將狀態$S$映射為每個可能操作的機率，如果代理在時間$t$遵循策略$\pi$，則$\pi(a|s)$即為在狀態$S_t=s$下操作為$A_t=a$機率。</p>
<p>策略$\pi$的<em>狀態-價值函數</em> :</p>
<p>$$
\begin{align*}
v_{\pi}(s) &amp;= \mathbb{E}<em>{\pi}[G_t|S_t=s] \newline
&amp;= \mathbb{E}</em>{\pi}[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}|S_t=s], \text{for all } s \in \mathcal{S}
\end{align*}
$$</p>
<p>策略$\pi$的<em>動作-價值函數</em> :</p>
<p>$$
q_{\pi}(s, a) = \mathbb{E}<em>{\pi}[G_t|S_t=s, A_t=a] = \mathbb{E}</em>{\pi}[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}|S_t=s, A_t=a]
$$</p>
<h2 id="最佳化策略和最佳化價值函數">最佳化策略和最佳化價值函數<a hidden class="anchor" aria-hidden="true" href="#最佳化策略和最佳化價值函數">#</a></h2>
<p>解RL任務即為找一可以在長久運行下取得大量回饋的策略。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lin-roger.github.io/tags/reinforcement-learning/">Reinforcement Learning</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://lin-roger.github.io/posts/selficlzeroshotincontextlearningwithselfgenerateddemonstrations/">
    <span class="title"> »</span>
    <br>
    <span>SELF-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor"
        class="bi bi-arrow-up-square-fill" viewBox="0 0 16 16">
        <path
            d="M2 16a2 2 0 0 1-2-2V2a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v12a2 2 0 0 1-2 2zm6.5-4.5V5.707l2.146 2.147a.5.5 0 0 0 .708-.708l-3-3a.5.5 0 0 0-.708 0l-3 3a.5.5 0 1 0 .708.708L7.5 5.707V11.5a.5.5 0 0 0 1 0" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

    let tocBtn = document.getElementById("toc-btn");
    tocBtn.addEventListener('click', () => {
        tocBtn.classList.toggle("noShowToc");
        tocBtn.classList.toggle("showToc")
        document.getElementById("toc-container").classList.toggle("show");
    }, false)

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > document.documentElement.clientHeight || document.documentElement.scrollTop > document.documentElement.clientHeight) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script></body>

</html>
