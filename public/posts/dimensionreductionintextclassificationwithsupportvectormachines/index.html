<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif+TC:wght@300;400;500;600;700;900&display=swap" rel="stylesheet">
<meta name="robots" content="index, follow">
<title>Dimension Reduction in Text Classification with Support Vector Machines | Flowey &amp; Code</title>
<meta name="keywords" content="">
<meta name="description" content="非經典古文，可以不用看，主要是我的筆記。SVM被公認是許多任務中效果最好的分類方法之一，SVM的學習能力和訓練計算複雜度與特徵空間維度無關，但在文本分類任務中，降低複雜度是有效處裡大量詞語的一個要點。該論文採用新的降維方法降低文檔向量的維度。還為基於中心的分類算法和SVM分類器引入決策函數，處理一個文檔可能屬於多個class的問題。分析大量的實驗結果表明使用為聚類資料設計的降維算法，使輸入維度降低，可在不犧牲預測精度的情況下取得更好的訓練效率。">
<meta name="author" content="">
<link rel="canonical" href="https://lin-roger.github.io/posts/dimensionreductionintextclassificationwithsupportvectormachines/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.aed8896a61b8e86e60ce83091abbc650813eeb132d9b9c63e731e509c8994ace.css" integrity="sha256-rtiJamG46G5gzoMJGrvGUIE&#43;6xMtm5xj5zHlCciZSs4=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js" integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://lin-roger.github.io/flowey.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://lin-roger.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lin-roger.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lin-roger.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://lin-roger.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: none;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>


<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false},
                {left: "\\[", right: "\\]", display: true}
            ]
        });
    });
</script>
<meta property="og:title" content="Dimension Reduction in Text Classification with Support Vector Machines" />
<meta property="og:description" content="非經典古文，可以不用看，主要是我的筆記。SVM被公認是許多任務中效果最好的分類方法之一，SVM的學習能力和訓練計算複雜度與特徵空間維度無關，但在文本分類任務中，降低複雜度是有效處裡大量詞語的一個要點。該論文採用新的降維方法降低文檔向量的維度。還為基於中心的分類算法和SVM分類器引入決策函數，處理一個文檔可能屬於多個class的問題。分析大量的實驗結果表明使用為聚類資料設計的降維算法，使輸入維度降低，可在不犧牲預測精度的情況下取得更好的訓練效率。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lin-roger.github.io/posts/dimensionreductionintextclassificationwithsupportvectormachines/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-07T02:46:26+08:00" />
<meta property="article:modified_time" content="2023-06-26T02:46:26+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Dimension Reduction in Text Classification with Support Vector Machines"/>
<meta name="twitter:description" content="非經典古文，可以不用看，主要是我的筆記。SVM被公認是許多任務中效果最好的分類方法之一，SVM的學習能力和訓練計算複雜度與特徵空間維度無關，但在文本分類任務中，降低複雜度是有效處裡大量詞語的一個要點。該論文採用新的降維方法降低文檔向量的維度。還為基於中心的分類算法和SVM分類器引入決策函數，處理一個文檔可能屬於多個class的問題。分析大量的實驗結果表明使用為聚類資料設計的降維算法，使輸入維度降低，可在不犧牲預測精度的情況下取得更好的訓練效率。"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://lin-roger.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Dimension Reduction in Text Classification with Support Vector Machines",
      "item": "https://lin-roger.github.io/posts/dimensionreductionintextclassificationwithsupportvectormachines/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Dimension Reduction in Text Classification with Support Vector Machines",
  "name": "Dimension Reduction in Text Classification with Support Vector Machines",
  "description": "非經典古文，可以不用看，主要是我的筆記。SVM被公認是許多任務中效果最好的分類方法之一，SVM的學習能力和訓練計算複雜度與特徵空間維度無關，但在文本分類任務中，降低複雜度是有效處裡大量詞語的一個要點。該論文採用新的降維方法降低文檔向量的維度。還為基於中心的分類算法和SVM分類器引入決策函數，處理一個文檔可能屬於多個class的問題。分析大量的實驗結果表明使用為聚類資料設計的降維算法，使輸入維度降低，可在不犧牲預測精度的情況下取得更好的訓練效率。",
  "keywords": [
    
  ],
  "articleBody": "摘要 SVM被公認是許多任務中效果最好的分類方法之一，SVM的學習能力和訓練計算複雜度與特徵空間維度無關，但在文本分類任務中，降低複雜度是有效處裡大量詞語的一個要點。該論文採用新的降維方法降低文檔向量的維度。還為基於中心的分類算法和SVM分類器引入決策函數，處理一個文檔可能屬於多個class的問題。分析大量的實驗結果表明使用為聚類資料設計的降維算法，使輸入維度降低，可在不犧牲預測精度的情況下取得更好的訓練效率。\n簡介 文本分類是一項監督是任務，將文本分類到預定義的class，用來從大樣文本中尋找有價值的資訊，base on Vctor Space的方法有以下特性，input高維且稀疏(one hot, bag of word)，線性可分性(存在超平面將資料分割)，少數特徵不相關(多數相關)。有人猜測，積極降維會導致資訊嚴重損失，導致分類效果不佳。\n給定訓練資料:\n$$(x_i, y_i)$$\n$$1\\le y_i \\le 1$$\n$$1\\le i\\le n$$\n具K, C的soft margin SVM之對偶式(Loss(target) funtion, Constraint(條件、約束))為:\n$\\max_{\\alpha_i} \\sum_{i=1}^{n} \\alpha i -\\frac{1}{2}\\sum{i,j=1}^{n} \\alpha_i \\alpha_j y_i y_jK(\\mathrm {x}_i, \\mathrm {x}_j),$\n$s.t. \\sum_{i=1}^{n} \\alpha_iy_i=0, 0\\le \\alpha_i\\le C, i=1,\\dots ,n.$\nK is (\u003c, \u003e 矩陣乘法，處裡非線性可分, $\\phi$是一個mapping)\n$K(\\mathrm {x}_i, \\mathrm {x}_j) = \u003c \\phi(\\mathrm {x}_i), \\phi(\\mathrm {x}_j)\u003e$\n如上式所述，SVM的複雜度取決於訓練樣本數，為$O(n)$。並且因K函數的使用不受dim of feature space影響，然而K函數的計算複雜性被忽略了其取決於dim of input space，就算在最佳分割超平面的情況K函數的計算複雜性也無法被省略。因此降維必定可以使訓練SVM和預測帶來更高效率。\n該論文假設文件集合表示為Document-Term Matrix(Bag of Word)，加權兩倍，假設資料的聚類已經進行。\n下一章回顧LSI，使用svd分解做a的低秩近似，但忽略了資料的聚類結構。第三節中，回顧幾種對聚類資料特別有效的降維演算法:兩種群中心方法和使用GSVD(廣義奇異值分解)的泛化LDA。通過降維SVM()和K-mens(計算向量對距離)等分類器的計算複雜度皆可大降低。\n多數文本資料集中，文本可被分入多個類，為更有效處理此問題，在第四節中介紹基於閥值的分類算法擴展，實驗表明，該論文提出的cluster preserving降維演算法沒有造成訊息損失，反而提升了分類器的預測精度(推斷具有去噪效果)。\n低秩近似使用隱含語意索引 LSI假設:Document-Term Matrix中存在隱含的語意結構，其被文件中出現各種詞所破壞(polysemy and synonymy)。基本概念:若兩個文檔向量代表同一主題，其會共享許多與關鍵詞相關的關聯詞，通過SVD其語意結構會十分接近(term vectors表示為左奇異向量document vectors表示為右奇異向量)。然而，LSI再降維時忽略了聚類結構，且並沒有理論最佳的參數選擇，需多次實驗來確定最佳維度(如第五章所述)。實驗結果證實，當資料已被聚類時，下節介紹的降維法對新資料的分類效果更好。\n聚類資料的降維演算法 為提升高維度資料的處裡效率，須將資料降維，此節回顧三種保留聚類結構的降維算法\nCentroid-based Algorithms for Dimension Reduction of Clustered Data 給定一Document-Term Matrix，找一變換映射每個document vector從m維空間降到l維空間(m\u003el)，兩種方法:\n線性轉換($G^T_{l\\times m}$) 低秩近似(分解為兩個矩陣) $A\\approx BY$\n只要計算給定資料的降維表示，就無須從B計算降維變換G，若確定矩陣B，Y即可用最小平方法求解。\n$\\min_{B,Y}\\left|BY-A\\right|_{F.}$\n給定任意文本$q\\in\\mathbb{R}^{m\\times1}$透過解最小化問題轉換到低維空間。\n$\\min_{\\hat{q}\\in\\mathbb{R}^{l\\times1}}\\left| B\\hat{q}-q\\right|_{2.}$\n在Centroid降維法中(A1)，B的第$i, (1\\le i\\le p)$列是第$i$個群的中心(均值中心)點向量，任一向量$q$，可在$p$維空間表示成$\\hat{q}$即為最小平方法的解。\n在Orthogonal Centroid演算法中(A2)，使用$p$維表示資料向量$q\\in \\mathbb{R}^{m\\times 1}$被給定為$\\hat{q}=Q_{p}^{T}q$，$Q_p$為$B$的正交基底(QR分解)。\n以上兩種等Centroid-based降維法在計算成本比LSI更低，且在聚類資料下的效果更好。雖然此方案只能在線性可分的資料使用，但文本資料扔然可用，因文本資料通常是線性可分的(非線性可分in 18)\nGeneralized Discriminant Analysis based on the Generalized Singular Value Decomposition 近期(2003)，出現了GSVD base 的 cluster-preserving降維法，使SVM泛化到高維空間資料。\n經典判別分析透過最大化聚類之間的散度和最小化即群內的散度，維持聚類結構。為此，其定義聚類內散度矩陣$S_w$和聚類間散度矩陣$S_b$，$N_i$表示集群$i$的列索引集合$n_i$表示集群$i$的列數，$C$表全域中心點。目標使群內散度最小化，和降維後群間散度最大化。再次請出$G^T\\in \\mathbb{R}^{l\\times m}$將A的每列m維向量映射到l維的變換，目標表示為最小化$trace(G^T S_wG)$和最大化$trace(G^T S_bG)$。\n當$S_w$可逆(nonsingular、invertible)，可視為解最大化問題。 全局最大成立於:$G$的列是$S_{w}^{-1}S_b$的特徵向量並對應於$l$個最大特徵值。 when $l\\le p-1$ is equals $λ_1 + \\dots +λ_{p−1},$ each $λ_i ≥ 0$。設Document-Term Matrix $A$被分為$A=[A_1,\\ \\dots , \\ A_p]$，$A_i \\in \\mathbb{R}^{m\\times n_i}\\ \\text{in cluster}\\ i$\n$H_w = [a_1-c_1, a_2-c_2,\\dots ,a_n-c_p]\\in \\mathbb{R}^{m\\times n}$\n$H_b = [\\sqrt{n_1}(c_1-c),\\dots ,\\sqrt{n_p}(c_p-c)]\\in \\mathbb{R}^{m\\times p}$\n$S_w=H_wH_w^T\\ \u0026 \\ S_b=H_bH_b^T$\n當詞數(terms) $m$ \u003e 文本數(doc) $n$，$S_w$不可逆(singular)，經典SVM失效。將問題(特徵值) $S_w^{-1}S_b\\mathrm{x}_i = λ_i\\mathrm{x}^i$ 改寫為 $\\mathrm\\beta_i^2H_bH_b^T\\mathrm{x}_i = \\mathrm\\alpha_i^2H_wH_w^T\\mathrm{x}_i$即可透過GSVD處理(LDA/GSVD in A3)。其中最複雜的計算部分複合矩陣$H$的完全正交分解，當$\\max (p, n)\\ll m$，$H=[H_b^T,H_w^T]\\in \\mathbb{R}^{(p+n)\\times m}$ 的SVD分解可被計算為:\n計算$H_t$的QR分解$Q_HR_H$ 計算$R_H\\in \\mathbb{R}^{(p+n)\\times (p+n)}$ 的SVD分解 $=Z\\begin{pmatrix} \\sum_H \u0026 0 \\ 0 \u0026 0 \\end{pmatrix}P^T$，使$H=R_H^TQ_H^T=P\\begin{pmatrix} \\sum_H \u0026 0 \\ 0 \u0026 0 \\end{pmatrix}Z^TQ_H^T$。其中$Q_HZ\\in\\mathbb{R}^{m\\times (p+n)}$的列之間為標準正交的(內積0、距離1)，存在正交$Q\\in \\mathbb{R} ^{m\\times m}$其前$p+n$列等同$Q_HZ$。將式整理為：\n$H = P\\begin{pmatrix} \\sum_H \u0026 0 \\ 0 \u0026 0 \\end{pmatrix}Q^T$\n式中$\\sum_H$的右邊有$m-t$個0列，因$R_H\\in \\mathbb{R}^{(p+n)\\times (p+n)}$遠小於$H\\in \\mathbb{R}^{(p+n)\\times m}$，記憶體需求大減，複雜度也降至$O(mn^2)+O(n^3)$ 分類方法 為測試降維效果，使用三種分類器測試:中心點分類、kNN和SVMs。皆引入閥值進行修改，以確保文本被判有多重類別資格時可以正確分類。\nCentroid-based 設新的文本資料為$q$，訓練資料共有$p$個群，$c_i$為第$i$個群的中心點向量:\n$arg \\max_{1\\le i\\le p} \\frac{q^Tc_i}{|q|_2 |c_i|_2}$\n多類別擴展($\\theta$為閥值):\n$y(\\mathrm{x}, j) = \\text{sign}{ sim(\\mathrm{x}, \\mathrm{c}_i)-\\theta_j^c }$\n$y(\\mathrm{x}, j)\\in { +1,-1 }$\n$$ \\begin{cases} \\text{Class is j} \u0026 \\text{:} \u0026 y(\\mathrm{x}, j)\u003e0 \\ \\text{Class is not j} \u0026 \\text{:} \u0026 y(\\mathrm{x}, j)\\le 0 \\end{cases} $$\nk-Nearest Neighbor 設新的文本資料為$q$，訓練文本資料共有$p$個群:\n在訓練資料中，使用餘弦相似度計算與$q$最近的k個文本向量 在這k個向量中，計算屬於各個群的數量，$q$將被分配到最多的那個。 多類別擴展($\\theta$為閥值，$kNN$為文本$x$的$k$個鄰近向量集合):\n$y(\\mathrm{x}, j) = \\text{sign}{ \\sum_{\\mathrm{d}_i\\in kNN} sim(\\mathrm{x}, \\mathrm{d}_i) y(\\mathrm{d}_i, j) -\\theta_j^{kNN} }$\nSVM OvR策略(為每個Class建一個分類器)的二元分類器的最佳分割超平面可透conventional SVM取得。引入多類別擴展:\n$$ y(\\mathrm{x}, j) = \\text{sign}{ \\sum_{\\mathrm{x}_i\\in SV} \\alpha_i y_i K(\\mathrm{x}, \\mathrm{x}_i)+ b -\\theta_j^{SVM} }\\ K=\u003c\\mathrm{x}, \\mathrm{x}_i\u003e \\ K=[\u003c\\mathrm{x}, \\mathrm{x}_i\u003e+1]^d \\ K=\\exp(-\\gamma|\\mathrm{x}, \\mathrm{x}_i|^2) $$\n$SV$為支援向量的集合，$\\theta$為閥值，$\\gamma$與高斯函數寬度成反比。\n實驗結果 預測結果包含:\n無降維 LSI/SVD Centroid Orthogonal Centroid LDA/GSVD SVM優化:\n正則參數$C$ polynomial 角度$d$ Gaussian RBF $\\gamma$ 資料集 subset of MEDLINE database: 5個class、每個class各有500份文本、每份文本只有一個class、train:test = 50:50、做詞型還原和處理剔除字(此為國家教育研究院翻譯版，俗稱為停用詞)後訓練集有22095個不重複的詞。 Reuter-21578 文本集的\"ModApte\"分割: 90個class、每份文本可能有多個class、每個class至少有一個train和一個test、共7769個train和3019個tess、做詞型還原和處理剔除字(此為國家教育研究院翻譯版，俗稱為停用詞)後訓練集有11941個不重複的詞、引入閥值模型。 DTM不用BoW，改用TF-IDF並歸一化。\n表一 對MEDLINE資料集使用LSI/SVD，用使用centroid-based, kNN和SVMs分類器分類的結果。觀察到:\nkNN使用L2 norm在$l$為100-500時效果不佳，與餘弦相似性在未正則化資料表現更好的印象相符，且5NN明顯落後其他更高的K，表明k=5太小。 表二 SVM使用不同的K(核)函數與降維演算法在MEDLINE資料集的結果。觀察到:\n降維後的預測結果與原始空間的預測結果相似且複雜度降低。 正交中心降維演算法對K函數的選擇不敏感，可以選複雜度更低的線性K函數。 表三 選擇不同分類演算法與降維演算法在MEDLINE資料集的結果。觀察到:\n使用LDA/GSVD降維演算法時，使用餘弦相似度的centroid-based與kNN分類演算法效果較差，而使用L2-norm的效果較好，因跡最佳化使用L2-norm表示。 因LDA/GSVD最小化群內散點的跡(距離)，自同一(相似)類別的文檔向量會被變換為一個緻密的群甚至一個點，使得SVM難以找到泛化性高的超平面。 表四 MEDLINE資料集中使用SVM配合不同降維演算法在5個不同類別中的準確率。觀察到:\ncolon cancer與oral cancer難以被區分。 表五 選擇不同分類演算法與降維演算法(Centroid、Orthogonal Centroid)在REUTERS資料集的結果。觀察到:\nOrthogonal Centroid 效果無明顯下降，Centroid則明顯降低，推測因將各個聚類中心映射至單位矩陣導致每個class間的資訊消失所造成。 結論與討論 本文使用三種降維方法Centroid、Orthogonal Centroid和LDA/GSVD，皆是為集群資料所設計，做為比較也使用了LSI/SVD這種不保留集群結構的降維演算法。其也測試了三種不同的分類演算法SVMs、kNN與centroid-based classification測試在不同降維法中的分類效果。測試結果取得了高的準度，即使在強力的降維下，也可與未降維的狀態近似。\n該論文還引入了基於閥值的分類器，用於centroid-based和SVM做到一對多的分類。centroid在Class互不關聯的情況下表現更好。\n結論 不犧牲精度的情況下，可以做到對文本大幅降為。Orthogonal Centroid在保留聚類結構的能力做的極佳，降為前後的預測準確率幾乎不變，在KNN或SVM使用可以極大減少計算複雜度。\n",
  "wordCount" : "392",
  "inLanguage": "en",
  "datePublished": "2022-11-07T02:46:26+08:00",
  "dateModified": "2023-06-26T02:46:26+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lin-roger.github.io/posts/dimensionreductionintextclassificationwithsupportvectormachines/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Flowey \u0026 Code",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lin-roger.github.io/flowey.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lin-roger.github.io/" accesskey="h" title="Flowey &amp; code (Alt + H)">
                <img src="https://lin-roger.github.io/flowey.png" alt="" aria-label="logo"
                    height="30">Flowey &amp; code</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lin-roger.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://lin-roger.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://lin-roger.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://lin-roger.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://lin-roger.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Dimension Reduction in Text Classification with Support Vector Machines
    </h1>
    <div class="post-meta">&lt;span title=&#39;2022-11-07 02:46:26 &#43;0800 CST&#39;&gt;November 7, 2022&lt;/span&gt;&amp;nbsp;·&amp;nbsp;2 min

</div>
  </header> 
  <button class="toc-btn noShowToc" id="toc-btn">
    <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="bi bi-square-half on"
      viewBox="0 0 16 16">
      <path
        d="M8 15V1h6a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1zm6 1a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H2a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2z" />
    </svg>
    <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" class="bi bi-x-square off"
      viewBox="0 0 16 16">
      <path
        d="M14 1a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1zM2 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2z" />
      <path
        d="M4.646 4.646a.5.5 0 0 1 .708 0L8 7.293l2.646-2.647a.5.5 0 0 1 .708.708L8.707 8l2.647 2.646a.5.5 0 0 1-.708.708L8 8.707l-2.646 2.647a.5.5 0 0 1-.708-.708L7.293 8 4.646 5.354a.5.5 0 0 1 0-.708" />
    </svg>
  </button><aside id="toc-container" class="toc-container">
        <div class="toc">
            <details  open>
                <summary accesskey="c" title="(Alt + C)">
                    <span class="details">Table of Contents</span>
                </summary>

                <div class="inner"><ul>
                        <li>
                            <a href="#%e6%91%98%e8%a6%81" aria-label="摘要">摘要</a></li>
                        <li>
                            <a href="#%e7%b0%a1%e4%bb%8b" aria-label="簡介">簡介</a></li>
                        <li>
                            <a href="#%e4%bd%8e%e7%a7%a9%e8%bf%91%e4%bc%bc%e4%bd%bf%e7%94%a8%e9%9a%b1%e5%90%ab%e8%aa%9e%e6%84%8f%e7%b4%a2%e5%bc%95" aria-label="低秩近似使用隱含語意索引">低秩近似使用隱含語意索引</a></li>
                        <li>
                            <a href="#%e8%81%9a%e9%a1%9e%e8%b3%87%e6%96%99%e7%9a%84%e9%99%8d%e7%b6%ad%e6%bc%94%e7%ae%97%e6%b3%95" aria-label="聚類資料的降維演算法">聚類資料的降維演算法</a><ul>
                                    
                        <li>
                            <a href="#centroid-based-algorithms-for-dimension-reduction-of-clustered-data" aria-label="Centroid-based Algorithms for Dimension Reduction of Clustered Data">Centroid-based Algorithms for Dimension Reduction of Clustered Data</a></li>
                        <li>
                            <a href="#generalized-discriminant-analysis-based-on-the-generalized-singular-value-decomposition" aria-label="Generalized Discriminant Analysis based on the Generalized Singular Value Decomposition">Generalized Discriminant Analysis based on the Generalized Singular Value Decomposition</a></li></ul>
                        </li>
                        <li>
                            <a href="#%e5%88%86%e9%a1%9e%e6%96%b9%e6%b3%95" aria-label="分類方法">分類方法</a><ul>
                                    
                        <li>
                            <a href="#centroid-based" aria-label="Centroid-based">Centroid-based</a></li>
                        <li>
                            <a href="#k-nearest-neighbor" aria-label="k-Nearest Neighbor">k-Nearest Neighbor</a></li>
                        <li>
                            <a href="#svm" aria-label="SVM">SVM</a></li></ul>
                        </li>
                        <li>
                            <a href="#%e5%af%a6%e9%a9%97%e7%b5%90%e6%9e%9c" aria-label="實驗結果">實驗結果</a><ul>
                                    
                        <li>
                            <a href="#%e8%b3%87%e6%96%99%e9%9b%86" aria-label="資料集">資料集</a></li>
                        <li>
                            <a href="#%e8%a1%a8%e4%b8%80" aria-label="表一">表一</a></li>
                        <li>
                            <a href="#%e8%a1%a8%e4%ba%8c" aria-label="表二">表二</a></li>
                        <li>
                            <a href="#%e8%a1%a8%e4%b8%89" aria-label="表三">表三</a></li>
                        <li>
                            <a href="#%e8%a1%a8%e5%9b%9b" aria-label="表四">表四</a></li>
                        <li>
                            <a href="#%e8%a1%a8%e4%ba%94" aria-label="表五">表五</a></li></ul>
                        </li>
                        <li>
                            <a href="#%e7%b5%90%e8%ab%96%e8%88%87%e8%a8%8e%e8%ab%96" aria-label="結論與討論">結論與討論</a><ul>
                                    
                        <li>
                            <a href="#%e7%b5%90%e8%ab%96" aria-label="結論">結論</a>
                        </li>
                    </ul>
                    </li>
                    </ul>
                </div>
            </details>
        </div>
    </aside>
    <script>
        let activeElement;
        let elements;
        window.addEventListener('DOMContentLoaded', function (event) {
            

            elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
            
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
        }, false);

        window.addEventListener('scroll', () => {
            
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                    (getOffsetTop(element) - window.pageYOffset) < window.innerHeight / 2) {
                    return element;
                }
            }) || activeElement

            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                if (element === activeElement) {
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
                } else {
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
                }
            })
        }, false);

        function getOffsetTop(element) {
            if (!element.getClientRects().length) {
                return 0;
            }
            let rect = element.getBoundingClientRect();
            let win = element.ownerDocument.defaultView;
            return rect.top + win.pageYOffset;
        }
    </script>
  <div class="post-content"><h2 id="摘要">摘要<a hidden class="anchor" aria-hidden="true" href="#摘要">#</a></h2>
<p>SVM被公認是許多任務中效果最好的分類方法之一，SVM的學習能力和訓練計算複雜度與特徵空間維度無關，但在文本分類任務中，降低複雜度是有效處裡大量詞語的一個要點。該論文採用新的降維方法降低文檔向量的維度。還為基於中心的分類算法和SVM分類器引入決策函數，處理一個文檔可能屬於多個class的問題。分析大量的實驗結果表明使用為聚類資料設計的降維算法，使輸入維度降低，可在不犧牲預測精度的情況下取得更好的訓練效率。</p>
<h2 id="簡介">簡介<a hidden class="anchor" aria-hidden="true" href="#簡介">#</a></h2>
<p>文本分類是一項監督是任務，將文本分類到預定義的class，用來從大樣文本中尋找有價值的資訊，base on Vctor Space的方法有以下特性，input高維且稀疏(one hot, bag of word)，線性可分性(存在超平面將資料分割)，少數特徵不相關(多數相關)。有人猜測，積極降維會導致資訊嚴重損失，導致分類效果不佳。</p>
<p>給定訓練資料:</p>
<p>$$(x_i, y_i)$$</p>
<p>$$1\le y_i \le 1$$</p>
<p>$$1\le i\le n$$</p>
<p>具K, C的soft margin SVM之對偶式(Loss(target) funtion, Constraint(條件、約束))為:</p>
<p>$\max_{\alpha_i} \sum_{i=1}^{n} \alpha <em>i -\frac{1}{2}\sum</em>{i,j=1}^{n} \alpha_i \alpha_j y_i y_jK(\mathrm {x}_i, \mathrm {x}_j),$</p>
<p>$s.t. \sum_{i=1}^{n} \alpha_iy_i=0, 0\le \alpha_i\le C, i=1,\dots ,n.$</p>
<p>K is (&lt;, &gt; 矩陣乘法，處裡非線性可分, $\phi$是一個mapping)</p>
<p>$K(\mathrm {x}_i, \mathrm {x}_j) = &lt; \phi(\mathrm {x}_i), \phi(\mathrm {x}_j)&gt;$</p>
<p>如上式所述，SVM的複雜度取決於訓練樣本數，為$O(n)$。並且因K函數的使用不受dim of feature space影響，然而K函數的計算複雜性被忽略了其取決於dim of input space，就算在最佳分割超平面的情況K函數的計算複雜性也無法被省略。因此降維必定可以使訓練SVM和預測帶來更高效率。</p>
<p>該論文假設文件集合表示為Document-Term Matrix(Bag of Word)，加權兩倍，假設資料的聚類已經進行。</p>
<p>下一章回顧LSI，使用svd分解做a的低秩近似，但忽略了資料的聚類結構。第三節中，回顧幾種對聚類資料特別有效的降維演算法:兩種群中心方法和使用GSVD(廣義奇異值分解)的泛化LDA。通過降維SVM()和K-mens(計算向量對距離)等分類器的計算複雜度皆可大降低。</p>
<p>多數文本資料集中，文本可被分入多個類，為更有效處理此問題，在第四節中介紹基於閥值的分類算法擴展，實驗表明，該論文提出的cluster preserving降維演算法沒有造成訊息損失，反而提升了分類器的預測精度(推斷具有去噪效果)。</p>
<h2 id="低秩近似使用隱含語意索引">低秩近似使用隱含語意索引<a hidden class="anchor" aria-hidden="true" href="#低秩近似使用隱含語意索引">#</a></h2>
<p>LSI假設:Document-Term Matrix中存在隱含的語意結構，其被文件中出現各種詞所破壞(polysemy and synonymy)。基本概念:若兩個文檔向量代表同一主題，其會共享許多與關鍵詞相關的關聯詞，通過SVD其語意結構會十分接近(term vectors表示為左奇異向量document vectors表示為右奇異向量)。然而，LSI再降維時忽略了聚類結構，且並沒有理論最佳的參數選擇，需多次實驗來確定最佳維度(如第五章所述)。實驗結果證實，當資料已被聚類時，下節介紹的降維法對新資料的分類效果更好。</p>
<h2 id="聚類資料的降維演算法">聚類資料的降維演算法<a hidden class="anchor" aria-hidden="true" href="#聚類資料的降維演算法">#</a></h2>
<p>為提升高維度資料的處裡效率，須將資料降維，此節回顧三種保留聚類結構的降維算法</p>
<h3 id="centroid-based-algorithms-for-dimension-reduction-of-clustered-data">Centroid-based Algorithms for Dimension Reduction of Clustered Data<a hidden class="anchor" aria-hidden="true" href="#centroid-based-algorithms-for-dimension-reduction-of-clustered-data">#</a></h3>
<p>給定一Document-Term Matrix，找一變換映射每個document vector從m維空間降到l維空間(m&gt;l)，兩種方法:</p>
<ol>
<li>線性轉換($G^T_{l\times m}$)</li>
<li>低秩近似(分解為兩個矩陣)</li>
</ol>
<p>$A\approx BY$</p>
<p>只要計算給定資料的降維表示，就無須從B計算降維變換G，若確定矩陣B，Y即可用最小平方法求解。</p>
<p>$\min_{B,Y}\left|BY-A\right|_{F.}$</p>
<p>給定任意文本$q\in\mathbb{R}^{m\times1}$透過解最小化問題轉換到低維空間。</p>
<p>$\min_{\hat{q}\in\mathbb{R}^{l\times1}}\left| B\hat{q}-q\right|_{2.}$</p>
<p>在Centroid降維法中(A1)，B的第$i, (1\le i\le p)$列是第$i$個群的中心(均值中心)點向量，任一向量$q$，可在$p$維空間表示成$\hat{q}$即為最小平方法的解。</p>
<p>在Orthogonal Centroid演算法中(A2)，使用$p$維表示資料向量$q\in \mathbb{R}^{m\times 1}$被給定為$\hat{q}=Q_{p}^{T}q$，$Q_p$為$B$的正交基底(QR分解)。</p>
<p>以上兩種等Centroid-based降維法在計算成本比LSI更低，且在聚類資料下的效果更好。雖然此方案只能在線性可分的資料使用，但文本資料扔然可用，因文本資料通常是線性可分的(非線性可分in 18)</p>
<h3 id="generalized-discriminant-analysis-based-on-the-generalized-singular-value-decomposition">Generalized Discriminant Analysis based on the Generalized Singular Value Decomposition<a hidden class="anchor" aria-hidden="true" href="#generalized-discriminant-analysis-based-on-the-generalized-singular-value-decomposition">#</a></h3>
<p>近期(2003)，出現了GSVD base 的 cluster-preserving降維法，使SVM泛化到高維空間資料。</p>
<p>經典判別分析透過最大化聚類之間的散度和最小化即群內的散度，維持聚類結構。為此，其定義聚類內散度矩陣$S_w$和聚類間散度矩陣$S_b$，$N_i$表示集群$i$的列索引集合$n_i$表示集群$i$的列數，$C$表全域中心點。目標使群內散度最小化，和降維後群間散度最大化。再次請出$G^T\in \mathbb{R}^{l\times m}$將A的每列m維向量映射到l維的變換，目標表示為最小化$trace(G^T S_wG)$和最大化$trace(G^T S_bG)$。</p>
<p>當$S_w$可逆(nonsingular、invertible)，可視為解最大化問題。
全局最大成立於:$G$的列是$S_{w}^{-1}S_b$的特徵向量並對應於$l$個最大特徵值。
when $l\le p-1$ is equals $λ_1 + \dots +λ_{p−1},$ each $λ_i ≥ 0$。設Document-Term Matrix $A$被分為$A=[A_1,\ \dots , \ A_p]$，$A_i \in \mathbb{R}^{m\times n_i}\  \text{in cluster}\ i$</p>
<p>$H_w = [a_1-c_1, a_2-c_2,\dots ,a_n-c_p]\in \mathbb{R}^{m\times n}$</p>
<p>$H_b = [\sqrt{n_1}(c_1-c),\dots ,\sqrt{n_p}(c_p-c)]\in \mathbb{R}^{m\times p}$</p>
<p>$S_w=H_wH_w^T\ &amp; \ S_b=H_bH_b^T$</p>
<p>當詞數(terms) $m$ &gt; 文本數(doc) $n$，$S_w$不可逆(singular)，經典SVM失效。將問題(特徵值) $S_w^{-1}S_b\mathrm{x}_i = λ_i\mathrm{x}^i$ 改寫為 $\mathrm\beta_i^2H_bH_b^T\mathrm{x}_i = \mathrm\alpha_i^2H_wH_w^T\mathrm{x}_i$即可透過GSVD處理(LDA/GSVD in A3)。其中最複雜的計算部分複合矩陣$H$的完全正交分解，當$\max (p, n)\ll m$，$H=[H_b^T,H_w^T]\in \mathbb{R}^{(p+n)\times m}$ 的SVD分解可被計算為:</p>
<ol>
<li>計算$H_t$的QR分解$Q_HR_H$</li>
<li>計算$R_H\in \mathbb{R}^{(p+n)\times (p+n)}$ 的SVD分解 $=Z\begin{pmatrix} \sum_H &amp; 0 \ 0 &amp; 0 \end{pmatrix}P^T$，使$H=R_H^TQ_H^T=P\begin{pmatrix} \sum_H &amp; 0 \ 0 &amp; 0 \end{pmatrix}Z^TQ_H^T$。其中$Q_HZ\in\mathbb{R}^{m\times (p+n)}$的列之間為標準正交的(內積0、距離1)，存在正交$Q\in \mathbb{R} ^{m\times m}$其前$p+n$列等同$Q_HZ$。將式整理為：<br>
$H = P\begin{pmatrix} \sum_H &amp; 0 \ 0 &amp; 0 \end{pmatrix}Q^T$<br>
式中$\sum_H$的右邊有$m-t$個0列，因$R_H\in \mathbb{R}^{(p+n)\times (p+n)}$遠小於$H\in \mathbb{R}^{(p+n)\times m}$，記憶體需求大減，複雜度也降至$O(mn^2)+O(n^3)$</li>
</ol>
<h2 id="分類方法">分類方法<a hidden class="anchor" aria-hidden="true" href="#分類方法">#</a></h2>
<p>為測試降維效果，使用三種分類器測試:中心點分類、kNN和SVMs。皆引入閥值進行修改，以確保文本被判有多重類別資格時可以正確分類。</p>
<h3 id="centroid-based">Centroid-based<a hidden class="anchor" aria-hidden="true" href="#centroid-based">#</a></h3>
<p>設新的文本資料為$q$，訓練資料共有$p$個群，$c_i$為第$i$個群的中心點向量:</p>
<p>$arg \max_{1\le i\le p} \frac{q^Tc_i}{|q|_2 |c_i|_2}$</p>
<p>多類別擴展($\theta$為閥值):</p>
<p>$y(\mathrm{x}, j) = \text{sign}{ sim(\mathrm{x}, \mathrm{c}_i)-\theta_j^c }$<br>
$y(\mathrm{x}, j)\in { +1,-1 }$<br>
$$
\begin{cases}
\text{Class is j}     &amp; \text{:} &amp; y(\mathrm{x}, j)&gt;0  \
\text{Class is not j} &amp; \text{:} &amp; y(\mathrm{x}, j)\le 0
\end{cases}
$$</p>
<p><img loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/c/c0/Signum_function.png" alt="sign"  />
</p>
<h3 id="k-nearest-neighbor">k-Nearest Neighbor<a hidden class="anchor" aria-hidden="true" href="#k-nearest-neighbor">#</a></h3>
<p>設新的文本資料為$q$，訓練文本資料共有$p$個群:</p>
<ol>
<li>在訓練資料中，使用餘弦相似度計算與$q$最近的k個文本向量</li>
<li>在這k個向量中，計算屬於各個群的數量，$q$將被分配到最多的那個。</li>
</ol>
<p>多類別擴展($\theta$為閥值，$kNN$為文本$x$的$k$個鄰近向量集合):</p>
<p>$y(\mathrm{x}, j) = \text{sign}{ \sum_{\mathrm{d}_i\in kNN} sim(\mathrm{x}, \mathrm{d}_i) y(\mathrm{d}_i, j) -\theta_j^{kNN} }$</p>
<h3 id="svm">SVM<a hidden class="anchor" aria-hidden="true" href="#svm">#</a></h3>
<p>OvR策略(為每個Class建一個分類器)的二元分類器的最佳分割超平面可透conventional SVM取得。引入多類別擴展:</p>
<p>$$
y(\mathrm{x}, j) = \text{sign}{
\sum_{\mathrm{x}_i\in SV}
\alpha_i y_i K(\mathrm{x}, \mathrm{x}_i)+ b -\theta_j^{SVM} }\
K=&lt;\mathrm{x}, \mathrm{x}_i&gt; \
K=[&lt;\mathrm{x}, \mathrm{x}_i&gt;+1]^d \
K=\exp(-\gamma|\mathrm{x}, \mathrm{x}_i|^2)
$$</p>
<p>$SV$為支援向量的集合，$\theta$為閥值，$\gamma$與高斯函數寬度成反比。</p>
<h2 id="實驗結果">實驗結果<a hidden class="anchor" aria-hidden="true" href="#實驗結果">#</a></h2>
<p>預測結果包含:</p>
<ol>
<li>無降維</li>
<li>LSI/SVD</li>
<li>Centroid</li>
<li>Orthogonal Centroid</li>
<li>LDA/GSVD</li>
</ol>
<p>SVM優化:</p>
<ul>
<li>正則參數$C$</li>
<li>polynomial 角度$d$</li>
<li>Gaussian RBF $\gamma$</li>
</ul>
<h3 id="資料集">資料集<a hidden class="anchor" aria-hidden="true" href="#資料集">#</a></h3>
<ol>
<li>subset of MEDLINE database: 5個class、每個class各有500份文本、每份文本只有一個class、train:test = 50:50、做詞型還原和處理剔除字(此為國家教育研究院翻譯版，俗稱為停用詞)後訓練集有22095個不重複的詞。</li>
<li>Reuter-21578 文本集的&quot;ModApte&quot;分割: 90個class、每份文本可能有多個class、每個class至少有一個train和一個test、共7769個train和3019個tess、做詞型還原和處理剔除字(此為國家教育研究院翻譯版，俗稱為停用詞)後訓練集有11941個不重複的詞、引入閥值模型。</li>
</ol>
<p>DTM不用BoW，改用TF-IDF並歸一化。</p>
<h3 id="表一">表一<a hidden class="anchor" aria-hidden="true" href="#表一">#</a></h3>
<p>對MEDLINE資料集使用LSI/SVD，用使用centroid-based, kNN和SVMs分類器分類的結果。觀察到:</p>
<ol>
<li>kNN使用L2 norm在$l$為100-500時效果不佳，與餘弦相似性在未正則化資料表現更好的印象相符，且5NN明顯落後其他更高的K，表明k=5太小。</li>
</ol>
<h3 id="表二">表二<a hidden class="anchor" aria-hidden="true" href="#表二">#</a></h3>
<p>SVM使用不同的K(核)函數與降維演算法在MEDLINE資料集的結果。觀察到:</p>
<ol>
<li>降維後的預測結果與原始空間的預測結果相似且複雜度降低。</li>
<li>正交中心降維演算法對K函數的選擇不敏感，可以選複雜度更低的線性K函數。</li>
</ol>
<h3 id="表三">表三<a hidden class="anchor" aria-hidden="true" href="#表三">#</a></h3>
<p>選擇不同分類演算法與降維演算法在MEDLINE資料集的結果。觀察到:</p>
<ol>
<li>使用LDA/GSVD降維演算法時，使用餘弦相似度的centroid-based與kNN分類演算法效果較差，而使用L2-norm的效果較好，因跡最佳化使用L2-norm表示。</li>
<li>因LDA/GSVD最小化群內散點的跡(距離)，自同一(相似)類別的文檔向量會被變換為一個緻密的群甚至一個點，使得SVM難以找到泛化性高的超平面。</li>
</ol>
<h3 id="表四">表四<a hidden class="anchor" aria-hidden="true" href="#表四">#</a></h3>
<p>MEDLINE資料集中使用SVM配合不同降維演算法在5個不同類別中的準確率。觀察到:</p>
<ol>
<li>colon cancer與oral cancer難以被區分。</li>
</ol>
<h3 id="表五">表五<a hidden class="anchor" aria-hidden="true" href="#表五">#</a></h3>
<p>選擇不同分類演算法與降維演算法(Centroid、Orthogonal Centroid)在REUTERS資料集的結果。觀察到:</p>
<ol>
<li>Orthogonal Centroid 效果無明顯下降，Centroid則明顯降低，推測因將各個聚類中心映射至單位矩陣導致每個class間的資訊消失所造成。</li>
</ol>
<h2 id="結論與討論">結論與討論<a hidden class="anchor" aria-hidden="true" href="#結論與討論">#</a></h2>
<p>本文使用三種降維方法Centroid、Orthogonal Centroid和LDA/GSVD，皆是為集群資料所設計，做為比較也使用了LSI/SVD這種不保留集群結構的降維演算法。其也測試了三種不同的分類演算法SVMs、kNN與centroid-based classification測試在不同降維法中的分類效果。測試結果取得了高的準度，即使在強力的降維下，也可與未降維的狀態近似。</p>
<p>該論文還引入了基於閥值的分類器，用於centroid-based和SVM做到一對多的分類。centroid在Class互不關聯的情況下表現更好。</p>
<h3 id="結論">結論<a hidden class="anchor" aria-hidden="true" href="#結論">#</a></h3>
<p>不犧牲精度的情況下，可以做到對文本大幅降為。Orthogonal Centroid在保留聚類結構的能力做的極佳，降為前後的預測準確率幾乎不變，在KNN或SVM使用可以極大減少計算複雜度。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://lin-roger.github.io/posts/promptee/">
    <span class="title">« Prev</span>
    <br>
    <span>prompt-engineering 基礎</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor"
        class="bi bi-arrow-up-square-fill" viewBox="0 0 16 16">
        <path
            d="M2 16a2 2 0 0 1-2-2V2a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v12a2 2 0 0 1-2 2zm6.5-4.5V5.707l2.146 2.147a.5.5 0 0 0 .708-.708l-3-3a.5.5 0 0 0-.708 0l-3 3a.5.5 0 1 0 .708.708L7.5 5.707V11.5a.5.5 0 0 0 1 0" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

    let tocBtn = document.getElementById("toc-btn");
    tocBtn.addEventListener('click', () => {
        tocBtn.classList.toggle("noShowToc");
        tocBtn.classList.toggle("showToc")
        document.getElementById("toc-container").classList.toggle("show");
    }, false)

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > document.documentElement.clientHeight || document.documentElement.scrollTop > document.documentElement.clientHeight) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script></body>

</html>
